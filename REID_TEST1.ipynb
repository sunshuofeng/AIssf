{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REID_TEST1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6GcAv-6SlFfh"
      ],
      "mount_file_id": "1T7HNwgtNGdRapM9NQ4oxjwruQ5R7Mrbp",
      "authorship_tag": "ABX9TyPWYC8ZQlE3JU2plOIrNzDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshuofeng/AIssf/blob/master/REID_TEST1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSBQLax1AC88"
      },
      "source": [
        "#加载kaggle文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwJGxCWF9q_-",
        "outputId": "b2b686b6-3070-4bee-a01a-137cb6ec3de5"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOreY7Kq-nFy"
      },
      "source": [
        "import json\r\n",
        "token = {\"username\":\"ssfailearning\",\"key\":\"bccedfffce653f07d5acaf4409341c47\"}\r\n",
        "with open('/content/kaggle.json', 'w') as file:\r\n",
        "  json.dump(token, file)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSUCpQuo_sAK"
      },
      "source": [
        "!mkdir -p ~/.kaggle\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG0vAfu6_-rY",
        "outputId": "3c83d17e-c673-4948-cbc4-d1566ff60dd7"
      },
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json\r\n",
        "!kaggle config set -n path -v /content"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- path is now set to: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VPVygJ6AO9l",
        "outputId": "6baaf005-fbe3-49d1-af58-9c734d8bbc5b"
      },
      "source": [
        "!kaggle datasets download -d rayiooo/reid_market-1501"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reid_market-1501.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-777-IiMBm-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922a781b-c1d4-42c7-d48d-6724fdad0bbe"
      },
      "source": [
        "!pip install geffnet\r\n",
        "!pip install git+https://github.com/pabloppp/pytorch-tools -U\r\n",
        "!pip install thop "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geffnet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from geffnet) (0.8.2+cu101)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from geffnet) (1.7.1+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->geffnet) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->geffnet) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->geffnet) (3.7.4.3)\n",
            "Collecting git+https://github.com/pabloppp/pytorch-tools\n",
            "  Cloning https://github.com/pabloppp/pytorch-tools to /tmp/pip-req-build-llusxk_e\n",
            "  Running command git clone -q https://github.com/pabloppp/pytorch-tools /tmp/pip-req-build-llusxk_e\n",
            "Requirement already satisfied, skipping upgrade: torch==1.* in /usr/local/lib/python3.7/dist-packages (from torchtools==0.2.6) (1.7.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from torchtools==0.2.6) (0.8.2+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy==1.* in /usr/local/lib/python3.7/dist-packages (from torchtools==0.2.6) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.*->torchtools==0.2.6) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->torchtools==0.2.6) (7.0.0)\n",
            "Building wheels for collected packages: torchtools\n",
            "  Building wheel for torchtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtools: filename=torchtools-0.2.6-cp37-none-any.whl size=21149 sha256=9bd03eee353c6eb3e94086858e5c40f7d3af126d5c38487d4ea5691e81e1412f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ljzif4_o/wheels/53/7a/b0/86ffb126404564b151069f4d8d9f43dbfe3f17bfc0b898b45f\n",
            "Successfully built torchtools\n",
            "Installing collected packages: torchtools\n",
            "  Found existing installation: torchtools 0.2.6\n",
            "    Uninstalling torchtools-0.2.6:\n",
            "      Successfully uninstalled torchtools-0.2.6\n",
            "Successfully installed torchtools-0.2.6\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.7.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pHZOn1hNueU"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Fr2DOSN6Xg"
      },
      "source": [
        "# <font color=red>***配置文件***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIT9OW4ZN8FD"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "np.random.seed(42)\r\n",
        "random.seed(42)\r\n",
        "torch.backends.cudnn.benchmark=True\r\n",
        "torch.manual_seed(42)\r\n",
        "torch.cuda.manual_seed(42)       \r\n",
        "cfg={\r\n",
        "    'train_size':[384,128],\r\n",
        "    'test_size':[384,128],\r\n",
        "    'padding':10,\r\n",
        "    'mean':[0.485, 0.456, 0.406],\r\n",
        "    'std':[0.229, 0.224, 0.225],\r\n",
        "\r\n",
        "    'Multi_Data':False,\r\n",
        "    'num_workers':8,\r\n",
        "    'SAMPLER':'',\r\n",
        "\r\n",
        "    'train_bs':48,\r\n",
        "    'test_bs':48,\r\n",
        "\r\n",
        "    'train_K_instances':12,\r\n",
        "     \r\n",
        "    'epochs':70,\r\n",
        "    'ckpt':'reid_test1.pt',\r\n",
        "    'logpt':'test1.log',\r\n",
        "\r\n",
        "    'rerank':False,\r\n",
        "   \r\n",
        "    'lr':3e-4,\r\n",
        "    'momentum':0.9,\r\n",
        "    'weight_decay':5e-4,\r\n",
        "     \r\n",
        "    'margin':0.3,\r\n",
        "    'steps':[35,55],\r\n",
        "    'warmup_iters':10,\r\n",
        "    'warmup_factors':1/3,\r\n",
        "    'gamma':0.1,\r\n",
        "    'model':'baseline',\r\n",
        "    'backbone':'resnet50',\r\n",
        "    'last_stride':(1,1),\r\n",
        "    'optimizer':'adam',\r\n",
        "    'scheduler':'warmup',\r\n",
        "    'grad_l2':False,\r\n",
        "     \r\n",
        "    'loss':[('cross',1.0),('cirlce',1.0)],\r\n",
        "\r\n",
        "    'two_stage':False, \r\n",
        "\r\n",
        "    'amp':True,\r\n",
        "    'ema':False,\r\n",
        "   \r\n",
        "    \r\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9xfXfqJNvNH"
      },
      "source": [
        "# <font color=red>***读取数据***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmKPLdgCQHM3"
      },
      "source": [
        "# import numpy as np # linear algebra\r\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "\r\n",
        "# # Input data files are available in the read-only \"../input/\" directory\r\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\r\n",
        "\r\n",
        "# import os\r\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\r\n",
        "#     for filename in filenames:\r\n",
        "#         print(os.path.join(dirname, filename))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVSkPA9dREsf"
      },
      "source": [
        "##<font color=blue>***取market1501***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkosuYLnNxBo"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "# os.makedirs('datasets')\r\n",
        "zip_file = zipfile.ZipFile('/content/drive/MyDrive/dataset/REID/reid1.zip')\r\n",
        "for names in zip_file.namelist():\r\n",
        "        zip_file.extract(names,'datasets')\r\n",
        "zip_file.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGwfmPqhPBWp"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from torch.cuda.amp import GradScaler,autocast\r\n",
        "import torch\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiEL5941NDEX"
      },
      "source": [
        "##<font color=blue>***数据增强***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOSE54gbzWn"
      },
      "source": [
        "###<font color=green>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HQcNH5LMilp"
      },
      "source": [
        "#### <font color=yellow>***随机擦除***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40V4tXtuMiSQ"
      },
      "source": [
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "\r\n",
        "class RandomErasing(object):\r\n",
        "    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\r\n",
        "        'Random Erasing Data Augmentation' by Zhong et al.\r\n",
        "        See https://arxiv.org/pdf/1708.04896.pdf\r\n",
        "    Args:\r\n",
        "         probability: The probability that the Random Erasing operation will be performed.\r\n",
        "         sl: Minimum proportion of erased area against input image.\r\n",
        "         sh: Maximum proportion of erased area against input image.\r\n",
        "         r1: Minimum aspect ratio of erased area.\r\n",
        "         mean: Erasing value.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=(0.4914, 0.4822, 0.4465)):\r\n",
        "        self.probability = probability\r\n",
        "        self.mean = mean\r\n",
        "        self.sl = sl\r\n",
        "        self.sh = sh\r\n",
        "        self.r1 = r1\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "\r\n",
        "        if random.uniform(0, 1) >= self.probability:\r\n",
        "            return img\r\n",
        "\r\n",
        "        for attempt in range(100):\r\n",
        "            area = img.size()[1] * img.size()[2]\r\n",
        "\r\n",
        "            target_area = random.uniform(self.sl, self.sh) * area\r\n",
        "            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\r\n",
        "\r\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\r\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\r\n",
        "\r\n",
        "            if w < img.size()[2] and h < img.size()[1]:\r\n",
        "                x1 = random.randint(0, img.size()[1] - h)\r\n",
        "                y1 = random.randint(0, img.size()[2] - w)\r\n",
        "                if img.size()[0] == 3:\r\n",
        "                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\r\n",
        "                    img[1, x1:x1 + h, y1:y1 + w] = self.mean[1]\r\n",
        "                    img[2, x1:x1 + h, y1:y1 + w] = self.mean[2]\r\n",
        "                else:\r\n",
        "                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\r\n",
        "                return img\r\n",
        "\r\n",
        "        return img"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmHKoMpY3MXg"
      },
      "source": [
        "#### 灰度+sketch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv9e3a1g3LPq"
      },
      "source": [
        "\r\n",
        "import math\r\n",
        "from PIL import Image  #PIL=python image library\r\n",
        "import random\r\n",
        "import  numpy as np\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "########################### this code is for Local Grayscale Patch Replacement(LGPR)  #################################\r\n",
        "class LGPR(object):\r\n",
        "\r\n",
        "    def __init__(self, probability=0.2, sl=0.02, sh=0.4, r1=0.3):\r\n",
        "        self.probability = probability\r\n",
        "        self.sl = sl\r\n",
        "        self.sh = sh\r\n",
        "        self.r1 = r1\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "\r\n",
        "        new = img.convert(\"L\")\r\n",
        "        np_img = np.array(new, dtype=np.uint8)\r\n",
        "        img_gray = np.dstack([np_img, np_img, np_img])\r\n",
        "\r\n",
        "        if random.uniform(0, 1) >= self.probability:\r\n",
        "            return img\r\n",
        "\r\n",
        "        for attempt in range(100):\r\n",
        "            area = img.size[0] * img.size[1]\r\n",
        "            target_area = random.uniform(self.sl, self.sh) * area\r\n",
        "            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\r\n",
        "\r\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\r\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\r\n",
        "\r\n",
        "            if w < img.size[1] and h < img.size[0]:\r\n",
        "                x1 = random.randint(0, img.size[0] - h)\r\n",
        "                y1 = random.randint(0, img.size[1] - w)\r\n",
        "                img = np.asarray(img).astype('float')\r\n",
        "\r\n",
        "                img[y1:y1 + h, x1:x1 + w, 0] = img_gray[y1:y1 + h, x1:x1 + w, 0]\r\n",
        "                img[y1:y1 + h, x1:x1 + w, 1] = img_gray[y1:y1 + h, x1:x1 + w, 1]\r\n",
        "                img[y1:y1 + h, x1:x1 + w, 2] = img_gray[y1:y1 + h, x1:x1 + w, 2]\r\n",
        "\r\n",
        "                img = Image.fromarray(img.astype('uint8'))\r\n",
        "\r\n",
        "                return img\r\n",
        "\r\n",
        "        return img\r\n",
        "#######################################################################################################################\r\n",
        "################################ this code is for Multi-Modal Defense  ################################################\r\n",
        "\r\n",
        "def toSketch(img):  # Convert visible  image to sketch image\r\n",
        "    img_np = np.asarray(img)\r\n",
        "    img_inv = 255 - img_np\r\n",
        "    img_blur = cv2.GaussianBlur(img_inv, ksize=(27, 27), sigmaX=0, sigmaY=0)\r\n",
        "    img_blend = cv2.divide(img_np, 255 - img_blur, scale=256)\r\n",
        "    img_blend = Image.fromarray(img_blend)\r\n",
        "    return img_blend\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Randomly select several channels of visible image (R, G, B), gray image (gray), and sketch image (sketch) \r\n",
        "to fuse them into a new 3-channel image.\r\n",
        "\"\"\"\r\n",
        "def random_choose(r, g, b, gray_or_sketch):\r\n",
        "    p = [r, g, b, gray_or_sketch, gray_or_sketch]\r\n",
        "    idx = [0, 1, 2, 3, 4]\r\n",
        "    random.shuffle(idx)\r\n",
        "    return Image.merge('RGB', [p[idx[0]], p[idx[1]], p[idx[2]]])\r\n",
        "\r\n",
        "\r\n",
        "# 10(%Grayscale) 5%(Grayscale-RGB) 5%(Sketch-RGB)\r\n",
        "class Fuse_RGB_Gray_Sketch(object):\r\n",
        "    def __init__(self,G=0.1,G_rgb = 0.05,S_rgb =0.05):\r\n",
        "        self.G = G\r\n",
        "        self.G_rgb = G_rgb\r\n",
        "        self.S_rgb = S_rgb\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "        r, g, b = img.split()\r\n",
        "        gray = img.convert('L') #convert visible  image to grayscale images\r\n",
        "        p = random.random()\r\n",
        "        if p < self.G: #just Grayscale\r\n",
        "            return Image.merge('RGB', [gray, gray, gray])\r\n",
        "\r\n",
        "        elif p < self.G + self.G_rgb: #fuse Grayscale-RGB\r\n",
        "            img2 = random_choose(r, g, b, gray)\r\n",
        "            return img2\r\n",
        "\r\n",
        "        elif p < self.G + self.G_rgb + self.S_rgb: #fuse Sketch-RGB\r\n",
        "            sketch = toSketch(gray)\r\n",
        "            img3 = random_choose(r, g, b, sketch)\r\n",
        "            return img3\r\n",
        "        else:\r\n",
        "            return img"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-6Dc1U3M_5_"
      },
      "source": [
        "####<font color=yellow>***其余数据增强（torchvision transformss）***</font>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFo4pix1VcKT"
      },
      "source": [
        "\r\n",
        "import torchvision.transforms as T\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_transforms(cfg, training=True):\r\n",
        "    normalize_transform = T.Normalize(mean=cfg['mean'], std=cfg['std'])\r\n",
        "    if training:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['train_size']),\r\n",
        "            T.RandomHorizontalFlip(p=0.5),\r\n",
        "            T.Pad(cfg['padding']),\r\n",
        "            T.RandomCrop(cfg['train_size']),\r\n",
        "        \r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform,\r\n",
        "            RandomErasing(probability=0.5, mean=cfg['mean'])\r\n",
        "        ])\r\n",
        "    else:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['test_size']),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform\r\n",
        "        ])\r\n",
        "\r\n",
        "    return transform\r\n",
        "\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWWYVq0Ixios"
      },
      "source": [
        "def build_change_transforms(cfg, training=True):\r\n",
        "    normalize_transform = T.Normalize(mean=cfg['mean'], std=cfg['std'])\r\n",
        "    if training:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['train_size']),\r\n",
        "            T.ColorJitter(),\r\n",
        "            T.RandomHorizontalFlip(p=0.5),\r\n",
        "            T.Pad(cfg['padding']),\r\n",
        "            T.RandomCrop(cfg['train_size']),\r\n",
        "            T.RandomGrayscale(p=0.1),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform,\r\n",
        "            RandomErasing(probability=0.5, mean=cfg['mean']),\r\n",
        "            \r\n",
        "        ])\r\n",
        "    else:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['test_size']),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform\r\n",
        "        ])\r\n",
        "\r\n",
        "    return transform\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LbctDphPFgB"
      },
      "source": [
        "##<font color=blue>***sampler***</font>\r\n",
        "\r\n",
        "<font color=red>抽取N个pid(即N个人)，每个人抽取K张图片，目的是为了能确保至少有三元组，**锚，正样本，负样本**</font>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoJVgt7sb4TU"
      },
      "source": [
        "###<font color=green>***图像***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9S_5uSFN46Z"
      },
      "source": [
        "\r\n",
        "import copy\r\n",
        "import random\r\n",
        "import torch\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "\r\n",
        "class RandomIdentitySampler(Sampler):\r\n",
        "    \"\"\"\r\n",
        "    Randomly sample N identities, then for each identity,\r\n",
        "    randomly sample K instances, therefore batch size is N*K.\r\n",
        "    Args:\r\n",
        "    - data_source (list): list of (img_path, pid, camid).\r\n",
        "    - num_instances (int): number of instances per identity in a batch.\r\n",
        "    - batch_size (int): number of examples in a batch.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data_source, batch_size, num_instances):\r\n",
        "        self.data_source = data_source\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_instances = num_instances\r\n",
        "        self.num_pids_per_batch = self.batch_size // self.num_instances\r\n",
        "        self.index_dic = defaultdict(list)\r\n",
        "        for index, (_, pid, _) in enumerate(self.data_source):\r\n",
        "            self.index_dic[pid].append(index)\r\n",
        "        self.pids = list(self.index_dic.keys())\r\n",
        "\r\n",
        "        # estimate number of examples in an epoch\r\n",
        "        self.length = 0\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = self.index_dic[pid]\r\n",
        "            num = len(idxs)\r\n",
        "            if num < self.num_instances:\r\n",
        "                num = self.num_instances\r\n",
        "            self.length += num - num % self.num_instances\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        batch_idxs_dict = defaultdict(list)\r\n",
        "\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = copy.deepcopy(self.index_dic[pid])\r\n",
        "            if len(idxs) < self.num_instances:\r\n",
        "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\r\n",
        "            random.shuffle(idxs)\r\n",
        "            batch_idxs = []\r\n",
        "            for idx in idxs:\r\n",
        "                batch_idxs.append(idx)\r\n",
        "                if len(batch_idxs) == self.num_instances:\r\n",
        "                    batch_idxs_dict[pid].append(batch_idxs)\r\n",
        "                    batch_idxs = []\r\n",
        "\r\n",
        "        avai_pids = copy.deepcopy(self.pids)\r\n",
        "        final_idxs = []\r\n",
        "\r\n",
        "        while len(avai_pids) >= self.num_pids_per_batch:\r\n",
        "            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)\r\n",
        "            for pid in selected_pids:\r\n",
        "                batch_idxs = batch_idxs_dict[pid].pop(0)\r\n",
        "                final_idxs.extend(batch_idxs)\r\n",
        "                if len(batch_idxs_dict[pid]) == 0:\r\n",
        "                    avai_pids.remove(pid)\r\n",
        "\r\n",
        "        self.length = len(final_idxs)\r\n",
        "        return iter(final_idxs)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.length\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63UZv9eQx_5m"
      },
      "source": [
        "class DateRandomIdentitySampler(Sampler):\r\n",
        "    \"\"\"\r\n",
        "    Randomly sample N identities, then for each identity,\r\n",
        "    randomly sample K instances, therefore batch size is N*K.\r\n",
        "    Args:\r\n",
        "    - data_source (list): list of (img_path, pid, camid).\r\n",
        "    - num_instances (int): number of instances per identity in a batch.\r\n",
        "    - batch_size (int): number of examples in a batch.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data_source, batch_size, num_instances):\r\n",
        "        self.data_source = data_source\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_instances = num_instances\r\n",
        "        self.num_pids_per_batch = self.batch_size // self.num_instances\r\n",
        "        self.index_dic = defaultdict(list)\r\n",
        "        for index, (_, pid, _,_) in enumerate(self.data_source):\r\n",
        "            self.index_dic[pid].append(index)\r\n",
        "        self.pids = list(self.index_dic.keys())\r\n",
        "\r\n",
        "        # estimate number of examples in an epoch\r\n",
        "        self.length = 0\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = self.index_dic[pid]\r\n",
        "            num = len(idxs)\r\n",
        "            if num < self.num_instances:\r\n",
        "                num = self.num_instances\r\n",
        "            self.length += num - num % self.num_instances\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        batch_idxs_dict = defaultdict(list)\r\n",
        "\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = copy.deepcopy(self.index_dic[pid])\r\n",
        "            if len(idxs) < self.num_instances:\r\n",
        "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\r\n",
        "            random.shuffle(idxs)\r\n",
        "            batch_idxs = []\r\n",
        "            for idx in idxs:\r\n",
        "                batch_idxs.append(idx)\r\n",
        "                if len(batch_idxs) == self.num_instances:\r\n",
        "                    batch_idxs_dict[pid].append(batch_idxs)\r\n",
        "                    batch_idxs = []\r\n",
        "\r\n",
        "        avai_pids = copy.deepcopy(self.pids)\r\n",
        "        final_idxs = []\r\n",
        "\r\n",
        "        while len(avai_pids) >= self.num_pids_per_batch:\r\n",
        "            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)\r\n",
        "            for pid in selected_pids:\r\n",
        "                batch_idxs = batch_idxs_dict[pid].pop(0)\r\n",
        "                final_idxs.extend(batch_idxs)\r\n",
        "                if len(batch_idxs_dict[pid]) == 0:\r\n",
        "                    avai_pids.remove(pid)\r\n",
        "\r\n",
        "        self.length = len(final_idxs)\r\n",
        "        return iter(final_idxs)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.length"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzMoTzRBS8WK"
      },
      "source": [
        "##<font color=blue>***建立数据集***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7OPh6Fabn__"
      },
      "source": [
        "###<font color=green>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBqvtdO-TA6N"
      },
      "source": [
        "####<font color=yellow>***图像读取数据集（market1501）***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a4BoMPfTDpS"
      },
      "source": [
        "import glob\r\n",
        "import re\r\n",
        "import os.path as osp\r\n",
        "class Market1501():\r\n",
        "    def __init__(self, root='/content/datasets', verbose=True, **kwargs):\r\n",
        "        super(Market1501, self).__init__()\r\n",
        "        self.dataset_dir = root\r\n",
        "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\r\n",
        "        self.query_dir = osp.join(self.dataset_dir, 'query')\r\n",
        "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\r\n",
        "        self._check_before_run()\r\n",
        "\r\n",
        "        train = self._process_dir(self.train_dir, relabel=True)\r\n",
        "        query = self._process_dir(self.query_dir, relabel=False)\r\n",
        "        gallery = self._process_dir(self.gallery_dir, relabel=False)\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"=> Market1501 loaded\")\r\n",
        "            self.print_dataset_statistics(train, query, gallery)\r\n",
        "\r\n",
        "        self.train = train\r\n",
        "        self.query = query\r\n",
        "        self.gallery = gallery\r\n",
        "\r\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "    def print_dataset_statistics(self, train, query, gallery):\r\n",
        "        num_train_pids, num_train_imgs, num_train_cams = self.get_imagedata_info(train)\r\n",
        "        num_query_pids, num_query_imgs, num_query_cams = self.get_imagedata_info(query)\r\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams = self.get_imagedata_info(gallery)\r\n",
        "\r\n",
        "        print(\"Dataset statistics:\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\r\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\r\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "    \r\n",
        "    def get_imagedata_info(self, data):\r\n",
        "        pids, cams = [], []\r\n",
        "        for _, pid, camid in data:\r\n",
        "            pids += [pid]\r\n",
        "            cams += [camid]\r\n",
        "        pids = set(pids)\r\n",
        "        cams = set(cams)\r\n",
        "        num_pids = len(pids)\r\n",
        "        num_cams = len(cams)\r\n",
        "        num_imgs = len(data)\r\n",
        "        return num_pids, num_imgs, num_cams\r\n",
        "    \r\n",
        "\r\n",
        "    def _check_before_run(self):\r\n",
        "        \"\"\"Check if all files are available before going deeper\"\"\"\r\n",
        "        if not osp.exists(self.dataset_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\r\n",
        "        if not osp.exists(self.train_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\r\n",
        "        if not osp.exists(self.query_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\r\n",
        "        if not osp.exists(self.gallery_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\r\n",
        "\r\n",
        "    \r\n",
        "    ##最终返回的是每张图片的路径，pid，camid\r\n",
        "    def _process_dir(self, dir_path, relabel=False):\r\n",
        "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\r\n",
        "\r\n",
        "        ##匹配文件名中的有用信息，0001_c1代表pid=1，cid=1，\r\n",
        "        ##至于为什么是[-\\d]+,是因为有-1_c1这种的存在，也应该匹配上\r\n",
        "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\r\n",
        "\r\n",
        "        pid_container = set()\r\n",
        "        for img_path in img_paths:\r\n",
        "            pid, _ = map(int, pattern.search(img_path).groups())\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            pid_container.add(pid)\r\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\r\n",
        "\r\n",
        "        dataset = []\r\n",
        "        for img_path in img_paths:\r\n",
        "            pid, camid = map(int, pattern.search(img_path).groups())\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            assert 0 <= pid <= 1501  # pid == 0 means background\r\n",
        "            assert 1 <= camid <= 6\r\n",
        "            camid -= 1  # index starts from 0\r\n",
        "            if relabel: pid = pid2label[pid]\r\n",
        "            dataset.append((img_path, pid, camid))\r\n",
        "\r\n",
        "        return dataset\r\n",
        "\r\n",
        "\r\n",
        "import torch.utils.data as Data\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "\r\n",
        "class ImageDataset(Data.Dataset):\r\n",
        "    def __init__(self, dataset, transform=None):\r\n",
        "        self.dataset = dataset\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataset)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        img_path, pid, camid = self.dataset[index]\r\n",
        "        img = Image.open(img_path)\r\n",
        "        if self.transform is not None:\r\n",
        "            img = self.transform(img)\r\n",
        "        return img, pid, camid, img_path\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7Iq8cykxsWG"
      },
      "source": [
        "from fastai.vision import get_image_files\r\n",
        "import torch.utils.data as Data\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "class PDataset:\r\n",
        "  def __init__(self,root):\r\n",
        "    self.root=root\r\n",
        "    self.train_dir=osp.join(self.root,'train')\r\n",
        "    self.query_dir=osp.join(self.root,'query')\r\n",
        "    self.gallery_dir=osp.join(self.root,'gallery')\r\n",
        "    \r\n",
        "    train=self._process_dir(self.train_dir)\r\n",
        "    query=self._process_dir(self.query_dir)\r\n",
        "    gallery=self._process_dir(self.gallery_dir)\r\n",
        "    self.train=train\r\n",
        "    self.query=query\r\n",
        "    self.gallery=gallery\r\n",
        "    self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "    self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "    self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "  \r\n",
        "  def get_imagedata_info(self,data):\r\n",
        "    pids,cams=[],[]\r\n",
        "    for _,pid,camid,_ in data:\r\n",
        "      pids+=[pid]\r\n",
        "      cams+=[camid]\r\n",
        "    pids=set(pids)\r\n",
        "    cams=set(cams)\r\n",
        "    num_pids=len(pids)\r\n",
        "    num_cams=len(cams)\r\n",
        "    num_imgs=len(data)\r\n",
        "    return num_pids,num_imgs,num_cams\r\n",
        "\r\n",
        "  def _process_dir(self,dir_path):\r\n",
        "    image_names=get_image_files(dir_path)\r\n",
        "    dataset=[]\r\n",
        "    pid_set=set()\r\n",
        "    for name in image_names:\r\n",
        "      name=str(name)\r\n",
        "      path=name.split('/')[-1]\r\n",
        "      pid,_,date,_=path.split('_')\r\n",
        "      pid_set.add(pid)\r\n",
        "    pid2label={pid: label for label, pid in enumerate(pid_set)}\r\n",
        "    for name in image_names:\r\n",
        "      name=str(name)\r\n",
        "      path=name.split('/')[-1]\r\n",
        "      pid,_,date,_=path.split('_')\r\n",
        "      pid=pid2label[pid]\r\n",
        "      cam=image_cams[path]\r\n",
        "      date=date_ids[date]\r\n",
        "      dataset.append((name,int(pid),int(cam),int(date)))\r\n",
        "    return dataset\r\n",
        "\r\n",
        "class ImageDataset(Data.Dataset):\r\n",
        "    def __init__(self, dataset, transform=None):\r\n",
        "        self.dataset = dataset\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataset)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        img_path, pid, camid,date = self.dataset[index]\r\n",
        "        img = Image.open(img_path)\r\n",
        "        if self.transform is not None:\r\n",
        "            img = self.transform(img)\r\n",
        "        return img, pid, camid,img_path,date\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUNQLWzGbCme"
      },
      "source": [
        "###<font color=green>***建立dataloader***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63cQkqjFbGL1"
      },
      "source": [
        "\r\n",
        "###对于训练的时候我们不需要关心摄像头的问题，我们只需要让模型能认出图片里的人就行\r\n",
        "def train_collate_fn(batch):\r\n",
        "    imgs, pids, _, _, = zip(*batch)\r\n",
        "    pids = torch.tensor(pids, dtype=torch.int64)\r\n",
        "    return torch.stack(imgs, dim=0), pids\r\n",
        "\r\n",
        "###对于验证集而言，为了提高验证的真实性，我们应该防止同一摄像头的图片进入验证（同一摄像头相当于数据泄露）\r\n",
        "def val_collate_fn(batch):\r\n",
        "    imgs, pids, camids, _ = zip(*batch)\r\n",
        "    return torch.stack(imgs, dim=0), pids, camids\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def make_data_loader(datasets,cfg):\r\n",
        "    train_transform=build_transforms(cfg,training=True)\r\n",
        "    test_transform=build_transforms(cfg,training=False)\r\n",
        "    if cfg['Multi_Data']:\r\n",
        "        data=datasets[0]\r\n",
        "        for dataset in datasets[1:]:\r\n",
        "            data.train.extend(dataset.train)\r\n",
        "            data.query.extend(dataset.query)\r\n",
        "            data.gallery.extend(dataset.gallery)\r\n",
        "\r\n",
        "            data.num_train_pids+=dataset.num_train_pids\r\n",
        "    else:\r\n",
        "        data=datasets\r\n",
        "\r\n",
        "    num_classes=data.num_train_pids\r\n",
        "    train_set=ImageDataset(data.train,train_transform)\r\n",
        "\r\n",
        "    if cfg['SAMPLER']=='softmax':\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     shuffle=True,collate_fn=train_collate_fn)\r\n",
        "    else:\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     sampler=RandomIdentitySampler(data.train,\r\n",
        "                                    cfg['train_bs'],cfg['train_K_instances']),\r\n",
        "                                    num_workers=cfg['num_workers'],\r\n",
        "                                    collate_fn=train_collate_fn)\r\n",
        "    \r\n",
        "    val_set = ImageDataset(data.query + data.gallery, test_transform)\r\n",
        "    val_loader = Data.DataLoader(\r\n",
        "        val_set, batch_size=cfg['test_bs'], shuffle=False, num_workers=cfg['num_workers'],\r\n",
        "        collate_fn=val_collate_fn\r\n",
        "    )\r\n",
        "    return train_loader, val_loader, len(data.query), num_classes\r\n",
        "\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MYnmQAkx1Vu"
      },
      "source": [
        "def train_collate_fn1(batch):\r\n",
        "    imgs, pids, _, _,_ = zip(*batch)\r\n",
        "    pids = torch.tensor(pids, dtype=torch.int64)\r\n",
        "    return torch.stack(imgs, dim=0), pids\r\n",
        "\r\n",
        "###对于验证集而言，为了提高验证的真实性，我们应该防止同一摄像头的图片进入验证（同一摄像头相当于数据泄露）\r\n",
        "def val_collate_fn1(batch):\r\n",
        "    imgs, pids, camids, _,dates = zip(*batch)\r\n",
        "    return torch.stack(imgs, dim=0), pids, camids,dates\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def make_date_data_loader(datasets,cfg):\r\n",
        "    train_transform=build_change_transforms(cfg,training=True)\r\n",
        "    test_transform=build_change_transforms(cfg,training=False)\r\n",
        "    if cfg['Multi_Data']:\r\n",
        "        data=datasets[0]\r\n",
        "        for dataset in datasets[1:]:\r\n",
        "            data.train.extend(dataset.train)\r\n",
        "            data.query.extend(dataset.query)\r\n",
        "            data.gallery.extend(dataset.gallery)\r\n",
        "\r\n",
        "            data.num_train_pids+=dataset.num_train_pids\r\n",
        "    else:\r\n",
        "        data=datasets\r\n",
        "\r\n",
        "    num_classes=data.num_train_pids\r\n",
        "    train_set=ImageDataset(data.train,train_transform)\r\n",
        "\r\n",
        "    if cfg['SAMPLER']=='softmax':\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     shuffle=True,collate_fn=train_collate_fn1)\r\n",
        "    else:\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     sampler=DateRandomIdentitySampler(data.train,\r\n",
        "                                    cfg['train_bs'],cfg['train_K_instances']),\r\n",
        "                                    num_workers=cfg['num_workers'],\r\n",
        "                                    collate_fn=train_collate_fn1)\r\n",
        "    \r\n",
        "    val_set = ImageDataset(data.query + data.gallery, test_transform)\r\n",
        "    val_loader = Data.DataLoader(\r\n",
        "        val_set, batch_size=cfg['test_bs'], shuffle=False, num_workers=cfg['num_workers'],\r\n",
        "        collate_fn=val_collate_fn1\r\n",
        "    )\r\n",
        "    return train_loader, val_loader, len(data.query), num_classes\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrTHGA10c2Ug"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW_0tNfkO4JZ"
      },
      "source": [
        "#<font color=red>***模型建立***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTBDCyLMehBG"
      },
      "source": [
        "##<font color=blue>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eagycO34btoY"
      },
      "source": [
        "###<font color=green>***backbone***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1hk38VrbzMx"
      },
      "source": [
        "####<font color=yellow>***resnet50***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUCLhWLadN3M"
      },
      "source": [
        "from torchvision.models import resnet50\r\n",
        "def ResNet50(cfg):\r\n",
        "  model=resnet50(True)\r\n",
        "  stride=cfg['last_stride']\r\n",
        "  model.layer4[0].downsample[0].stride=stride\r\n",
        "  model.layer4[0].conv2.stride=stride\r\n",
        "  base=nn.Sequential(\r\n",
        "            model.conv1,\r\n",
        "            model.bn1,\r\n",
        "            model.maxpool,\r\n",
        "            model.layer1,\r\n",
        "            model.layer2,\r\n",
        "            model.layer3,\r\n",
        "            model.layer4\r\n",
        "        )\r\n",
        "  return base,2048"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Q1WOzueXX1"
      },
      "source": [
        "####<font color=yellow>***resnet101***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcrd5Cw4ecne"
      },
      "source": [
        "from torchvision.models import resnet101\r\n",
        "def ResNet101(cfg):\r\n",
        "  model=resnet101(True)\r\n",
        "  stride=cfg['last_stride']\r\n",
        "  model.layer4[0].downsample[0].stride=stride\r\n",
        "  model.layer4[0].conv2.stride=stride\r\n",
        "  base=nn.Sequential(\r\n",
        "            model.conv1,\r\n",
        "            model.bn1,\r\n",
        "            model.maxpool,\r\n",
        "            model.layer1,\r\n",
        "            model.layer2,\r\n",
        "            model.layer3,\r\n",
        "            model.layer4\r\n",
        "        )\r\n",
        "  return base,2048"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm5eBg46e47t"
      },
      "source": [
        "####<font color=yellow>***effnet_b3***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gin30U1gf1E"
      },
      "source": [
        "from geffnet import efficientnet_b3\r\n",
        "def Eff_b3(cfg):\r\n",
        "  model=efficientnet_b3(True)\r\n",
        "  base=nn.Sequential(\r\n",
        "    model.conv_stem,\r\n",
        "    model.bn1,\r\n",
        "    model.act1,\r\n",
        "    model.blocks,\r\n",
        "    model.conv_head,\r\n",
        "    model.bn2,\r\n",
        "    model.act2\r\n",
        "  )\r\n",
        "  return base,1536"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blyabbawiUeW"
      },
      "source": [
        "####<font color=yellow>***effnet_b5***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WScqbtDirD6"
      },
      "source": [
        "from geffnet import efficientnet_b5\r\n",
        "def Eff_b5(cfg):\r\n",
        "  model=efficientnet_b5(True)\r\n",
        "  base=nn.Sequential(\r\n",
        "    model.conv_stem,\r\n",
        "    model.bn1,\r\n",
        "    model.act1,\r\n",
        "    model.blocks,\r\n",
        "    model.conv_head,\r\n",
        "    model.bn2,\r\n",
        "    model.act2\r\n",
        "  )\r\n",
        "  return base,2048"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp4X-H6Kwy8u"
      },
      "source": [
        "####<font color=yellow>***eff_b6***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Qz5YY2w3eL"
      },
      "source": [
        "from geffnet import tf_efficientnet_b6_ap\r\n",
        "def Eff_b6(cfg):\r\n",
        "  model=tf_efficientnet_b6_ap(True)\r\n",
        "  base=nn.Sequential(\r\n",
        "    model.conv_stem,\r\n",
        "    model.bn1,\r\n",
        "    model.act1,\r\n",
        "    model.blocks,\r\n",
        "    model.conv_head,\r\n",
        "    model.bn2,\r\n",
        "    model.act2\r\n",
        "  )\r\n",
        "  return base,2304"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvOcprzHF4lC"
      },
      "source": [
        "####<font color=yellow>***eff_b7***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QVFSPRlGOWh"
      },
      "source": [
        "from geffnet import tf_efficientnet_b7_ap\r\n",
        "def Eff_b7(cfg):\r\n",
        "  model=tf_efficientnet_b7_ap(True)\r\n",
        "  base=nn.Sequential(\r\n",
        "    model.conv_stem,\r\n",
        "    model.bn1,\r\n",
        "    model.act1,\r\n",
        "    model.blocks,\r\n",
        "    model.conv_head,\r\n",
        "    model.bn2,\r\n",
        "    model.act2\r\n",
        "  )\r\n",
        "  return base,2560"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGKKFCmvejZ_"
      },
      "source": [
        "###<font color=green>***strong-baseline***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZZXgt-O5jV"
      },
      "source": [
        "from torchvision.models import resnet50\r\n",
        "import torch.nn as nn\r\n",
        "def weights_init_kaiming(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find('Linear') != -1:\r\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\r\n",
        "        nn.init.constant_(m.bias, 0.0)\r\n",
        "    elif classname.find('Conv') != -1:\r\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\r\n",
        "        if m.bias is not None:\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "    elif classname.find('BatchNorm') != -1:\r\n",
        "        if m.affine:\r\n",
        "            nn.init.constant_(m.weight, 1.0)\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "\r\n",
        "def weights_init_classifier(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find('Linear') != -1:\r\n",
        "        nn.init.normal_(m.weight, std=0.001)\r\n",
        "        if m.bias:\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "class Baseline(nn.Module):\r\n",
        "   \r\n",
        "    def __init__(self,num_classes,cfg,neck='bnneck', neck_feat='after'):\r\n",
        "        super(Baseline, self).__init__()\r\n",
        "        if cfg['backbone']=='resnet50':\r\n",
        "          self.base,in_planes=ResNet50(cfg)\r\n",
        "        if cfg['backbone']=='resnet101':\r\n",
        "          self.base,in_planes=ResNet101(cfg) \r\n",
        "        if cfg['backbone']=='eff_b3':\r\n",
        "          self.base,in_planes=Eff_b3(cfg)  \r\n",
        "        if cfg['backbone']=='eff_b5':\r\n",
        "          self.base,in_planes=Eff_b5(cfg)\r\n",
        "        if cfg['backbone']=='eff_b7':\r\n",
        "          self.base,in_planes=Eff_b7(cfg)\r\n",
        "        if cfg['backbone']=='eff_b6':\r\n",
        "          self.base,in_planes=Eff_b6(cfg)   \r\n",
        "\r\n",
        "\r\n",
        "        self.in_planes=in_planes\r\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\r\n",
        "        # self.gap = nn.AdaptiveMaxPool2d(1)\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.neck = neck\r\n",
        "        self.neck_feat = neck_feat\r\n",
        "\r\n",
        "        if self.neck == 'no':\r\n",
        "            self.classifier = nn.Linear(self.in_planes, self.num_classes)\r\n",
        "            # self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)     # new add by luo\r\n",
        "            # self.classifier.apply(weights_init_classifier)  # new add by luo\r\n",
        "        elif self.neck == 'bnneck':\r\n",
        "            self.bottleneck = nn.BatchNorm1d(self.in_planes)\r\n",
        "            self.bottleneck.bias.requires_grad_(False)  # no shift\r\n",
        "            self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)\r\n",
        "\r\n",
        "            self.bottleneck.apply(weights_init_kaiming)\r\n",
        "            self.classifier.apply(weights_init_classifier)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        global_feat = self.gap(self.base(x))  # (b, 2048, 1, 1)\r\n",
        "        global_feat = global_feat.view(global_feat.shape[0], -1)  # flatten to (bs, 2048)\r\n",
        "        out={}\r\n",
        "        if self.neck == 'no':\r\n",
        "            feat = global_feat\r\n",
        "        elif self.neck == 'bnneck':\r\n",
        "            feat = self.bottleneck(global_feat)  # normalize for angular softmax\r\n",
        "\r\n",
        "        if self.training:\r\n",
        "            cls_score = self.classifier(feat)\r\n",
        "            out['logit']=cls_score\r\n",
        "            out['feat']=global_feat\r\n",
        "            return out  # global feature for triplet loss\r\n",
        "        else:\r\n",
        "            if self.neck_feat == 'after':\r\n",
        "                # print(\"Test with feature after BN\")\r\n",
        "                out['feat']=feat\r\n",
        "                return out\r\n",
        "            else:\r\n",
        "                # print(\"Test with feature before BN\")\r\n",
        "                out['feat']=global_feat\r\n",
        "                return out\r\n",
        "\r\n",
        "    def load_param(self, trained_path):\r\n",
        "        param_dict = torch.load(trained_path)\r\n",
        "        for i in param_dict:\r\n",
        "            if 'classifier' in i:\r\n",
        "                continue\r\n",
        "            self.state_dict()[i].copy_(param_dict[i])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfDEoyGAnsRt"
      },
      "source": [
        "###<font color=green>***全局注意力网络***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qegke04Kn3qM"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "import pdb\r\n",
        "\r\n",
        "# ===================\r\n",
        "#     RGA Module\r\n",
        "# ===================\r\n",
        "\r\n",
        "class RGA_Module(nn.Module):\r\n",
        "\tdef __init__(self, in_channel, in_spatial, use_spatial=True, use_channel=True, \\\r\n",
        "\t\tcha_ratio=8, spa_ratio=8, down_ratio=8):\r\n",
        "\t\tsuper(RGA_Module, self).__init__()\r\n",
        "\r\n",
        "\t\tself.in_channel = in_channel\r\n",
        "\t\tself.in_spatial = in_spatial\r\n",
        "\t\t\r\n",
        "\t\tself.use_spatial = use_spatial\r\n",
        "\t\tself.use_channel = use_channel\r\n",
        "\r\n",
        "\t\tprint ('Use_Spatial_Att: {};\\tUse_Channel_Att: {}.'.format(self.use_spatial, self.use_channel))\r\n",
        "\r\n",
        "\t\tself.inter_channel = in_channel // cha_ratio\r\n",
        "\t\tself.inter_spatial = in_spatial // spa_ratio\r\n",
        "\t\t\r\n",
        "\t\t# Embedding functions for original features\r\n",
        "\t\tif self.use_spatial:\r\n",
        "\t\t\tself.gx_spatial = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_channel, out_channels=self.inter_channel,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_channel),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\tif self.use_channel:\r\n",
        "\t\t\tself.gx_channel = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_spatial, out_channels=self.inter_spatial,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_spatial),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\t\r\n",
        "\t\t# Embedding functions for relation features\r\n",
        "\t\tif self.use_spatial:\r\n",
        "\t\t\tself.gg_spatial = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_spatial * 2, out_channels=self.inter_spatial,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_spatial),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\tif self.use_channel:\r\n",
        "\t\t\tself.gg_channel = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_channel*2, out_channels=self.inter_channel,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_channel),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\t\r\n",
        "\t\t# Networks for learning attention weights\r\n",
        "\t\tif self.use_spatial:\r\n",
        "\t\t\tnum_channel_s = 1 + self.inter_spatial\r\n",
        "\t\t\tself.W_spatial = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=num_channel_s, out_channels=num_channel_s//down_ratio,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(num_channel_s//down_ratio),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=num_channel_s//down_ratio, out_channels=1,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(1)\r\n",
        "\t\t\t)\r\n",
        "\t\tif self.use_channel:\t\r\n",
        "\t\t\tnum_channel_c = 1 + self.inter_channel\r\n",
        "\t\t\tself.W_channel = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=num_channel_c, out_channels=num_channel_c//down_ratio,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(num_channel_c//down_ratio),\r\n",
        "\t\t\t\tnn.ReLU(),\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=num_channel_c//down_ratio, out_channels=1,\r\n",
        "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(1)\r\n",
        "\t\t\t)\r\n",
        "\r\n",
        "\t\t# Embedding functions for modeling relations\r\n",
        "\t\tif self.use_spatial:\r\n",
        "\t\t\tself.theta_spatial = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_channel, out_channels=self.inter_channel,\r\n",
        "\t\t\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_channel),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\t\tself.phi_spatial = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_channel, out_channels=self.inter_channel,\r\n",
        "\t\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_channel),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\tif self.use_channel:\r\n",
        "\t\t\tself.theta_channel = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_spatial, out_channels=self.inter_spatial,\r\n",
        "\t\t\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_spatial),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\t\tself.phi_channel = nn.Sequential(\r\n",
        "\t\t\t\tnn.Conv2d(in_channels=self.in_spatial, out_channels=self.inter_spatial,\r\n",
        "\t\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=False),\r\n",
        "\t\t\t\tnn.BatchNorm2d(self.inter_spatial),\r\n",
        "\t\t\t\tnn.ReLU()\r\n",
        "\t\t\t)\r\n",
        "\t\t\t\t\r\n",
        "\tdef forward(self, x):\r\n",
        "\t\tb, c, h, w = x.size()\r\n",
        "\t\t\r\n",
        "\t\tif self.use_spatial:\r\n",
        "\t\t\t# spatial attention\r\n",
        "\t\t\ttheta_xs = self.theta_spatial(x)\t\r\n",
        "\t\t\tphi_xs = self.phi_spatial(x)\r\n",
        "\t\t\ttheta_xs = theta_xs.view(b, self.inter_channel, -1)\r\n",
        "\t\t\ttheta_xs = theta_xs.permute(0, 2, 1)\r\n",
        "\t\t\tphi_xs = phi_xs.view(b, self.inter_channel, -1)\r\n",
        "\t\t\tGs = torch.matmul(theta_xs, phi_xs)\r\n",
        "\t\t\tGs_in = Gs.permute(0, 2, 1).view(b, h*w, h, w)\r\n",
        "\t\t\tGs_out = Gs.view(b, h*w, h, w)\r\n",
        "\t\t\tGs_joint = torch.cat((Gs_in, Gs_out), 1)\r\n",
        "\t\t\tGs_joint = self.gg_spatial(Gs_joint)\r\n",
        "\t\t\r\n",
        "\t\t\tg_xs = self.gx_spatial(x)\r\n",
        "\t\t\tg_xs = torch.mean(g_xs, dim=1, keepdim=True)\r\n",
        "\t\t\tys = torch.cat((g_xs, Gs_joint), 1)\r\n",
        "\r\n",
        "\t\t\tW_ys = self.W_spatial(ys)\r\n",
        "\t\t\tif not self.use_channel:\r\n",
        "\t\t\t\tout = F.sigmoid(W_ys.expand_as(x)) * x\r\n",
        "\t\t\t\treturn out\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\tx = F.sigmoid(W_ys.expand_as(x)) * x\r\n",
        "\r\n",
        "\t\tif self.use_channel:\r\n",
        "\t\t\t# channel attention\r\n",
        "\t\t\txc = x.view(b, c, -1).permute(0, 2, 1).unsqueeze(-1)\r\n",
        "\t\t\ttheta_xc = self.theta_channel(xc).squeeze(-1).permute(0, 2, 1)\r\n",
        "\t\t\tphi_xc = self.phi_channel(xc).squeeze(-1)\r\n",
        "\t\t\tGc = torch.matmul(theta_xc, phi_xc)\r\n",
        "\t\t\tGc_in = Gc.permute(0, 2, 1).unsqueeze(-1)\r\n",
        "\t\t\tGc_out = Gc.unsqueeze(-1)\r\n",
        "\t\t\tGc_joint = torch.cat((Gc_in, Gc_out), 1)\r\n",
        "\t\t\tGc_joint = self.gg_channel(Gc_joint)\r\n",
        "\r\n",
        "\t\t\tg_xc = self.gx_channel(xc)\r\n",
        "\t\t\tg_xc = torch.mean(g_xc, dim=1, keepdim=True)\r\n",
        "\t\t\tyc = torch.cat((g_xc, Gc_joint), 1)\r\n",
        "\r\n",
        "\t\t\tW_yc = self.W_channel(yc).transpose(1, 2)\r\n",
        "\t\t\tout = F.sigmoid(W_yc) * x\r\n",
        "\r\n",
        "\t\t\treturn out\r\n",
        "\r\n",
        "\r\n",
        "def weights_init_kaiming(m):\r\n",
        "\tclassname = m.__class__.__name__\r\n",
        "\tif classname.find('Linear') != -1:\r\n",
        "\t\tnn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\r\n",
        "\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\telif classname.find('Conv') != -1:\r\n",
        "\t\tnn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\r\n",
        "\t\tif m.bias is not None:\r\n",
        "\t\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\telif classname.find('BatchNorm') != -1:\r\n",
        "\t\tif m.affine:\r\n",
        "\t\t\tnn.init.normal_(m.weight, 1.0, 0.02)\r\n",
        "\t\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "\r\n",
        "def weights_init_fc(m):\r\n",
        "\tclassname = m.__class__.__name__\r\n",
        "\tif classname.find('Linear') != -1:\r\n",
        "\t\tnn.init.normal_(m.weight, std=0.001)\r\n",
        "\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\telif classname.find('BatchNorm') != -1:\r\n",
        "\t\tif m.affine:\r\n",
        "\t\t\tnn.init.normal_(m.weight, 1.0, 0.02)\r\n",
        "\t\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "\texpansion = 4\r\n",
        "\r\n",
        "\tdef __init__(self, in_channels, out_channels, stride=1, downsample=None):\r\n",
        "\t\tsuper(Bottleneck, self).__init__()\r\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\r\n",
        "\t\tself.bn1 = nn.BatchNorm2d(out_channels)\r\n",
        "\t\tself.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride,\r\n",
        "\t\t\t\t\t\t\t   padding=1, bias=False)\r\n",
        "\t\tself.bn2 = nn.BatchNorm2d(out_channels)\r\n",
        "\t\tself.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, bias=False)\r\n",
        "\t\tself.bn3 = nn.BatchNorm2d(out_channels * 4)\r\n",
        "\t\tself.relu = nn.ReLU(inplace=True)\r\n",
        "\t\tself.downsample = downsample\r\n",
        "\t\tself.stride = stride\r\n",
        "\r\n",
        "\tdef forward(self, x):\r\n",
        "\t\tresidual = x\r\n",
        "\r\n",
        "\t\tout = self.conv1(x)\r\n",
        "\t\tout = self.bn1(out)\r\n",
        "\t\tout = self.relu(out)\r\n",
        "\r\n",
        "\t\tout = self.conv2(out)\r\n",
        "\t\tout = self.bn2(out)\r\n",
        "\t\tout = self.relu(out)\r\n",
        "\r\n",
        "\t\tout = self.conv3(out)\r\n",
        "\t\tout = self.bn3(out)\r\n",
        "\r\n",
        "\t\tif self.downsample is not None:\r\n",
        "\t\t\tresidual = self.downsample(x)\r\n",
        "\r\n",
        "\t\tout += residual\r\n",
        "\t\tout = self.relu(out)\r\n",
        "\r\n",
        "\t\treturn out\r\n",
        "\r\n",
        "\r\n",
        "class RGA_Branch(nn.Module):\r\n",
        "    def __init__(self,  last_stride=1, block=Bottleneck, layers=[3, 4, 6, 3],\r\n",
        "        spa_on=True, cha_on=True, s_ratio=8, c_ratio=8, d_ratio=8, height=256, width=128):\r\n",
        "        super(RGA_Branch, self).__init__()\r\n",
        "\r\n",
        "        self.in_channels = 64\r\n",
        "\r\n",
        "            # Networks\r\n",
        "        backbone=resnet50(True)\r\n",
        "        backbone.layer4[0].downsample[0].stride=1\r\n",
        "        backbone.layer4[0].conv2.stride=1\r\n",
        "        self.conv1=backbone.conv1\r\n",
        "        self.bn1=backbone.bn1\r\n",
        "        self.relu=backbone.relu\r\n",
        "        self.maxpool=backbone.maxpool\r\n",
        "        self.layer1=backbone.layer1\r\n",
        "        self.layer2=backbone.layer2\r\n",
        "        self.layer3=backbone.layer3\r\n",
        "        self.layer4=backbone.layer4\r\n",
        "        \r\n",
        "\r\n",
        "        # RGA Modules\r\n",
        "        self.rga_att1 = RGA_Module(256, (height//4)*(width//4), use_spatial=spa_on, use_channel=cha_on,\r\n",
        "                                cha_ratio=c_ratio, spa_ratio=s_ratio, down_ratio=d_ratio)\r\n",
        "        self.rga_att2 = RGA_Module(512, (height//8)*(width//8), use_spatial=spa_on, use_channel=cha_on,\r\n",
        "                                cha_ratio=c_ratio, spa_ratio=s_ratio, down_ratio=d_ratio)\r\n",
        "        self.rga_att3 = RGA_Module(1024, (height//16)*(width//16), use_spatial=spa_on, use_channel=cha_on,\r\n",
        "                                cha_ratio=c_ratio, spa_ratio=s_ratio, down_ratio=d_ratio)\r\n",
        "        self.rga_att4 = RGA_Module(2048, (height//16)*(width//16), use_spatial=spa_on, use_channel=cha_on,\r\n",
        "                                cha_ratio=c_ratio, spa_ratio=s_ratio, down_ratio=d_ratio)\r\n",
        "        \r\n",
        "        # Load the pre-trained model weights\r\n",
        "\r\n",
        "\r\n",
        "    def load_partial_param(self, state_dict, model_index, model_path):\r\n",
        "        param_dict = torch.load(model_path)\r\n",
        "        for i in state_dict:\r\n",
        "            key = 'layer{}.'.format(model_index)+i\r\n",
        "            state_dict[i].copy_(param_dict[key])\r\n",
        "        del param_dict\r\n",
        "\r\n",
        "    def load_specific_param(self, state_dict, param_name, model_path):\r\n",
        "        param_dict = torch.load(model_path)\r\n",
        "        for i in state_dict:\r\n",
        "            key = param_name + '.' + i\r\n",
        "            state_dict[i].copy_(param_dict[key])\r\n",
        "        del param_dict\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      \r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.maxpool(x)\r\n",
        "        x = self.layer1(x)\r\n",
        "\r\n",
        "        x = self.rga_att1(x)\r\n",
        "\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.rga_att2(x)\r\n",
        "        \r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.rga_att3(x)\r\n",
        "\r\n",
        "        x = self.layer4(x)\r\n",
        "    \r\n",
        "        x = self.rga_att4(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "        \r\n",
        "\r\n",
        "class ResNet50_RGA_Model(nn.Module):\r\n",
        "    '''\r\n",
        "    Backbone: ResNet-50 + RGA modules.\r\n",
        "    '''\r\n",
        "    def __init__(self,  num_feat=2048, height=384, width=128, \r\n",
        "        dropout=0, num_classes=0, last_stride=1, branch_name='rgasc', scale=8, d_scale=8):\r\n",
        "        super(ResNet50_RGA_Model, self).__init__()\r\n",
        "    \r\n",
        "        self.num_feat = num_feat\r\n",
        "        self.dropout = dropout\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.branch_name = branch_name\r\n",
        "    \r\n",
        "        if 'rgasc' in branch_name:\r\n",
        "            spa_on=True \r\n",
        "            cha_on=True\r\n",
        "        elif 'rgas' in branch_name:\r\n",
        "            spa_on=True\r\n",
        "            cha_on=False\r\n",
        "        elif 'rgac' in branch_name:\r\n",
        "            spa_on=False\r\n",
        "            cha_on=True\r\n",
        "        else:\r\n",
        "            raise NameError\r\n",
        "        \r\n",
        "        self.backbone = RGA_Branch(last_stride=last_stride, \r\n",
        "                        spa_on=spa_on, cha_on=cha_on, height=height, width=width,\r\n",
        "                        s_ratio=scale, c_ratio=scale, d_ratio=d_scale)\r\n",
        "\r\n",
        "        self.feat_bn = nn.BatchNorm1d(self.num_feat)\r\n",
        "        self.feat_bn.bias.requires_grad_(False)\r\n",
        "        if self.dropout > 0:\r\n",
        "            self.drop = nn.Dropout(self.dropout)\r\n",
        "        self.cls = nn.Linear(self.num_feat, self.num_classes, bias=False)\r\n",
        "\r\n",
        "        self.feat_bn.apply(weights_init_kaiming)\r\n",
        "        self.cls.apply(weights_init_classifier)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, inputs, training=True):\r\n",
        "     \r\n",
        "        feat_ = self.backbone(inputs)\r\n",
        "        feat_ = F.avg_pool2d(feat_, feat_.size()[2:]).view(feat_.size(0), -1)\r\n",
        "        feat = self.feat_bn(feat_)\r\n",
        "        if self.dropout > 0:\r\n",
        "            feat = self.drop(feat)\r\n",
        "        if training and self.num_classes is not None:\r\n",
        "            cls_feat = self.cls(feat)\r\n",
        "        out={}\r\n",
        "        out['feat']=feat\r\n",
        "        out['logit']=cls_feat\r\n",
        "        return out\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHa_TXIgG3FB"
      },
      "source": [
        "###<font color=green>***relation network***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h42dtt0JOp2p"
      },
      "source": [
        "\r\n",
        "def weights_init_kaiming(m):\r\n",
        "\tclassname = m.__class__.__name__\r\n",
        "\tif classname.find('Linear') != -1:\r\n",
        "\t\tnn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\r\n",
        "\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\telif classname.find('Conv') != -1:\r\n",
        "\t\tnn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\r\n",
        "\t\tif m.bias is not None:\r\n",
        "\t\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\telif classname.find('BatchNorm') != -1:\r\n",
        "\t\tif m.affine:\r\n",
        "\t\t\tnn.init.normal_(m.weight, 1.0, 0.02)\r\n",
        "\t\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "\r\n",
        "def weights_init_fc(m):\r\n",
        "\tclassname = m.__class__.__name__\r\n",
        "\tif classname.find('Linear') != -1:\r\n",
        "\t\tnn.init.normal_(m.weight, std=0.001)\r\n",
        "\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "\telif classname.find('BatchNorm') != -1:\r\n",
        "\t\tif m.affine:\r\n",
        "\t\t\tnn.init.normal_(m.weight, 1.0, 0.02)\r\n",
        "\t\t\tnn.init.constant_(m.bias, 0.0)\r\n",
        "class RelationModel(nn.Module):\r\n",
        "  def __init__(self,cfg,num_stripes,num_classes):\r\n",
        "    super(RelationModel,self).__init__()\r\n",
        "    if cfg['backbone']=='resnet50':\r\n",
        "          self.base,in_planes=ResNet50(cfg)\r\n",
        "    if cfg['backbone']=='resnet101':\r\n",
        "          self.base,in_planes=ResNet101(cfg) \r\n",
        "    if cfg['backbone']=='eff_b3':\r\n",
        "          self.base,in_planes=Eff_b3(cfg)  \r\n",
        "    if cfg['backbone']=='eff_b5':\r\n",
        "          self.base,in_planes=Eff_b5(cfg)\r\n",
        "    if cfg['backbone']=='eff_b7':\r\n",
        "          self.base,in_planes=Eff_b7(cfg)\r\n",
        "    if cfg['backbone']=='eff_b6':\r\n",
        "          self.base,in_planes=Eff_b6(cfg)  \r\n",
        "    self.num_classes=num_classes\r\n",
        "    self.num_stripes=num_stripes\r\n",
        "    out_channel=int(in_planes/8)\r\n",
        "    self.local_conv_list=nn.ModuleList()\r\n",
        "    self.rest_conv_list=nn.ModuleList()\r\n",
        "    self.relation_conv_list=nn.ModuleList()\r\n",
        "    for i in range(num_stripes):\r\n",
        "      self.local_conv_list.append(\r\n",
        "          nn.Sequential(\r\n",
        "              nn.Conv2d(in_planes,out_channel,1),\r\n",
        "              nn.BatchNorm2d(out_channel),\r\n",
        "              nn.Relu(inplace=True)\r\n",
        "          )\r\n",
        "      ) \r\n",
        "    for i in range(num_stripes):\r\n",
        "      self.rest_conv_list.append(\r\n",
        "          nn.Sequential(\r\n",
        "              nn.Conv2d(in_planes,out_channel,1),\r\n",
        "              nn.BatchNorm2d(out_channel),\r\n",
        "              nn.Relu(inplace=True)\r\n",
        "          )\r\n",
        "      ) \r\n",
        "    for i in range(num_stripes):\r\n",
        "      self.relation_conv_list.append(\r\n",
        "          nn.Sequential(\r\n",
        "              nn.Conv2d(2*in_planes,out_channel,1),\r\n",
        "              nn.BatchNorm2d(out_channel),\r\n",
        "              nn.Relu(inplace=True)\r\n",
        "          )\r\n",
        "      ) \r\n",
        "    \r\n",
        "    self.global_max_conv=nn.Sequential(\r\n",
        "              nn.Conv2d(in_planes,out_channel,1),\r\n",
        "              nn.BatchNorm2d(out_channel),\r\n",
        "              nn.Relu(inplace=True)\r\n",
        "          )\r\n",
        "    self.global_rest_conv=nn.Sequential(\r\n",
        "              nn.Conv2d(in_planes,out_channel,1),\r\n",
        "              nn.BatchNorm2d(out_channel),\r\n",
        "              nn.Relu(inplace=True)\r\n",
        "          )\r\n",
        "    self.global_pooling_conv=nn.Sequential(\r\n",
        "              nn.Conv2d(2*in_planes,out_channel,1),\r\n",
        "              nn.BatchNorm2d(out_channel),\r\n",
        "              nn.Relu(inplace=True)\r\n",
        "          )\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "    self.fc_local_list=nn.ModuleList()\r\n",
        "    self.fc_rest_list=nn.ModuleList()\r\n",
        "    self.fc_relation_list=nn.ModuleList()\r\n",
        "    for _ in range(num_stripes):\r\n",
        "        fc = nn.Linear(out_channel, num_classes)\r\n",
        "        fc.apply(weights_init_fc)\r\n",
        "        self.fc_local_list.append(fc)\r\n",
        "    for _ in range(num_stripes):\r\n",
        "        fc = nn.Linear(out_channel, num_classes)\r\n",
        "        fc.apply(weights_init_fc)\r\n",
        "        self.fc_rest_list.append(fc)\r\n",
        "    for _ in range(num_stripes):\r\n",
        "        fc = nn.Linear(out_channel, num_classes)\r\n",
        "        fc.apply(weights_init_fc)\r\n",
        "        self.fc_relation_list.append(fc)\r\n",
        "    \r\n",
        "    self.fc_max=nn.Linear(out_channel, num_classes)\r\n",
        "    self.fc_max.apply(weights_init_fc)\r\n",
        "\r\n",
        "    self.fc_rest=nn.Linear(out_channel, num_classes)\r\n",
        "    self.fc_rest.apply(weights_init_fc)\r\n",
        "\r\n",
        "    self.fc_pooling=nn.Linear(out_channel, num_classes)\r\n",
        "    self.fc_pooling.apply(weights_init_fc)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    feat=self.base(x)\r\n",
        "    out=[]\r\n",
        "    if self.training:\r\n",
        "      stripe_h=int(feat.size(2) / self.num_stripes)\r\n",
        "      local_feat_list=[]\r\n",
        "      other_feat_list=[]\r\n",
        "      final_feat_list=[]\r\n",
        "      logit_list=[]\r\n",
        "      rest_feat_list=[]\r\n",
        "      \r\n",
        "      for i in range(self.num_stripes): # 得到6块中每一个的特征\r\n",
        "        local_feat = F.max_pool2d(\r\n",
        "                  feat[:, :, i * stripe_h: (i + 1) * stripe_h, :], # 每一块是4*w\r\n",
        "                  (stripe_h, feat.size(-1))) #pool成1*1的\r\n",
        "              #print(local_6_feat.shape) #8 2048 1 1\r\n",
        "              \r\n",
        "        local_feat_list.append(local_feat)\r\n",
        "\r\n",
        "      for i in range(self.num_stripes): #对于每块特征，除去自己之后其他的特征组合在一起\r\n",
        "              \r\n",
        "              rest_feat_list.append((local_feat_list[(i+1)%self.num_stripes]#论文公式1处的ri \r\n",
        "                                    + local_feat_list[(i+2)%self.num_stripes]\r\n",
        "                                    + local_feat_list[(i+3)%self.num_stripes] \r\n",
        "                                    + local_feat_list[(i+4)%self.num_stripes]\r\n",
        "                                    + local_feat_list[(i+5)%self.num_stripes])/5)\r\n",
        "              \r\n",
        "      global_max_feat = F.max_pool2d(feat, (feat.size(2), feat.size(3)))\r\n",
        "      global_rest_feat = (local_feat_list[0] + local_feat_list[1] + local_feat_list[2]#局部与全局的差异 \r\n",
        "                                + local_feat_list[3] + local_feat_list[4] + local_feat_list[5] - global_max_feat)/5\r\n",
        "      global_max_feat = self.global_max_conv(global_max_feat)\r\n",
        "      global_rest_feat=self.global_rest_conv(global_rest_feat)\r\n",
        "\r\n",
        "      global_pooling_feat=self.global_pooling_conv(torch.cat((global_max_feat,global_rest_feat),1))\r\n",
        "      \r\n",
        "      \r\n",
        "\r\n",
        "      global_feat=(global_pooling_feat+global_max_feat)/2\r\n",
        "      final_feat_list.append(global_feat)\r\n",
        "      other_feat_list.append(global_max_feat)\r\n",
        "      other_feat_list.append(global_rest_feat)\r\n",
        "\r\n",
        "      global_max_logit=self.fc_max(global_max_feat)\r\n",
        "      global_rest_logit=self.fc_rest(global_rest_feat)\r\n",
        "      global_pooling_logit=self.fc_pooling(global_feat)\r\n",
        "      logit_list.append(global_max_logit)\r\n",
        "      logit_list.append(global_rest_logit)\r\n",
        "      logit_list.append(global_pooling_logit)\r\n",
        "      \r\n",
        "      for i in range(self.num_stripes):\r\n",
        "        local_feat=self.local_conv_list[i](local_feat_list[i]).squeeze(3).squeeze(2)\r\n",
        "        local_rest_feat=self.rest_conv_list[i](rest_feat_list[i]).squeeze(3).squeeze(2)\r\n",
        "        relation_feat=self.relation_conv_list[i](torch.cat((local_feat,local_rest_feat),1).unsqueeze(2).unsqueeze(3))\r\n",
        "        relation_feat=(relation_feat+local_feat.unsqueeze(2).unsqueeze(3)).squeeze(3).squeeze(2)\r\n",
        "\r\n",
        "        final_feat_list.append(relation_feat)\r\n",
        "        other_feat_list.append(local_feat)\r\n",
        "        other_feat_list.append(local_rest_feat)\r\n",
        "\r\n",
        "        local_logits=self.fc_local_list[i](local_feat)\r\n",
        "        rest_logit=self.fc_rest_list[i](local_rest_feat)\r\n",
        "        relation_logit=self.fc_relation_list[i](relation_feat)\r\n",
        "\r\n",
        "        logit_list.append(local_logits)\r\n",
        "        logit_list.append(rest_logit)\r\n",
        "        logit_list.append(relation_logit)\r\n",
        "    \r\n",
        "    \r\n",
        "      final_feat_list.extend(other_feat_list)\r\n",
        "      out['feat']=final_feat_list\r\n",
        "      out['logit']=logit_list\r\n",
        "    else:\r\n",
        "      output_feat=0\r\n",
        "      for i in range(self.num_stripes): # 得到6块中每一个的特征\r\n",
        "        local_feat = F.max_pool2d(\r\n",
        "                  feat[:, :, i * stripe_h: (i + 1) * stripe_h, :], # 每一块是4*w\r\n",
        "                  (stripe_h, feat.size(-1))) #pool成1*1的\r\n",
        "              #print(local_6_feat.shape) #8 2048 1 1\r\n",
        "              \r\n",
        "        local_feat_list.append(local_feat)\r\n",
        "\r\n",
        "      for i in range(self.num_stripes): #对于每块特征，除去自己之后其他的特征组合在一起\r\n",
        "              \r\n",
        "              rest_feat_list.append((local_feat_list[(i+1)%self.num_stripes]#论文公式1处的ri \r\n",
        "                                    + local_feat_list[(i+2)%self.num_stripes]\r\n",
        "                                    + local_feat_list[(i+3)%self.num_stripes] \r\n",
        "                                    + local_feat_list[(i+4)%self.num_stripes]\r\n",
        "                                    + local_feat_list[(i+5)%self.num_stripes])/5)\r\n",
        "              \r\n",
        "      global_max_feat = F.max_pool2d(feat, (feat.size(2), feat.size(3)))\r\n",
        "      global_rest_feat = (local_feat_list[0] + local_feat_list[1] + local_feat_list[2]#局部与全局的差异 \r\n",
        "                                + local_feat_list[3] + local_feat_list[4] + local_feat_list[5] - global_max_feat)/5\r\n",
        "      global_max_feat = self.global_max_conv(global_max_feat)\r\n",
        "      global_rest_feat=self.global_rest_conv(global_rest_feat)\r\n",
        "\r\n",
        "      global_pooling_feat=self.global_pooling_conv(torch.cat(global_max_feat,global_rest_feat,1))\r\n",
        "      \r\n",
        "      \r\n",
        "\r\n",
        "      global_feat=(global_pooling_feat+global_max_feat)/2\r\n",
        "      output_feat=global_feat\r\n",
        "      for i in range(self.num_stripes):\r\n",
        "        local_feat=self.local_conv_list[i](local_feat_list[i]).squeeze(3).squeeze(2)\r\n",
        "        local_rest_feat=self.rest_conv_list[i](rest_feat_list[i]).squeeze(3).squeeze(2)\r\n",
        "        relation_feat=self.relation_conv_list[i](torch.cat(local_feat,local_rest_feat,1).unsqueeze(2).unsqueeze(3))\r\n",
        "        relation_feat=((relation_feat+local_feat.unsqueeze(2).unsqueeze(3)).squeeze(3).squeeze(2))/2\r\n",
        "        output_feat=torch.cat((output_fet,relation_feat),1)\r\n",
        "\r\n",
        "      out['feat']=[output_feat]\r\n",
        "    return out\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtwWsO03jp5P"
      },
      "source": [
        "#<font color=red>***metric***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2PVazPOjtDY"
      },
      "source": [
        "##<font color=blue>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr1C6ee-kvTT"
      },
      "source": [
        "###<font color=green>***普通map***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4QtUvoajnuC"
      },
      "source": [
        "\r\n",
        "'''reid计算map函数'''\r\n",
        "def MARKET_EVAL_FUNC(dismat,q_pids,g_pids,q_camids,g_camids,max_rank=50):\r\n",
        "    '''\r\n",
        "    :param dismat: 每个query与其对应查询结果的距离\r\n",
        "    :param q_pids: 每个query行人的id\r\n",
        "    :param g_pids: 每个查询结果的行人的id\r\n",
        "    :param q_camids: 摄像头id，对于同一个query，如果查询结果与query都来自一个摄像头，则丢弃\r\n",
        "    :param g_camids: 同上\r\n",
        "    :param max_rank:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    num_q,num_g=dismat.shape\r\n",
        "    if num_g<max_rank:\r\n",
        "        max_rank=num_g\r\n",
        "\r\n",
        "    ##根据相似度进行排序\r\n",
        "    indices=np.argsort(dismat,axis=1)\r\n",
        "    \r\n",
        "    \r\n",
        "    ##在排序结果找到同id图片，用于计算cmc\r\n",
        "    matchs=(g_pids[indices]==q_pids[:,np.newaxis]).astype(np.int32)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    all_cmc=[]\r\n",
        "    all_AP=[]\r\n",
        "    num_valid_q=0\r\n",
        "\r\n",
        "    for q_idx in range(num_q):\r\n",
        "        q_pid=q_pids[q_idx]\r\n",
        "        q_camid=q_camids[q_idx]\r\n",
        "\r\n",
        "        '''当前query对应查询结果的排序'''\r\n",
        "        order=indices[q_idx]\r\n",
        "\r\n",
        "        '''若查询结果与query同摄像机且同id，删除'''\r\n",
        "        remove=(g_pids[order]==q_pid)&(g_camids[order]==q_camid)\r\n",
        "        keep=np.invert(remove)\r\n",
        "\r\n",
        "        '''计算CMC'''\r\n",
        "\r\n",
        "        '''首先先筛选'''\r\n",
        "        orig_cmc=matchs[q_idx][keep]\r\n",
        "\r\n",
        "        '''全是false'''\r\n",
        "        if not np.any(orig_cmc):\r\n",
        "            continue\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "        '''累加和，因为orig_cmc里面是true，false，true为1，得到cmc列表'''\r\n",
        "        cmc=orig_cmc.cumsum()\r\n",
        "        cmc[cmc>1]=1\r\n",
        "        all_cmc.append(cmc[:max_rank])\r\n",
        "        num_valid_q+=1\r\n",
        "\r\n",
        "        '''计算map'''\r\n",
        "\r\n",
        "        '''预测正确的数量'''\r\n",
        "        num_rel=orig_cmc.sum()\r\n",
        "\r\n",
        "        '''累加和，用于计算'''\r\n",
        "        tmp_cmc=orig_cmc.cumsum()\r\n",
        "\r\n",
        "        '''计算准确率，前k个查询结果的准确率'''\r\n",
        "        tmp_cmc=[x/(i+1) for i,x in enumerate(tmp_cmc)]\r\n",
        "\r\n",
        "        '''计算召回率，召回率每次变化都是当查询结果id等于query时，故乘orig_cmc'''\r\n",
        "        '''最终的结果类似 0,0,1,0,0,2,0,0,3这种，每个有值的都是查询正确的'''\r\n",
        "        '''然后后面除以查询结果中同id数量，也就是上面num_rel就是找召回率'''\r\n",
        "        '''但是这里其实已经提前乘上准确率了，最终结果类似于 0,0,1/3,0,0,2/6,0,0,3/9'''\r\n",
        "        tmp_cmc=np.asarray(tmp_cmc)*orig_cmc\r\n",
        "\r\n",
        "        '''以上为例，除以同id数量后，假设为五个'''\r\n",
        "        '''0，0，1/3 /5，0，0，2/6 /5，0，0，3/9 /5'''\r\n",
        "        '''等价于 0,0, 1/3(准确率)*1/5(召回率) ....'''\r\n",
        "        \r\n",
        "        if num_rel==0:\r\n",
        "          AP=0\r\n",
        "          print('AP=0')\r\n",
        "        else:  \r\n",
        "          AP = tmp_cmc.sum() / num_rel\r\n",
        "          \r\n",
        "        all_AP.append(AP)\r\n",
        "\r\n",
        "    all_cmc = np.asarray(all_cmc).astype(np.float32)\r\n",
        "    all_cmc = all_cmc.sum(0) / num_valid_q\r\n",
        "    if len(all_AP)==0:\r\n",
        "      print('all_ap=0')\r\n",
        "      return all_cmc,0\r\n",
        "    mAP = np.mean(all_AP)\r\n",
        "\r\n",
        "    return all_cmc, mAP\r\n",
        "\r\n",
        "\r\n",
        "class MARKET_MAP():\r\n",
        "    def __init__(self,num_query,max_rank=50,feat_norm=True,one_day=True,all=False,date_length=11):\r\n",
        "        '''\r\n",
        "\r\n",
        "        :param num_query: 查询数量\r\n",
        "        :param max_rank: 每个query限定的最大查询结果数量\r\n",
        "        :param feat_norm:是否对特征进行l2norm\r\n",
        "        '''\r\n",
        "        self.num_query=num_query\r\n",
        "        self.max_rank=max_rank\r\n",
        "        self.feat_norm=feat_norm\r\n",
        "        self.one_day=one_day\r\n",
        "        self.all=all\r\n",
        "        self.date_length=date_length\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        '''\r\n",
        "        feats:模型返回的特征\r\n",
        "        pids: 图片中行人的id\r\n",
        "        camids:图片摄像头的id\r\n",
        "        :return:\r\n",
        "        '''\r\n",
        "        self.feats=[]\r\n",
        "        self.pids=[]\r\n",
        "        self.camids=[]\r\n",
        "        self.dates=[]\r\n",
        "\r\n",
        "    def update(self,output):\r\n",
        "        feat,pid,camid,date=output\r\n",
        "    \r\n",
        "        self.feats.append(feat)\r\n",
        "        self.pids.append(pid)\r\n",
        "        self.camids.append(camid)\r\n",
        "        self.dates.append(date)\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        feats=torch.stack(self.feats,dim=0)\r\n",
        "          \r\n",
        "        if self.feat_norm:\r\n",
        "            feats=torch.nn.functional.normalize(feats,p=2)\r\n",
        "\r\n",
        "        \r\n",
        "        '''用于查询的图片的特征及其id和摄像头id'''\r\n",
        "        \r\n",
        "        qf=feats[:self.num_query]\r\n",
        "        q_pids=np.asarray(self.pids[:self.num_query])\r\n",
        "        q_camids = np.asarray(self.camids[:self.num_query])\r\n",
        "        q_dates=np.asarray(self.dates[:self.num_query])  \r\n",
        "\r\n",
        "        '''每个query查询结果的特征及其id和摄像头id'''\r\n",
        "        gf = feats[self.num_query:]\r\n",
        "        g_pids = np.asarray(self.pids[self.num_query:])\r\n",
        "        g_camids = np.asarray(self.camids[self.num_query:])\r\n",
        "        g_dates=np.asarray(self.dates[self.num_query:])\r\n",
        "        if self.all:\r\n",
        "          m,n=qf.shape[0],gf.shape[0]\r\n",
        "      \r\n",
        "          '''计算qf每个query和对应查询结果的距离'''\r\n",
        "          distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                    torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "        \r\n",
        "          distmat.addmm_(1, -2, qf, gf.t())\r\n",
        "          distmat = distmat.cpu().numpy()\r\n",
        "        \r\n",
        "          cmc,mAP=MARKET_EVAL_FUNC(distmat,q_pids,g_pids,q_camids,g_camids)\r\n",
        "          return mAP\r\n",
        "        else:\r\n",
        "          if self.one_day:\r\n",
        "            \r\n",
        "            mean_map=0\r\n",
        "            count=0\r\n",
        "            for date in range(self.date_length):\r\n",
        "              date=date+1\r\n",
        "              q_index=np.where(q_dates==date)\r\n",
        "              \r\n",
        "              date_qf=qf[q_index]\r\n",
        "              date_q_pids=q_pids[q_index]\r\n",
        "              date_q_camids=q_camids[q_index]\r\n",
        "\r\n",
        "              g_index=np.where(g_dates==date)\r\n",
        "              date_gf=gf[g_index]\r\n",
        "              date_g_pids=g_pids[g_index]\r\n",
        "              date_g_camids=g_camids[g_index]\r\n",
        "              m,n=date_qf.shape[0],date_gf.shape[0]\r\n",
        "\r\n",
        "              if m==0:\r\n",
        "                continue\r\n",
        "              distmat = torch.pow(date_qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                    torch.pow(date_gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "              \r\n",
        "              distmat.addmm_(1, -2, date_qf, date_gf.t())\r\n",
        "              distmat = distmat.cpu().numpy()\r\n",
        "              \r\n",
        "              cmc,mAP=MARKET_EVAL_FUNC(distmat,date_q_pids,date_g_pids,date_q_camids,date_g_camids)\r\n",
        "              mean_map+=mAP\r\n",
        "              count+=1\r\n",
        "            \r\n",
        "            if count==0:\r\n",
        "              return 0\r\n",
        "            return mean_map/count\r\n",
        "          else:\r\n",
        "           \r\n",
        "            mean_map=0\r\n",
        "            count=0\r\n",
        "            for date in range(self.date_length):\r\n",
        "              date=date+1\r\n",
        "              q_index=np.where(q_dates==date)\r\n",
        "              \r\n",
        "              date_qf=qf[q_index]\r\n",
        "              date_q_pids=q_pids[q_index]\r\n",
        "              date_q_camids=q_camids[q_index]\r\n",
        "\r\n",
        "              g_index=np.where(g_dates!=date)\r\n",
        "              date_gf=gf[g_index]\r\n",
        "              date_g_pids=g_pids[g_index]\r\n",
        "              date_g_camids=g_camids[g_index]\r\n",
        "              m,n=date_qf.shape[0],date_gf.shape[0]\r\n",
        "\r\n",
        "              if m==0 or n==0:\r\n",
        "                continue\r\n",
        "              distmat = torch.pow(date_qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                    torch.pow(date_gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "              \r\n",
        "              distmat.addmm_(1, -2, date_qf, date_gf.t())\r\n",
        "              distmat = distmat.cpu().numpy()\r\n",
        "              \r\n",
        "              cmc,mAP=MARKET_EVAL_FUNC(distmat,date_q_pids,date_g_pids,date_q_camids,date_g_camids)\r\n",
        "              mean_map+=mAP\r\n",
        "              count+=1\r\n",
        "            if count==0:\r\n",
        "              return 0\r\n",
        "            return mean_map/count\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GcAv-6SlFfh"
      },
      "source": [
        "###<font color=green>***rerank map***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLgr8-fllG_a"
      },
      "source": [
        "def RE_RANKING(porbFea,galFea,k1,k2,lambda_value,local_distmat=None,only_local=False):\r\n",
        "    '''\r\n",
        "    :param porbFea:query特征\r\n",
        "    :param galFea: 查询结果特征\r\n",
        "    :param k1: 一开始k近邻的k\r\n",
        "    :param k2:\r\n",
        "    :param lambda_value:\r\n",
        "    :param locl_distmat:\r\n",
        "    :param only_loca:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    query_num=porbFea.shape[0]\r\n",
        "    all_num=query_num+galFea.shape[0]\r\n",
        "    if only_local:\r\n",
        "        original_dist=local_distmat\r\n",
        "    else:\r\n",
        "        '''把query加进去，然后算每个图片之间的距离，用于后面reranking'''\r\n",
        "        feat=torch.cat([porbFea,galFea])\r\n",
        "        distmat = torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num) + \\\r\n",
        "                  torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()\r\n",
        "        distmat.addmm_(1, -2, feat, feat.t())\r\n",
        "        original_dist = distmat.cpu().numpy()\r\n",
        "        del feat\r\n",
        "        if not local_distmat is None:\r\n",
        "            original_dist = original_dist + local_distmat\r\n",
        "    gallery_num=original_dist.shape[0]\r\n",
        "\r\n",
        "    '''进行归一化，然后进行转置，得到归一化的距离矩阵'''\r\n",
        "    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))\r\n",
        "\r\n",
        "    V=np.zeros_like(original_dist).astype(np.float16)\r\n",
        "\r\n",
        "    '''初始rank列表就是直接根据距离最小值'''\r\n",
        "    initial_rank=np.argsort(original_dist).astype(np.int32)\r\n",
        "\r\n",
        "    '''进行重排序'''\r\n",
        "    '''最核心的思想就是对于query，前k个距离最近的查询结果，每一个查询结果在所有图片中的k近邻\r\n",
        "    应该包含query，根据这个进行重排，使得rerank后尽量满足这个条件'''\r\n",
        "    for i in range(all_num):\r\n",
        "        '''选择当前图片的k近邻'''\r\n",
        "        forward_k_neigh_index=initial_rank[i,:k1+1]\r\n",
        "\r\n",
        "        '''选择k近邻的k近邻'''\r\n",
        "        backward_k_neigh_index=initial_rank[forward_k_neigh_index,:k1+1]\r\n",
        "\r\n",
        "        '''满足上面的条件，k近邻的k近邻包含当前图片'''\r\n",
        "        fi=np.where(backward_k_neigh_index==i)[0]\r\n",
        "\r\n",
        "        '''那么满足上面条件的k近邻作为rerank的候选集'''\r\n",
        "        k_reciprocal_index=forward_k_neigh_index[fi]\r\n",
        "        k_reciprocal_expansion_index = k_reciprocal_index\r\n",
        "\r\n",
        "        '''如果满足上面的条件，通常就是正样本无误了，这也就是rerank的目的'''\r\n",
        "        '''但是问题在于如果因为一些干扰，导致正样本在k1近邻之外，那rerank就没用'''\r\n",
        "        '''所以提出一个方法进行候选集的扩充，希望能够保证正样本进入候选集'''\r\n",
        "\r\n",
        "        '''基本的思想就是对于当前图片，我们找到了符合条件的原候选集'''\r\n",
        "        '''那么对于候选集每一个元素，找到以一半k1为长度的满足上面条件的候选集'''\r\n",
        "        '''如果这个候选集与原候选集的交集超过这个候选集的2/3长度，那么把这个候选集加入原候选集'''\r\n",
        "        for j in range(len(k_reciprocal_index)):\r\n",
        "            '''原候选集每个元素'''\r\n",
        "            candidate=k_reciprocal_index[j]\r\n",
        "            '''找到当前元素满足条件的候选集'''\r\n",
        "            candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]\r\n",
        "            candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index,\r\n",
        "                                               :int(np.around(k1 / 2)) + 1]\r\n",
        "            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]\r\n",
        "            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]\r\n",
        "\r\n",
        "            '''如果交集长度超过这个候选集的2/3长度'''\r\n",
        "            if len(np.intersect1d(candidate_k_reciprocal_index,k_reciprocal_index))>(2/3)*len(candidate_k_reciprocal_index):\r\n",
        "                k_reciprocal_expansion_index=np.append(k_reciprocal_expansion_index,candidate_k_reciprocal_index)\r\n",
        "        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\r\n",
        "\r\n",
        "        '''获取当前图片及其候选集的距离，进行exp'''\r\n",
        "        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])\r\n",
        "\r\n",
        "        '''找到每个图片与其候选集的一个权值表示，用于后面计算jaccard距离'''\r\n",
        "        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)\r\n",
        "    original_dist=original_dist[:query_num,]\r\n",
        "\r\n",
        "    '''这部分处理时针对线上reid系统，因为gallery通常是一直更新的，由于计算V的过程比较麻烦'''\r\n",
        "    '''我们只能隔一段时间计算，那么对于新加入的图片，其在V中没有值，所以采用这个近似方法来计算'''\r\n",
        "    if k2!=1:\r\n",
        "        V_qe=np.zeros_like(V,dtype=np.float16)\r\n",
        "        for i in range(all_num):\r\n",
        "            '''采用查询样本的特征均值代替上面的reciprocal编码，只是近似处理'''\r\n",
        "            V_qe[i,:]=np.mean(V[initial_rank[i,:k2],:],axis=0)\r\n",
        "        V=V_qe\r\n",
        "        del V_qe\r\n",
        "    del initial_rank\r\n",
        "    invIndex=[]\r\n",
        "    for i in range(gallery_num):\r\n",
        "        '''找到候选集包含当前图片的所有query'''\r\n",
        "        invIndex.append(np.where(V[:,i]!=0)[0])\r\n",
        "\r\n",
        "    '''计算jaccard距离'''\r\n",
        "    jaccard_dist=np.zeros_like(original_dist,dtype=np.float16)\r\n",
        "\r\n",
        "    '''jaccard距离的计算，对于query q和其中一个查询样本gi的距离，我们用1-其候选集的交并比来代表，如果交并比越大，代表两者'''\r\n",
        "    '''候选集很接近，说明两者很接近，则其距离就很小'''\r\n",
        "\r\n",
        "    '''为了方便表示交集和并集，我们建立Vq，其中V(q,gi)代表gi是否在q候选集中，如果在为1，否则为0，那么这个Vq就是针对所有图片，类似候选集的one-hot'''\r\n",
        "    '''Vgi同理，但是one-hot是hard编码，为了soft编码，我们使用上面np.exp(-距离）的计算'''\r\n",
        "\r\n",
        "    '''然后交集怎么表示呢？ sum(min(vq,vgi))，就是每个位置取Vq和Vgi的最小值,然后求和，为什么呢？我们以one-hot去想，交集意味着两者一样，那么对于相同位置，只有Vq和Vgi在这个位置\r\n",
        "    都为1，那么这个位置才为1，否则为0，所以求和的时候就是求1的个数，相当于求对应位置是相同的个数，这就是交集的定义'''\r\n",
        "\r\n",
        "    '''并集是sum(max(Vq,Vgi)),意义同上。'''\r\n",
        "\r\n",
        "    for i in range(query_num):\r\n",
        "        temp_min=np.zeros(shape=[1,gallery_num],dtype=np.float16)\r\n",
        "        '''找到当前图片的候选集'''\r\n",
        "        indNonZero=np.where(V[i,:]!=0)[0]\r\n",
        "\r\n",
        "        '''候选集的候选集'''\r\n",
        "        indImages=[invIndex[ind] for ind in indNonZero]\r\n",
        "        '''交集的计算'''\r\n",
        "        '''不断求和，看公式'''\r\n",
        "        for j in range(len(indNonZero)):\r\n",
        "               temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]],\r\n",
        "                                                                                       V[indImages[j], indNonZero[j]])\r\n",
        "        '''2-temp_min是并集的快速计算'''\r\n",
        "        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)\r\n",
        "\r\n",
        "\r\n",
        "    '''作为最终的距离矩阵'''\r\n",
        "    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value\r\n",
        "    del original_dist\r\n",
        "    del V\r\n",
        "    del jaccard_dist\r\n",
        "    final_dist = final_dist[:query_num, query_num:]\r\n",
        "    return final_dist\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "'''使用reranking技术后的准确度'''\r\n",
        "class MARKET_mAP_reranking():\r\n",
        "    def __init__(self, num_query, max_rank=50, feat_norm=True):\r\n",
        "        self.num_query = num_query\r\n",
        "        self.max_rank = max_rank\r\n",
        "        self.feat_norm = feat_norm\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.feats = []\r\n",
        "        self.pids = []\r\n",
        "        self.camids = []\r\n",
        "\r\n",
        "    def update(self, output):\r\n",
        "        feat, pid, camid = output\r\n",
        "        self.feats.append(feat)\r\n",
        "        self.pids.append(pid)\r\n",
        "        self.camids.append(camid)\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        feats = torch.stack(self.feats, dim=0)\r\n",
        "        if self.feat_norm :\r\n",
        "           \r\n",
        "            feats = torch.nn.functional.normalize(feats, p=2)\r\n",
        "\r\n",
        "        # query\r\n",
        "        qf = feats[:self.num_query]\r\n",
        "        q_pids = np.asarray(self.pids[:self.num_query])\r\n",
        "        q_camids = np.asarray(self.camids[:self.num_query])\r\n",
        "        # gallery\r\n",
        "        gf = feats[self.num_query:]\r\n",
        "        g_pids = np.asarray(self.pids[self.num_query:])\r\n",
        "        g_camids = np.asarray(self.camids[self.num_query:])\r\n",
        "        # m, n = qf.shape[0], gf.shape[0]\r\n",
        "        # distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "        #           torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "        # distmat.addmm_(1, -2, qf, gf.t())\r\n",
        "        # distmat = distmat.cpu().numpy()\r\n",
        "        print(\"Enter reranking\")\r\n",
        "        distmat = RE_RANKING(qf, gf, k1=20, k2=6, lambda_value=0.3)\r\n",
        "        cmc, mAP = MARKET_EVAL_FUNC(distmat, q_pids, g_pids, q_camids, g_camids)\r\n",
        "\r\n",
        "        return cmc, mAP"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwMCZH4-lXBK"
      },
      "source": [
        "# <font color=red>***Loss***\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1OHkqhMlY8T"
      },
      "source": [
        "##<font color=blue>***分类损失***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeL1k-gLlYKF"
      },
      "source": [
        "class CrossEntropyLabelSmoothLoss(nn.Module):\r\n",
        "\t\"\"\"Cross entropy loss with label smoothing regularizer.\r\n",
        "\tReference:\r\n",
        "\tSzegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\r\n",
        "\tEquation: y = (1 - epsilon) * y + epsilon / K.\r\n",
        "\tArgs:\r\n",
        "\t\tnum_classes (int): number of classes.\r\n",
        "\t\tepsilon (float): weight.\r\n",
        "\t\"\"\"\r\n",
        "\tdef __init__(self, num_classes, epsilon=0.1, use_gpu=True):\r\n",
        "\t\tsuper(CrossEntropyLabelSmoothLoss, self).__init__()\r\n",
        "\t\tself.num_classes = num_classes\r\n",
        "\t\tself.epsilon = epsilon\r\n",
        "\t\tself.use_gpu = use_gpu\r\n",
        "\t\tself.logsoftmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "\tdef forward(self, inputs, targets):\r\n",
        "\t\t\"\"\"\r\n",
        "\t\tArgs:\r\n",
        "\t\t\tinputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\r\n",
        "\t\t\ttargets: ground truth labels with shape (num_classes)\r\n",
        "\t\t\"\"\"\r\n",
        " \r\n",
        "\t\tlog_probs = self.logsoftmax(inputs['logit'])\r\n",
        "\t\ttargets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).cpu(), 1)\r\n",
        "\t\tif self.use_gpu: targets = targets.cuda()\r\n",
        "\t\ttargets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\r\n",
        "\t\tloss = (- targets * log_probs).mean(0).sum()\r\n",
        "\t\treturn loss"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbvoFc_kld7d"
      },
      "source": [
        "## <font color=blue>***度量学习***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grBozebilpOY"
      },
      "source": [
        "###<font color=green>***triplet***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YutBFMCOlgN3"
      },
      "source": [
        "  \r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def normalize(x, axis=-1):\r\n",
        "\t\"\"\"Normalizing to unit length along the specified dimension.\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable\r\n",
        "\tReturns:\r\n",
        "\t  x: pytorch Variable, same shape as input\r\n",
        "\t\"\"\"\r\n",
        "\tx = 1. * x / (torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12)\r\n",
        "\treturn x\r\n",
        "\r\n",
        "def euclidean_dist(x, y):\r\n",
        "\t\"\"\"\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable, with shape [m, d]\r\n",
        "\t  y: pytorch Variable, with shape [n, d]\r\n",
        "\tReturns:\r\n",
        "\t  dist: pytorch Variable, with shape [m, n]\r\n",
        "\t\"\"\"\r\n",
        "\tm, n = x.size(0), y.size(0)\r\n",
        "\txx = torch.pow(x.float(), 2).sum(1, keepdim=True).expand(m, n)\r\n",
        "\tyy = torch.pow(y.float(), 2).sum(1, keepdim=True).expand(n, m).t()\r\n",
        "\tdist = xx + yy\r\n",
        "\tdist.addmm_(1, -2, x.float(), y.t().float())\r\n",
        "\tdist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\r\n",
        "\treturn dist\r\n",
        "\r\n",
        "def cosine_dist(x, y):\r\n",
        "\t\"\"\"\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable, with shape [m, d]\r\n",
        "\t  y: pytorch Variable, with shape [n, d]\r\n",
        "\t\"\"\"\r\n",
        "\tx_normed = F.normalize(x, p=2, dim=1)\r\n",
        "\ty_normed = F.normalize(y, p=2, dim=1)\r\n",
        "\treturn 1 - torch.mm(x_normed, y_normed.t())\r\n",
        "\r\n",
        "def cosine_similarity(x, y):\r\n",
        "\t\"\"\"\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable, with shape [m, d]\r\n",
        "\t  y: pytorch Variable, with shape [n, d]\r\n",
        "\t\"\"\"\r\n",
        "\tx_normed = F.normalize(x, p=2, dim=1)\r\n",
        "\ty_normed = F.normalize(y, p=2, dim=1)\r\n",
        "\treturn torch.mm(x_normed, y_normed.t())\r\n",
        "\r\n",
        "\r\n",
        "def hard_example_mining(dist_mat, labels, return_inds=False):\r\n",
        "\t\"\"\"For each anchor, find the hardest positive and negative sample.\r\n",
        "\tArgs:\r\n",
        "\t  dist_mat: pytorch Variable, pair wise distance between samples, shape [N, N]\r\n",
        "\t  labels: pytorch LongTensor, with shape [N]\r\n",
        "\t  return_inds: whether to return the indices. Save time if `False`(?)\r\n",
        "\tReturns:\r\n",
        "\t  dist_ap: pytorch Variable, distance(anchor, positive); shape [N]\r\n",
        "\t  dist_an: pytorch Variable, distance(anchor, negative); shape [N]\r\n",
        "\t  p_inds: pytorch LongTensor, with shape [N];\r\n",
        "\t\tindices of selected hard positive samples; 0 <= p_inds[i] <= N - 1\r\n",
        "\t  n_inds: pytorch LongTensor, with shape [N];\r\n",
        "\t\tindices of selected hard negative samples; 0 <= n_inds[i] <= N - 1\r\n",
        "\tNOTE: Only consider the case in which all labels have same num of samples,\r\n",
        "\t  thus we can cope with all anchors in parallel.\r\n",
        "\t\"\"\"\r\n",
        "\tassert len(dist_mat.size()) == 2\r\n",
        "\tassert dist_mat.size(0) == dist_mat.size(1)\r\n",
        "\tN = dist_mat.size(0)\r\n",
        "\r\n",
        "\tis_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\r\n",
        "\tis_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\r\n",
        "\r\n",
        "\tdist_ap, relative_p_inds = torch.max(\r\n",
        "\t\tdist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\r\n",
        "\tdist_an, relative_n_inds = torch.min(\r\n",
        "\t\tdist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\r\n",
        "\t\r\n",
        "\tdist_ap = dist_ap.squeeze(1)\r\n",
        "\tdist_an = dist_an.squeeze(1)\r\n",
        "\r\n",
        "\tif return_inds:\r\n",
        "\t\tind = (labels.new().resize_as_(labels)\r\n",
        "\t\t\t   .copy_(torch.arange(0, N).long())\r\n",
        "\t\t\t   .unsqueeze(0).expand(N, N))\r\n",
        "\t\tp_inds = torch.gather(\r\n",
        "\t\t\tind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\r\n",
        "\t\tn_inds = torch.gather(\r\n",
        "\t\t\tind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\r\n",
        "\t\tp_inds = p_inds.squeeze(1)\r\n",
        "\t\tn_inds = n_inds.squeeze(1)\r\n",
        "\t\treturn dist_ap, dist_an, p_inds, n_inds\r\n",
        "\r\n",
        "\treturn dist_ap, dist_an\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class TripletHardLoss(object):\r\n",
        "\t\"\"\"Modified from Tong Xiao's open-reid (https://github.com/Cysu/open-reid).\r\n",
        "\tRelated Triplet Loss theory can be found in paper 'In Defense of the Triplet\r\n",
        "\tLoss for Person Re-Identification'.\"\"\"\r\n",
        "\tdef __init__(self, margin=None, metric=\"euclidean\"):\r\n",
        "\t\tself.margin = margin\r\n",
        "\t\tself.metric = metric\r\n",
        "\t\tif margin is not None:\r\n",
        "\t\t\tself.ranking_loss = nn.MarginRankingLoss(margin=margin)\r\n",
        "\t\telse:\r\n",
        "\t\t\tself.ranking_loss = nn.SoftMarginLoss()\r\n",
        "\r\n",
        "\tdef __call__(self, outs, labels, normalize_feature=True):\r\n",
        "   \r\n",
        "\t\tif normalize_feature:\r\n",
        "\t\t\tglobal_feat = normalize(outs['feat'], axis=-1)\r\n",
        "     \r\n",
        "\t\tif self.metric == \"euclidean\":\r\n",
        "\t\t\tdist_mat = euclidean_dist(outs['feat'], global_feat)\r\n",
        "\t\telif self.metric == \"cosine\":\r\n",
        "\t\t\tdist_mat = cosine_dist(outs['feat'], global_feat)\r\n",
        "\t\telse:\r\n",
        "\t\t\traise NameError\r\n",
        "\r\n",
        "\t\tdist_ap, dist_an = hard_example_mining(\r\n",
        "\t\t\tdist_mat, labels)\r\n",
        "\t\ty = dist_an.new().resize_as_(dist_an).fill_(1)\r\n",
        "\t\t\r\n",
        "\t\tif self.margin is not None:\r\n",
        "\t\t\tloss = self.ranking_loss(dist_an, dist_ap, y)\r\n",
        "\t\telse:\r\n",
        "\t\t\tloss = self.ranking_loss(dist_an - dist_ap, y)\r\n",
        "\t\tprec = (dist_an.data > dist_ap.data).sum() * 1. / y.size(0)\r\n",
        "\t\treturn loss"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DoBceFMJbxp"
      },
      "source": [
        "### <font color=green>***center loss***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8nKAjlSoOk8"
      },
      "source": [
        "\r\n",
        "class CenterLoss(nn.Module):\r\n",
        "    \"\"\"Center loss.\r\n",
        "    Reference:\r\n",
        "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\r\n",
        "    Args:\r\n",
        "        num_classes (int): number of classes.\r\n",
        "        feat_dim (int): feature dimension.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, num_classes=751, feat_dim=2048, use_gpu=True):\r\n",
        "        super(CenterLoss, self).__init__()\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.feat_dim = feat_dim\r\n",
        "        self.use_gpu = use_gpu\r\n",
        "\r\n",
        "        if self.use_gpu:\r\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\r\n",
        "        else:\r\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\r\n",
        "\r\n",
        "    def forward(self, x, labels):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\r\n",
        "            labels: ground truth labels with shape (num_classes).\r\n",
        "        \"\"\"\r\n",
        "        x=x['feat']\r\n",
        "        assert x.size(0) == labels.size(0), \"features.size(0) is not equal to labels.size(0)\"\r\n",
        "\r\n",
        "        batch_size = x.size(0)\r\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\r\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\r\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\r\n",
        "\r\n",
        "        classes = torch.arange(self.num_classes).long()\r\n",
        "        if self.use_gpu: classes = classes.cuda()\r\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\r\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\r\n",
        "\r\n",
        "        dist = distmat * mask.float()\r\n",
        "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\r\n",
        "        #dist = []\r\n",
        "        #for i in range(batch_size):\r\n",
        "        #    value = distmat[i][mask[i]]\r\n",
        "        #    value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability\r\n",
        "        #    dist.append(value)\r\n",
        "        #dist = torch.cat(dist)\r\n",
        "        #loss = dist.mean()\r\n",
        "        return loss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuWO25Rs0ON8"
      },
      "source": [
        "###<font color=green>***ms_loss***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnchy6Tv0Npo"
      },
      "source": [
        "class MultiSimilarityLoss(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MultiSimilarityLoss, self).__init__()\r\n",
        "        self.thresh = 0.5\r\n",
        "\r\n",
        "        '''自我相似度的margin'''\r\n",
        "        self.margin = 0.1\r\n",
        "\r\n",
        "        '''正例对的系数，也就是alpha'''\r\n",
        "        self.scale_pos = 2\r\n",
        "\r\n",
        "        '''负例对的系数，也就是beta'''\r\n",
        "        self.scale_neg = 50\r\n",
        "\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def forward(self, feats, labels):\r\n",
        "        assert feats.size(0) == labels.size(0), \\\r\n",
        "            f\"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}\"\r\n",
        "        feats=feats['feat']\r\n",
        "        feats=normalize(feats,axis=-1)\r\n",
        "        batch_size = feats.size(0)\r\n",
        "\r\n",
        "        '''相似性矩阵，Sij=第i行元素与第j行元素逐元素相乘再相加'''\r\n",
        "        sim_mat = torch.matmul(feats, torch.t(feats))\r\n",
        "\r\n",
        "\r\n",
        "        epsilon = 1e-5\r\n",
        "        loss = list()\r\n",
        "\r\n",
        "        for i in range(batch_size):\r\n",
        "\r\n",
        "            '''获取正例对的相似矩阵'''\r\n",
        "            pos_pair_ = sim_mat[i][labels == labels[i]]\r\n",
        "\r\n",
        "            '''去除掉自己与自己的正例对'''\r\n",
        "            pos_pair_ = pos_pair_[pos_pair_ < (1 - epsilon)]\r\n",
        "            if len(pos_pair_) == 0:\r\n",
        "                print('MS False  count:{}'.format(self.count))\r\n",
        "                self.count += 1\r\n",
        "                continue\r\n",
        "\r\n",
        "            '''获取反例对'''\r\n",
        "            neg_pair_ = sim_mat[i][labels != labels[i]]\r\n",
        "\r\n",
        "            '''根据positive relative similiarty来进行pair mining,具体看论文笔记'''\r\n",
        "            neg_pair = neg_pair_[neg_pair_ + self.margin > min(pos_pair_)]\r\n",
        "            pos_pair = pos_pair_[pos_pair_ - self.margin < max(neg_pair_)]\r\n",
        "\r\n",
        "            if len(neg_pair) < 1 or len(pos_pair) < 1:\r\n",
        "                continue\r\n",
        "\r\n",
        "            # weighting step\r\n",
        "            '''然后就是根据最终公式用代码实现'''\r\n",
        "            pos_loss = 1.0 / self.scale_pos * torch.log(\r\n",
        "                1 + torch.sum(torch.exp(-self.scale_pos * (pos_pair - self.thresh))))\r\n",
        "            neg_loss = 1.0 / self.scale_neg * torch.log(\r\n",
        "                1 + torch.sum(torch.exp(self.scale_neg * (neg_pair - self.thresh))))\r\n",
        "            loss.append(pos_loss + neg_loss)\r\n",
        "\r\n",
        "        if len(loss) == 0:\r\n",
        "            return torch.zeros([], requires_grad=True)\r\n",
        "\r\n",
        "        loss = sum(loss) / batch_size\r\n",
        "        return loss\r\n",
        "\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ibnlr6YeL6h"
      },
      "source": [
        "### circle loss\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIH1pBRQeTNc"
      },
      "source": [
        "from typing import Tuple\r\n",
        "from torch import Tensor\r\n",
        "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\r\n",
        "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\r\n",
        "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\r\n",
        "\r\n",
        "    positive_matrix = label_matrix.triu(diagonal=1)\r\n",
        "    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\r\n",
        "\r\n",
        "    similarity_matrix = similarity_matrix.view(-1)\r\n",
        "    positive_matrix = positive_matrix.view(-1)\r\n",
        "    negative_matrix = negative_matrix.view(-1)\r\n",
        "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\r\n",
        "\r\n",
        "\r\n",
        "class CircleLoss(nn.Module):\r\n",
        "    def __init__(self, m: float, gamma: float) -> None:\r\n",
        "        super(CircleLoss, self).__init__()\r\n",
        "        self.m = m\r\n",
        "        self.gamma = gamma\r\n",
        "        self.soft_plus = nn.Softplus()\r\n",
        "\r\n",
        "    def forward(feat,label) -> Tensor:\r\n",
        "        feat=feat['feat']\r\n",
        "        feat=normalize(feat,axis=-1)\r\n",
        "        sp, sn = convert_label_to_similarity(feat, lbl)\r\n",
        "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\r\n",
        "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\r\n",
        "\r\n",
        "        delta_p = 1 - self.m\r\n",
        "        delta_n = self.m\r\n",
        "\r\n",
        "        logit_p = - ap * (sp - delta_p) * self.gamma\r\n",
        "        logit_n = an * (sn - delta_n) * self.gamma\r\n",
        "\r\n",
        "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-2TJdWOoucU"
      },
      "source": [
        "##训练loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtrIOLhU2_FF"
      },
      "source": [
        "def make_loss(cfg,num_classes):\r\n",
        "  loss=cfg['loss']\r\n",
        "  criterion=[]\r\n",
        "  for name,weight in loss:\r\n",
        "    if name=='cross':\r\n",
        "      criterion.append((CrossEntropyLabelSmoothLoss(num_classes),weight))\r\n",
        "    elif name=='tri':\r\n",
        "      criterion.append((TripletHardLoss(cfg['margin']),weight))\r\n",
        "    elif name=='ms':\r\n",
        "      criterion.append((MultiSimilarityLoss(),weight))\r\n",
        "    elif name=='circle':\r\n",
        "      criterion.append((CircleLoss(m=0.25, gamma=80),weight))\r\n",
        "  return criterion\r\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwD9tdg_o5Cl"
      },
      "source": [
        "# <font color=red>***lr_scheduler***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2XalTTRvhz2"
      },
      "source": [
        "## <font color=blue>***warm up***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1YdaRgToxrD"
      },
      "source": [
        "from bisect import bisect_right\r\n",
        "import torch\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        milestones,\r\n",
        "        gamma=0.1,\r\n",
        "        warmup_factor=1.0 / 3,\r\n",
        "        warmup_iters=500,\r\n",
        "        warmup_method=\"linear\",\r\n",
        "        last_epoch=-1,\r\n",
        "    ):\r\n",
        "        if not list(milestones) == sorted(milestones):\r\n",
        "            raise ValueError(\r\n",
        "                \"Milestones should be a list of\" \" increasing integers. Got {}\",\r\n",
        "                milestones,\r\n",
        "            )\r\n",
        "\r\n",
        "        if warmup_method not in (\"constant\", \"linear\"):\r\n",
        "            raise ValueError(\r\n",
        "                \"Only 'constant' or 'linear' warmup_method accepted\"\r\n",
        "                \"got {}\".format(warmup_method)\r\n",
        "            )\r\n",
        "        self.milestones = milestones\r\n",
        "        self.gamma = gamma\r\n",
        "        self.warmup_factor = warmup_factor\r\n",
        "        self.warmup_iters = warmup_iters\r\n",
        "        self.warmup_method = warmup_method\r\n",
        "        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\r\n",
        "\r\n",
        "    def get_lr(self):\r\n",
        "        warmup_factor = 1\r\n",
        "        if self.last_epoch < self.warmup_iters:\r\n",
        "            if self.warmup_method == \"constant\":\r\n",
        "                warmup_factor = self.warmup_factor\r\n",
        "            elif self.warmup_method == \"linear\":\r\n",
        "                alpha = self.last_epoch / self.warmup_iters\r\n",
        "                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\r\n",
        "        return [\r\n",
        "            base_lr\r\n",
        "            * warmup_factor\r\n",
        "            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\r\n",
        "            for base_lr in self.base_lrs\r\n",
        "        ]            \r\n",
        "        "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbZrnAdK8rSH"
      },
      "source": [
        "##<font color=blue>***Flat***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg4hvxcy8wOK"
      },
      "source": [
        "class Flat(torch.optim.lr_scheduler._LRScheduler):\r\n",
        "  def __init__(self,optimizer,cfg,anneal_start=0.65,last_epoch=-1):\r\n",
        "    self.epochs=cfg['epochs']\r\n",
        "    self.start=anneal_start\r\n",
        "    super(Flat,self).__init__(optimizer,last_epoch)\r\n",
        "  \r\n",
        "  def get_lr(self):\r\n",
        "    if self.last_epoch<self.epochs*self.start:\r\n",
        "      return [base_lr for base_lr in self.base_lrs]\r\n",
        "    else:\r\n",
        "      return [\r\n",
        "        base_lr*(1+math.cos(math.pi*self.last_epoch/5))/2  \r\n",
        "        for base_lr in self.base_lrs    \r\n",
        "      ]\r\n",
        "     \r\n",
        "\r\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0qKFMmwdHwQ"
      },
      "source": [
        "##<font color=blue>***不变化的scheduler***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAmXuhx5d3O0"
      },
      "source": [
        "class Unchange_sch:\r\n",
        "  def __init__(self):\r\n",
        "    pass\r\n",
        "  \r\n",
        "  def step(self):\r\n",
        "    pass"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VITUywn2w5bK"
      },
      "source": [
        "#<font color=red>***自定义optimizer***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiReKXojxGop"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pep0zTlKO7S"
      },
      "source": [
        "#<font color=red>***自定义eva***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VZu_FcobkO4"
      },
      "source": [
        "##<font color=blue>***普通***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6igUDwI5azrU"
      },
      "source": [
        "def do_eva1(cfg,model,val_loader,num_query):\r\n",
        "  model.eval()\r\n",
        "  device=torch.device('cuda:0')\r\n",
        "  \r\n",
        "  metric0=MARKET_MAP(num_query,all=False,one_day=True)\r\n",
        "  metric1=MARKET_MAP(num_query,all=False,one_day=False)\r\n",
        "\r\n",
        "  \r\n",
        "  with torch.no_grad():\r\n",
        "    for imgs,pids,cams,dates in val_loader:\r\n",
        "      imgs=imgs.to(device)\r\n",
        "      outs=model(imgs)\r\n",
        "      \r\n",
        "      for out in zip(outs['feat'],pids,cams,dates):\r\n",
        "          metric0.update(out)\r\n",
        "          metric1.update(out)\r\n",
        "         \r\n",
        "      \r\n",
        "\r\n",
        "  map0=metric0.compute()\r\n",
        "  map1=metric1.compute()\r\n",
        "  \r\n",
        "  \r\n",
        "  return map0,map1\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKLIQonmOmt2"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OsA3g4ntlgV"
      },
      "source": [
        "#<font color=red>***训练辅助函数***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryoETDPctsIv"
      },
      "source": [
        "##<font color=blue>***checkpoint***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZF5we9Ct47q"
      },
      "source": [
        "class Checkpoint:\r\n",
        "  def __init__(self,ckpt):\r\n",
        "    self.ckpt=ckpt\r\n",
        "    self.init=-1000\r\n",
        "  \r\n",
        "  def __call__(self,metric,model,optimizer):\r\n",
        "    if metric>self.init:\r\n",
        "      self.init=metric\r\n",
        "      torch.save({'model':model.state_dict(),'optimizer':optimizer.state_dict()},self.ckpt)\r\n",
        "    \r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MroXXo59fjnA"
      },
      "source": [
        "##<font color=blue>***EMA***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPY1GGoYfwpH"
      },
      "source": [
        "from copy import deepcopy\r\n",
        "from collections import OrderedDict\r\n",
        "from sys import stderr\r\n",
        "\r\n",
        "# for type hint\r\n",
        "from torch import Tensor\r\n",
        "\r\n",
        "\r\n",
        "class EMA(nn.Module):\r\n",
        "    def __init__(self, model: nn.Module, decay: float):\r\n",
        "        super().__init__()\r\n",
        "        self.decay = decay\r\n",
        "\r\n",
        "        self.model = model\r\n",
        "        self.shadow = deepcopy(self.model)\r\n",
        "\r\n",
        "        for param in self.shadow.parameters():\r\n",
        "            param.detach_()\r\n",
        "\r\n",
        "    @torch.no_grad()\r\n",
        "    def update(self):\r\n",
        "        if not self.training:\r\n",
        "            print(\"EMA update should only be called during training\", file=stderr, flush=True)\r\n",
        "            return\r\n",
        "\r\n",
        "        model_params = OrderedDict(self.model.named_parameters())\r\n",
        "        shadow_params = OrderedDict(self.shadow.named_parameters())\r\n",
        "\r\n",
        "        # check if both model contains the same set of keys\r\n",
        "        assert model_params.keys() == shadow_params.keys()\r\n",
        "\r\n",
        "        for name, param in model_params.items():\r\n",
        "            # see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\r\n",
        "            # shadow_variable -= (1 - decay) * (shadow_variable - variable)\r\n",
        "            shadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param))\r\n",
        "\r\n",
        "        model_buffers = OrderedDict(self.model.named_buffers())\r\n",
        "        shadow_buffers = OrderedDict(self.shadow.named_buffers())\r\n",
        "\r\n",
        "        # check if both model contains the same set of keys\r\n",
        "        assert model_buffers.keys() == shadow_buffers.keys()\r\n",
        "\r\n",
        "        for name, buffer in model_buffers.items():\r\n",
        "            # buffers are copied\r\n",
        "            shadow_buffers[name].copy_(buffer)\r\n",
        "\r\n",
        "    def forward(self, inputs: Tensor, return_feature: bool = False) -> Tensor:\r\n",
        "        if self.training:\r\n",
        "            return self.model(inputs, return_feature)\r\n",
        "        else:\r\n",
        "            return self.shadow(inputs, return_feature)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUKZ7bA3qxkb"
      },
      "source": [
        "##<font color=blue>***计算参数和flops***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRJBQVetvcf5"
      },
      "source": [
        "from thop import profile\r\n",
        "from thop import clever_format\r\n",
        "\r\n",
        "def count_your_model(model, x, y):\r\n",
        "  outs=model(x)\r\n",
        "  \r\n",
        "def get_pf(model):\r\n",
        "\r\n",
        "\r\n",
        "  device=torch.device('cuda:0')\r\n",
        "  input = torch.randn(1, 3, 224, 224)\r\n",
        "  input=input.to(device)\r\n",
        "  flops, params = profile(model, inputs=(input, ))\r\n",
        "  flops, params = clever_format([flops, params], \"%.3f\")\r\n",
        "  return flops,params"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z61u-GZxI86"
      },
      "source": [
        "#<font color=red>***自定义训练过程***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKuzTaBDxUjt"
      },
      "source": [
        "##<font color=blue>普通网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgifGg5XxPii"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import logging\r\n",
        "def do_train1(cfg,model,train_loader,val_loader,optimizer,scheduler,criterion,num_query,logger,checkpoint):\r\n",
        "  device=torch.device('cuda:0')\r\n",
        "  model=model.to(device)\r\n",
        "  flops,params=get_pf(model)\r\n",
        "  logger.info('param:{}'.format(params))\r\n",
        "  logger.info('flops:{}'.format(flops))\r\n",
        "  epochs=1\r\n",
        "  if cfg['ema']:\r\n",
        "    model=EMA(model,0.999)\r\n",
        "  if cfg['amp']:\r\n",
        "    scaler=GradScaler()\r\n",
        "  with tqdm(total=epochs) as pbar:\r\n",
        "    for epoch in range(epochs):\r\n",
        "      model.train()\r\n",
        "      \r\n",
        "      for images,pids in train_loader:\r\n",
        "        loss=0\r\n",
        "        optimizer.zero_grad()\r\n",
        "        images=images.to(device)\r\n",
        "        if cfg['amp']:\r\n",
        "          if cfg['grad_l2']:\r\n",
        "            with autocast():\r\n",
        "              outs = model(images)\r\n",
        "              pids=pids.to(device)\r\n",
        "              for cri,weight in criterion:\r\n",
        "                loss=loss+weight*cri(outs,pids)\r\n",
        "            new_parameters=[p for p in model.parameters() if p.requires_grad]\r\n",
        "            scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),\r\n",
        "                                                 inputs=new_parameters,\r\n",
        "                                                 create_graph=True)\r\n",
        "            inv_scale = 1./scaler.get_scale()\r\n",
        "            grad_params = [p * inv_scale for p in scaled_grad_params]\r\n",
        "                                     \r\n",
        "            with autocast():\r\n",
        "              grad_norm = 0\r\n",
        "              for grad in grad_params:\r\n",
        "                  grad_norm += grad.pow(2).sum()\r\n",
        "              grad_norm = grad_norm.sqrt()\r\n",
        "              loss = loss + grad_norm  \r\n",
        "            scaler.scale(loss).backward() \r\n",
        "            scaler.step(optimizer)\r\n",
        "            scaler.update()                                  \r\n",
        "\r\n",
        "          else:\r\n",
        "            with autocast():\r\n",
        "              outs = model(images)\r\n",
        "              pids=pids.to(device)\r\n",
        "              for cri,weight in criterion:\r\n",
        "                loss=loss+weight*cri(outs,pids)\r\n",
        "            scaler.scale(loss).backward()\r\n",
        "            scaler.step(optimizer)\r\n",
        "            scaler.update()  \r\n",
        "\r\n",
        "          if cfg['ema']:\r\n",
        "            model.update()\r\n",
        "        else:\r\n",
        "          outs = model(images)\r\n",
        "          pids=pids.to(device)\r\n",
        "          for cri,weight in criterion:\r\n",
        "            loss=loss+weight*cri(outs,pids)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "          if cfg['ema']:\r\n",
        "            model.update()\r\n",
        "      scheduler.step()\r\n",
        "      print('eval')\r\n",
        "      if epoch%5==0:\r\n",
        "        map0,map1=do_eva1(cfg,model,val_loader,num_query)\r\n",
        "        print(map0,map1)\r\n",
        "        logger.info('epoch:{}---map0:{}---map1:{}----'.format(epoch,map0,map1))\r\n",
        "        checkpoint((map0+map1)/2,model,optimizer)\r\n",
        "      pbar.update(1)\r\n",
        "  \r\n",
        "  \r\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX-nyeucxwns"
      },
      "source": [
        "# <font color=red>***训练***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP9jNkU2oSUA"
      },
      "source": [
        "##<font color=blue>***普通训练*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6TwHwQSx1rA",
        "outputId": "4c77aab7-d3e5-4020-f8d7-00e3b5ce69a2"
      },
      "source": [
        "from torchtools.optim import RangerLars\r\n",
        "def main1(cfg,root):\r\n",
        "  dataset=PDataset(root)\r\n",
        "  train_loader,val_loader,num_query,num_classes=make_date_data_loader(dataset,cfg)\r\n",
        "\r\n",
        "  \r\n",
        "  if cfg['model']=='baseline':\r\n",
        "      model=Baseline(num_classes,cfg)\r\n",
        "  elif cfg['model']=='Global_Attention':\r\n",
        "      model=ResNet50_RGA_Model(num_classes=num_classes)\r\n",
        "  elif cfg['model']=='Relation':\r\n",
        "    model=RelationModel(cfg,6,num_classes)\r\n",
        "\r\n",
        "  \r\n",
        "  criterion=make_loss(cfg,num_classes)\r\n",
        "  if cfg['optimizer']=='adam':\r\n",
        "    optimizer=torch.optim.Adam(model.parameters(),lr=cfg['lr'],weight_decay=cfg['weight_decay'])\r\n",
        "  elif cfg['optimizer']=='sgd':\r\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=cfg['lr'],momentum=cfg['momentum'],weight_decay=cfg['weight_decay'])\r\n",
        "  elif cfg['optimizer']=='rangerlars':\r\n",
        "    optimizer=RangerLars(model.parameters(),lr=cfg['lr'])\r\n",
        "  \r\n",
        "\r\n",
        "  if cfg['scheduler']=='warmup':\r\n",
        "\r\n",
        "    scheduler = WarmupMultiStepLR(optimizer,cfg['steps'], cfg['gamma'],\r\n",
        "                  cfg['warmup_factors'],cfg['warmup_iters'], \r\n",
        "                  'linear')\r\n",
        "  elif cfg['scheduler']=='flat':\r\n",
        "    scheduler=Flat(optimizer,cfg)\r\n",
        "  \r\n",
        "  elif cfg['scheduler']=='cos_warmup':\r\n",
        "    scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer ,T_0=5 ,T_mult=2)\r\n",
        "\r\n",
        "  ckpt=cfg['ckpt']\r\n",
        "  logpt=cfg['logpt']\r\n",
        "  logger=logging.getLogger()\r\n",
        "  fh = logging.FileHandler(\"spam.log\")\r\n",
        "  fh.setLevel(logging.DEBUG)\r\n",
        "  logger.addHandler(fh)\r\n",
        "  checkpoint=Checkpoint(ckpt)\r\n",
        "  do_train1(cfg,model,train_loader,val_loader,optimizer,scheduler,criterion,num_query,\r\n",
        "            logger,checkpoint)\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRXJOh8jz4n3"
      },
      "source": [
        "##<font color=blue>***两阶段***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vNQkuQJNX0J"
      },
      "source": [
        "def main2(cfg,root):\r\n",
        "  dataset=Market1501(root)\r\n",
        "  train_loader,val_loader,num_query,num_classes=make_data_loader(dataset,cfg)\r\n",
        "\r\n",
        "\r\n",
        "  model=Baseline(num_classes,cfg)\r\n",
        "\r\n",
        "\r\n",
        "  criterion=make_loss(cfg,num_classes)\r\n",
        "\r\n",
        "  step1_sgd=torch.optim.SGD(model.parameters(),lr=0.1,weight_decay=0.0001)\r\n",
        "  scheduler=Unchange_sch()\r\n",
        "  ckpt=cfg['ckpt']\r\n",
        "  logpt=cfg['logpt']\r\n",
        "  logger=logging.getLogger()\r\n",
        "  fh = logging.FileHandler(\"spam.log\")\r\n",
        "  fh.setLevel(logging.DEBUG)\r\n",
        "  logger.addHandler(fh)\r\n",
        "  checkpoint=Checkpoint(ckpt)\r\n",
        "\r\n",
        "  do_train1(cfg,model,train_loader,val_loader,step1_sgd,scheduler,criterion,num_query,\r\n",
        "            logger,checkpoint)\r\n",
        "  \r\n",
        "  state=torch.load(cfg['ckpt'])\r\n",
        "  model.load_state_dict(state['model'])\r\n",
        "  step2_sgd=torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.95,weight_decay=0.0001)\r\n",
        "  scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(step2_sgd,T_0=5,T_mult=2)\r\n",
        "  do_train1(cfg,model,train_loader,val_loader,step2_sgd,scheduler,criterion,num_query,\r\n",
        "            logger,checkpoint)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WnbzLduc5B5"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzxfl_U3WwLl"
      },
      "source": [
        "#<font color=red>***代码运行***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wGQ9XPK3wCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7014d294-ea37-44ed-cefb-eb510cc0d774"
      },
      "source": [
        "file1=open('/content/drive/MyDrive/dataset/REID/image_date.json','r')\r\n",
        "image_dates=json.load(file1)\r\n",
        "file1.close()\r\n",
        "\r\n",
        "file1=open('/content/drive/MyDrive/dataset/REID/image_cam.json','r')\r\n",
        "image_cams=json.load(file1)\r\n",
        "file1.close()\r\n",
        "\r\n",
        "file1=open('/content/drive/MyDrive/dataset/REID/image_cam_dict.json','r')\r\n",
        "image_cam_dict=json.load(file1)\r\n",
        "file1.close()\r\n",
        "\r\n",
        "file1=open('/content/drive/MyDrive/dataset/REID/image_p.json','r')\r\n",
        "image_p=json.load(file1)\r\n",
        "file1.close()\r\n",
        "\r\n",
        "file1=open('/content/drive/MyDrive/dataset/REID/date_id.json','r')\r\n",
        "date_ids=json.load(file1)\r\n",
        "file1.close()\r\n",
        "root='/kaggle/input/p-final-reid/reid1'\r\n",
        "root1='/content/datasets/reid1'\r\n",
        "main1(cfg,root1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Baseline'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:186: UserWarning: This overload of addmm_ is deprecated:\n",
            "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "all_ap=0\n",
            "all_ap=0\n",
            "all_ap=0\n",
            "0.39573557863737485 0.07226504510839239\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:21<00:00, 141.92s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj8c69dMO9yR"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}