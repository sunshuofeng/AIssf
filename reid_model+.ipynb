{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reid_model+.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BGVr-TNWFdCi",
        "Fq664kQWGfQM",
        "c_NJxhdYEs4x",
        "hIpaBSREEvAr"
      ],
      "mount_file_id": "1NA6m6l4HubG8DDFw7v3aTDIZMzpwSYtd",
      "authorship_tag": "ABX9TyOfiBqzs+8G11sJTlChyMo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshuofeng/AIssf/blob/master/reid_model%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F9sUlsWo9XM"
      },
      "source": [
        "# 数据和包的准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH4hbabclobT",
        "outputId": "9d6aefb2-9755-41a9-b83b-34ef4e67e4b8"
      },
      "source": [
        "!pip install geffnet\r\n",
        "!pip install git+https://github.com/pabloppp/pytorch-tools -U\r\n",
        "!pip install thop \r\n",
        "!pip install -U people_segmentation\r\n",
        "!pip install lambda-networks\r\n",
        "!pip install kaggle\r\n",
        "!pip install iglovikov_helper_functions"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geffnet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from geffnet) (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from geffnet) (0.9.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->geffnet) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->geffnet) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->geffnet) (7.0.0)\n",
            "Collecting git+https://github.com/pabloppp/pytorch-tools\n",
            "  Cloning https://github.com/pabloppp/pytorch-tools to /tmp/pip-req-build-ht7by5ol\n",
            "  Running command git clone -q https://github.com/pabloppp/pytorch-tools /tmp/pip-req-build-ht7by5ol\n",
            "Requirement already satisfied, skipping upgrade: torch==1.* in /usr/local/lib/python3.7/dist-packages (from torchtools==0.2.6) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.7/dist-packages (from torchtools==0.2.6) (0.9.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy==1.* in /usr/local/lib/python3.7/dist-packages (from torchtools==0.2.6) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.*->torchtools==0.2.6) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->torchtools==0.2.6) (7.0.0)\n",
            "Building wheels for collected packages: torchtools\n",
            "  Building wheel for torchtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtools: filename=torchtools-0.2.6-cp37-none-any.whl size=21149 sha256=cd4602523e5761d505035195e91dc067db44be0dbfec4b5ecb8183c238e4e423\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ii2orpv0/wheels/53/7a/b0/86ffb126404564b151069f4d8d9f43dbfe3f17bfc0b898b45f\n",
            "Successfully built torchtools\n",
            "Installing collected packages: torchtools\n",
            "  Found existing installation: torchtools 0.2.6\n",
            "    Uninstalling torchtools-0.2.6:\n",
            "      Successfully uninstalled torchtools-0.2.6\n",
            "Successfully installed torchtools-0.2.6\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (1.19.5)\n",
            "Requirement already up-to-date: people_segmentation in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied, skipping upgrade: segmentation-models-pytorch in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (0.1.3)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (1.2.3)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-toolbelt in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: iglovikov-helper-functions in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (0.0.53)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: albumentations in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (0.1.12)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from people_segmentation) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch->people_segmentation) (0.7.4)\n",
            "Requirement already satisfied, skipping upgrade: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch->people_segmentation) (0.9.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: timm==0.3.2 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch->people_segmentation) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch->people_segmentation) (0.6.3)\n",
            "Requirement already satisfied, skipping upgrade: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->people_segmentation) (0.18.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->people_segmentation) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->people_segmentation) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->people_segmentation) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->people_segmentation) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-toolbelt->people_segmentation) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: imagecorruptions in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: addict in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: jpeg4py in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (0.1.4)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.7/dist-packages (from iglovikov-helper-functions->people_segmentation) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from albumentations->people_segmentation) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->people_segmentation) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch->people_segmentation) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (3.7.0)\n",
            "Requirement already satisfied, skipping upgrade: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (3.7.4.post0)\n",
            "Requirement already satisfied, skipping upgrade: cffi in /usr/local/lib/python3.7/dist-packages (from jpeg4py->iglovikov-helper-functions->people_segmentation) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->iglovikov-helper-functions->people_segmentation) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->iglovikov-helper-functions->people_segmentation) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov-helper-functions->people_segmentation) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov-helper-functions->people_segmentation) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov-helper-functions->people_segmentation) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov-helper-functions->people_segmentation) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->people_segmentation) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->jpeg4py->iglovikov-helper-functions->people_segmentation) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->iglovikov-helper-functions->people_segmentation) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->iglovikov-helper-functions->people_segmentation) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->iglovikov-helper-functions->people_segmentation) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->iglovikov-helper-functions->people_segmentation) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->people_segmentation) (3.1.0)\n",
            "Requirement already satisfied: lambda-networks in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from lambda-networks) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from lambda-networks) (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->lambda-networks) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->lambda-networks) (1.19.5)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: iglovikov_helper_functions in /usr/local/lib/python3.7/dist-packages (0.0.53)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (0.22.2.post1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (4.1.2.30)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (2.4.0)\n",
            "Requirement already satisfied: imagecorruptions in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (1.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (7.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (1.4.1)\n",
            "Requirement already satisfied: jpeg4py in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (0.1.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (0.16.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (1.8.0+cu101)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from iglovikov_helper_functions) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->iglovikov_helper_functions) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->iglovikov_helper_functions) (2018.9)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from jpeg4py->iglovikov_helper_functions) (1.14.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov_helper_functions) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov_helper_functions) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov_helper_functions) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->iglovikov_helper_functions) (2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->iglovikov_helper_functions) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->iglovikov_helper_functions) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->jpeg4py->iglovikov_helper_functions) (2.20)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->iglovikov_helper_functions) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->iglovikov_helper_functions) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->iglovikov_helper_functions) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->iglovikov_helper_functions) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oCymeDCpCMG",
        "outputId": "0d658d39-1f56-4344-e2a9-c27f50ec9848"
      },
      "source": [
        "import json\r\n",
        "token = {\"username\":\"ssfailearning\",\"key\":\"bccedfffce653f07d5acaf4409341c47\"}\r\n",
        "with open('/content/kaggle.json', 'w') as file:\r\n",
        "  json.dump(token, file)\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp /content/kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json\r\n",
        "!kaggle config set -n path -v /content\r\n",
        "!kaggle datasets download -d rayiooo/reid_market-1501\r\n",
        "!kaggle datasets download -d ssf0921/p-final-reid\r\n",
        "!kaggle datasets download -d ssfailearning/vc-cloths"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- path is now set to: /content\n",
            "reid_market-1501.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "p-final-reid.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "vc-cloths.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5WDfoUmpNIN",
        "outputId": "3b54642f-f9f5-4dcf-f42f-b5982523a887"
      },
      "source": [
        "import sys\r\n",
        "!git clone https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB.git\r\n",
        "sys.path.append('Ultra-Light-Fast-Generic-Face-Detector-1MB')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Ultra-Light-Fast-Generic-Face-Detector-1MB' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XAA0M8GpisV"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "\r\n",
        "zip_file = zipfile.ZipFile('/content/datasets/rayiooo/reid_market-1501/reid_market-1501.zip')\r\n",
        "for names in zip_file.namelist():\r\n",
        "        zip_file.extract(names,'datasets')\r\n",
        "zip_file.close()\r\n",
        "\r\n",
        "zip_file = zipfile.ZipFile('/content/datasets/ssf0921/p-final-reid/p-final-reid.zip')\r\n",
        "for names in zip_file.namelist():\r\n",
        "        zip_file.extract(names,'datasets')\r\n",
        "zip_file.close()\r\n",
        "zip_file = zipfile.ZipFile('/content/datasets/ssfailearning/vc-cloths/vc-cloths.zip')\r\n",
        "for names in zip_file.namelist():\r\n",
        "        zip_file.extract(names,'datasets')\r\n",
        "zip_file.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbitXx1ZpeRJ"
      },
      "source": [
        "#数据准备"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLCtnoOp7Yf"
      },
      "source": [
        "##数据增强"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2OR9IfpZK1"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from torch.cuda.amp import GradScaler,autocast\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "\r\n",
        "class RandomErasing(object):\r\n",
        "    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\r\n",
        "        'Random Erasing Data Augmentation' by Zhong et al.\r\n",
        "        See https://arxiv.org/pdf/1708.04896.pdf\r\n",
        "    Args:\r\n",
        "         probability: The probability that the Random Erasing operation will be performed.\r\n",
        "         sl: Minimum proportion of erased area against input image.\r\n",
        "         sh: Maximum proportion of erased area against input image.\r\n",
        "         r1: Minimum aspect ratio of erased area.\r\n",
        "         mean: Erasing value.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=(0.4914, 0.4822, 0.4465)):\r\n",
        "        self.probability = probability\r\n",
        "        self.mean = mean\r\n",
        "        self.sl = sl\r\n",
        "        self.sh = sh\r\n",
        "        self.r1 = r1\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "\r\n",
        "        if random.uniform(0, 1) >= self.probability:\r\n",
        "            return img\r\n",
        "\r\n",
        "        for attempt in range(100):\r\n",
        "            area = img.size()[1] * img.size()[2]\r\n",
        "\r\n",
        "            target_area = random.uniform(self.sl, self.sh) * area\r\n",
        "            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\r\n",
        "\r\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\r\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\r\n",
        "\r\n",
        "            if w < img.size()[2] and h < img.size()[1]:\r\n",
        "                x1 = random.randint(0, img.size()[1] - h)\r\n",
        "                y1 = random.randint(0, img.size()[2] - w)\r\n",
        "                if img.size()[0] == 3:\r\n",
        "                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\r\n",
        "                    img[1, x1:x1 + h, y1:y1 + w] = self.mean[1]\r\n",
        "                    img[2, x1:x1 + h, y1:y1 + w] = self.mean[2]\r\n",
        "                else:\r\n",
        "                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\r\n",
        "                return img\r\n",
        "\r\n",
        "        return img\r\n",
        "\r\n",
        "import math\r\n",
        "from PIL import Image\r\n",
        "import random\r\n",
        "import  numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "# This is the code of Random Grayscale Patch Replace\r\n",
        "\r\n",
        "class RGPR(object):\r\n",
        "\r\n",
        "    def __init__(self, probability=0.2, sl=0.02, sh=0.4, r1=0.3):\r\n",
        "        self.probability = probability\r\n",
        "        self.sl = sl\r\n",
        "        self.sh = sh\r\n",
        "        self.r1 = r1\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "\r\n",
        "        new = img.convert(\"L\")   # Convert from here to the corresponding grayscale image\r\n",
        "        np_img = np.array(new, dtype=np.uint8)\r\n",
        "        img_gray = np.dstack([np_img, np_img, np_img])\r\n",
        "\r\n",
        "        if random.uniform(0, 1) >= self.probability:\r\n",
        "            return img\r\n",
        "\r\n",
        "        for attempt in range(100):\r\n",
        "            area = img.size[0] * img.size[1]\r\n",
        "            target_area = random.uniform(self.sl, self.sh) * area\r\n",
        "            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\r\n",
        "\r\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\r\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\r\n",
        "\r\n",
        "            if w < img.size[1] and h < img.size[0]:\r\n",
        "                x1 = random.randint(0, img.size[0] - h)\r\n",
        "                y1 = random.randint(0, img.size[1] - w)\r\n",
        "                img = np.asarray(img).astype('float')\r\n",
        "\r\n",
        "                img[y1:y1 + h, x1:x1 + w, 0] = img_gray[y1:y1 + h, x1:x1 + w, 0]\r\n",
        "                img[y1:y1 + h, x1:x1 + w, 1] = img_gray[y1:y1 + h, x1:x1 + w, 1]\r\n",
        "                img[y1:y1 + h, x1:x1 + w, 2] = img_gray[y1:y1 + h, x1:x1 + w, 2]\r\n",
        "\r\n",
        "                img = Image.fromarray(img.astype('uint8'))\r\n",
        "\r\n",
        "                return img\r\n",
        "\r\n",
        "        return img\r\n",
        "import torchvision.transforms as T\r\n",
        "def build_change_transforms(cfg, training=True):\r\n",
        "    normalize_transform = T.Normalize(mean=cfg['mean'], std=cfg['std'])\r\n",
        "    if training:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['train_size']),\r\n",
        "            T.ColorJitter(),\r\n",
        "            T.RandomHorizontalFlip(p=0.5),\r\n",
        "            T.Pad(cfg['padding']),\r\n",
        "            T.RandomCrop(cfg['train_size']),\r\n",
        "            T.RandomGrayscale(p=0.05),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform,\r\n",
        "            RandomErasing(probability=0.5, mean=cfg['mean']),\r\n",
        "            \r\n",
        "        ])\r\n",
        "    else:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['test_size']),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform\r\n",
        "        ])\r\n",
        "\r\n",
        "    return transform\r\n",
        "\r\n",
        "def build_origin_transoform(cfg):\r\n",
        "  \r\n",
        "    transform=T.Compose([\r\n",
        "         T.Resize((320,320)),\r\n",
        "         T.ToTensor()                \r\n",
        "    ])\r\n",
        "    return transform\r\n",
        "\r\n",
        "def build_face_transform(cfg,training=True):\r\n",
        "  normalize_transform = T.Normalize(mean=cfg['mean'], std=cfg['std'])\r\n",
        "  if training:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.ToPILImage(),\r\n",
        "            T.Resize(cfg['train_face_size']),\r\n",
        "            T.ColorJitter(),\r\n",
        "            T.RandomHorizontalFlip(p=0.5),\r\n",
        "            T.Pad(cfg['padding']),\r\n",
        "            T.RandomCrop(cfg['train_face_size']),\r\n",
        "            T.RandomGrayscale(p=0.05),\r\n",
        "            T.RandomAffine(degree=5),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform,\r\n",
        "            RandomErasing(probability=0.5, mean=cfg['mean'],sh=0.3),\r\n",
        "        ])\r\n",
        "  else:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['test_face_size']),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform\r\n",
        "        ])\r\n",
        "\r\n",
        "  return transform\r\n",
        "  \r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN0R1rRBujnq"
      },
      "source": [
        "##sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0xlhtcouk2T"
      },
      "source": [
        "\r\n",
        "import copy\r\n",
        "import random\r\n",
        "import torch\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "class DateRandomIdentitySampler(Sampler):\r\n",
        "    \"\"\"\r\n",
        "    Randomly sample N identities, then for each identity,\r\n",
        "    randomly sample K instances, therefore batch size is N*K.\r\n",
        "    Args:\r\n",
        "    - data_source (list): list of (img_path, pid, camid).\r\n",
        "    - num_instances (int): number of instances per identity in a batch.\r\n",
        "    - batch_size (int): number of examples in a batch.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data_source, batch_size, num_instances):\r\n",
        "        self.data_source = data_source\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_instances = num_instances\r\n",
        "        self.num_pids_per_batch = self.batch_size // self.num_instances\r\n",
        "        self.index_dic = defaultdict(list)\r\n",
        "        for index, (_,_, pid, _,_) in enumerate(self.data_source):\r\n",
        "            self.index_dic[pid].append(index)\r\n",
        "        self.pids = list(self.index_dic.keys())\r\n",
        "\r\n",
        "        # estimate number of examples in an epoch\r\n",
        "        self.length = 0\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = self.index_dic[pid]\r\n",
        "            num = len(idxs)\r\n",
        "            if num < self.num_instances:\r\n",
        "                num = self.num_instances\r\n",
        "            self.length += num - num % self.num_instances\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        batch_idxs_dict = defaultdict(list)\r\n",
        "\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = copy.deepcopy(self.index_dic[pid])\r\n",
        "            if len(idxs) < self.num_instances:\r\n",
        "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\r\n",
        "            random.shuffle(idxs)\r\n",
        "            batch_idxs = []\r\n",
        "            for idx in idxs:\r\n",
        "                batch_idxs.append(idx)\r\n",
        "                if len(batch_idxs) == self.num_instances:\r\n",
        "                    batch_idxs_dict[pid].append(batch_idxs)\r\n",
        "                    batch_idxs = []\r\n",
        "\r\n",
        "        avai_pids = copy.deepcopy(self.pids)\r\n",
        "        final_idxs = []\r\n",
        "\r\n",
        "        while len(avai_pids) >= self.num_pids_per_batch:\r\n",
        "            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)\r\n",
        "            for pid in selected_pids:\r\n",
        "                batch_idxs = batch_idxs_dict[pid].pop(0)\r\n",
        "                final_idxs.extend(batch_idxs)\r\n",
        "                if len(batch_idxs_dict[pid]) == 0:\r\n",
        "                    avai_pids.remove(pid)\r\n",
        "\r\n",
        "        self.length = len(final_idxs)\r\n",
        "        return iter(final_idxs)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.length"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lNEiP9cqLly"
      },
      "source": [
        "##数据集和loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHnlyMuUqOJg"
      },
      "source": [
        "import glob\r\n",
        "import re\r\n",
        "import os.path as osp\r\n",
        "class Market1501():\r\n",
        "    def __init__(self, root='/content/datasets', verbose=True, **kwargs):\r\n",
        "        super(Market1501, self).__init__()\r\n",
        "        self.dataset_dir = root\r\n",
        "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\r\n",
        "        self.query_dir = osp.join(self.dataset_dir, 'query')\r\n",
        "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\r\n",
        "        self._check_before_run()\r\n",
        "\r\n",
        "        train = self._process_dir(self.train_dir, relabel=True)\r\n",
        "        query = self._process_dir(self.query_dir, relabel=False)\r\n",
        "        gallery = self._process_dir(self.gallery_dir, relabel=False)\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"=> Market1501 loaded\")\r\n",
        "            self.print_dataset_statistics(train, query, gallery)\r\n",
        "\r\n",
        "        self.train = train\r\n",
        "        self.query = query\r\n",
        "        self.gallery = gallery\r\n",
        "\r\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "    def print_dataset_statistics(self, train, query, gallery):\r\n",
        "        num_train_pids, num_train_imgs, num_train_cams = self.get_imagedata_info(train)\r\n",
        "        num_query_pids, num_query_imgs, num_query_cams = self.get_imagedata_info(query)\r\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams = self.get_imagedata_info(gallery)\r\n",
        "\r\n",
        "        print(\"Dataset statistics:\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\r\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\r\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "    \r\n",
        "    def get_imagedata_info(self, data):\r\n",
        "        pids, cams = [], []\r\n",
        "        for _, pid, camid,_ in data:\r\n",
        "            pids += [pid]\r\n",
        "            cams += [camid]\r\n",
        "        pids = set(pids)\r\n",
        "        cams = set(cams)\r\n",
        "        num_pids = len(pids)\r\n",
        "        num_cams = len(cams)\r\n",
        "        num_imgs = len(data)\r\n",
        "        return num_pids, num_imgs, num_cams\r\n",
        "    \r\n",
        "\r\n",
        "    def _check_before_run(self):\r\n",
        "        \"\"\"Check if all files are available before going deeper\"\"\"\r\n",
        "        if not osp.exists(self.dataset_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\r\n",
        "        if not osp.exists(self.train_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\r\n",
        "        if not osp.exists(self.query_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\r\n",
        "        if not osp.exists(self.gallery_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\r\n",
        "\r\n",
        "    \r\n",
        "    ##最终返回的是每张图片的路径，pid，camid\r\n",
        "    def _process_dir(self, dir_path, relabel=False):\r\n",
        "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\r\n",
        "\r\n",
        "        ##匹配文件名中的有用信息，0001_c1代表pid=1，cid=1，\r\n",
        "        ##至于为什么是[-\\d]+,是因为有-1_c1这种的存在，也应该匹配上\r\n",
        "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\r\n",
        "\r\n",
        "        pid_container = set()\r\n",
        "        for img_path in img_paths:\r\n",
        "            pid, _ = map(int, pattern.search(img_path).groups())\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            pid_container.add(pid)\r\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\r\n",
        "\r\n",
        "        dataset = []\r\n",
        "        for img_path in img_paths:\r\n",
        "            pid, camid = map(int, pattern.search(img_path).groups())\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            assert 0 <= pid <= 1501  # pid == 0 means background\r\n",
        "            assert 1 <= camid <= 6\r\n",
        "            camid -= 1  # index starts from 0\r\n",
        "            if relabel: pid = pid2label[pid]\r\n",
        "            dataset.append((img_path, pid, camid,12))\r\n",
        "\r\n",
        "        return dataset\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blzE30o-uuhG"
      },
      "source": [
        "from fastai.vision import get_image_files\r\n",
        "import torch.utils.data as Data\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "class PDataset:\r\n",
        "  def __init__(self,root):\r\n",
        "    self.root=root\r\n",
        "    self.train_dir=osp.join(self.root,'train')\r\n",
        "    self.query_dir=osp.join(self.root,'query')\r\n",
        "    self.gallery_dir=osp.join(self.root,'gallery')\r\n",
        "    \r\n",
        "    train=self._process_dir(self.train_dir)\r\n",
        "    query=self._process_dir(self.query_dir)\r\n",
        "    gallery=self._process_dir(self.gallery_dir)\r\n",
        "    self.train=train\r\n",
        "    self.query=query\r\n",
        "    self.gallery=gallery\r\n",
        "    self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "    self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "    self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "  \r\n",
        "  def get_imagedata_info(self,data):\r\n",
        "    pids,cams=[],[]\r\n",
        "    for _,pid,camid,_ in data:\r\n",
        "      pids+=[pid]\r\n",
        "      cams+=[camid]\r\n",
        "    pids=set(pids)\r\n",
        "    cams=set(cams)\r\n",
        "    num_pids=len(pids)\r\n",
        "    num_cams=len(cams)\r\n",
        "    num_imgs=len(data)\r\n",
        "    return num_pids,num_imgs,num_cams\r\n",
        "\r\n",
        "  def _process_dir(self,dir_path):\r\n",
        "    image_names=get_image_files(dir_path)\r\n",
        "    dataset=[]\r\n",
        "    pid_set=set()\r\n",
        "    for name in image_names:\r\n",
        "      name=str(name)\r\n",
        "      path=name.split('/')[-1]\r\n",
        "      pid,_,date,_=path.split('_')\r\n",
        "      pid_set.add(pid)\r\n",
        "    pid2label={pid: label for label, pid in enumerate(pid_set)}\r\n",
        "    for name in image_names:\r\n",
        "      name=str(name)\r\n",
        "      path=name.split('/')[-1]\r\n",
        "      pid,_,date,_=path.split('_')\r\n",
        "      pid=pid2label[pid]\r\n",
        "      cam=image_cams[path]\r\n",
        "      date=date_ids[date]\r\n",
        "      dataset.append((name,int(pid),int(cam),int(date)))\r\n",
        "    return dataset\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDWGu6BNuxpn"
      },
      "source": [
        "import glob\r\n",
        "import re\r\n",
        "import os.path as osp\r\n",
        "class VC_Clothes():\r\n",
        "    def __init__(self, root='/content/datasets', verbose=True, **kwargs):\r\n",
        "        super(VC_Clothes, self).__init__()\r\n",
        "        self.dataset_dir = root\r\n",
        "        \r\n",
        "     \r\n",
        "\r\n",
        "        train = self._process_dir(self.dataset_dir, relabel=True)\r\n",
        "        query = self._process_dir(self.dataset_dir, relabel=False)\r\n",
        "        gallery = self._process_dir(self.dataset_dir, relabel=False)\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"=> VC loaded\")\r\n",
        "            self.print_dataset_statistics(train, query, gallery)\r\n",
        "\r\n",
        "        self.train = train\r\n",
        "        self.query = query\r\n",
        "        self.gallery = gallery\r\n",
        "\r\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "\r\n",
        "    def print_dataset_statistics(self, train, query, gallery):\r\n",
        "        num_train_pids, num_train_imgs, num_train_cams = self.get_imagedata_info(train)\r\n",
        "        num_query_pids, num_query_imgs, num_query_cams = self.get_imagedata_info(query)\r\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams = self.get_imagedata_info(gallery)\r\n",
        "\r\n",
        "        print(\"Dataset statistics:\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\r\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\r\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "    \r\n",
        "    def get_imagedata_info(self, data):\r\n",
        "        pids, cams = [], []\r\n",
        "        for _, pid, camid,_ in data:\r\n",
        "            pids += [pid]\r\n",
        "            cams += [camid]\r\n",
        "        pids = set(pids)\r\n",
        "        cams = set(cams)\r\n",
        "        num_pids = len(pids)\r\n",
        "        num_cams = len(cams)\r\n",
        "        num_imgs = len(data)\r\n",
        "        return num_pids, num_imgs, num_cams\r\n",
        "    \r\n",
        "\r\n",
        "  \r\n",
        "    \r\n",
        "    ##最终返回的是每张图片的路径，pid，camid\r\n",
        "    def _process_dir(self, dir_path, relabel=False):\r\n",
        "        image_names=get_image_files(dir_path)\r\n",
        "        \r\n",
        "        pid_container = set()\r\n",
        "        for name in image_names:\r\n",
        "            name=str(name)\r\n",
        "            pid,date,_,_=name.split('-')\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            pid_container.add(pid)\r\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\r\n",
        "\r\n",
        "        dataset = []\r\n",
        "        for name in image_names:\r\n",
        "            name=str(name)\r\n",
        "            pid,date,_,_=name.split('-')\r\n",
        "            if relabel: pid = pid2label[pid]\r\n",
        "            dataset.append((name, pid, 1,1))\r\n",
        "        return dataset\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krCw9XxdCCYB"
      },
      "source": [
        "class CUHK03():\r\n",
        "    def __init__(self, root='/content/datasets', verbose=True, **kwargs):\r\n",
        "        super(CUHK03, self).__init__()\r\n",
        "        self.dataset_dir = root\r\n",
        "        \r\n",
        "     \r\n",
        "\r\n",
        "        train = self._process_dir(self.dataset_dir, relabel=True)\r\n",
        "        query = self._process_dir(self.dataset_dir, relabel=False)\r\n",
        "        gallery = self._process_dir(self.dataset_dir, relabel=False)\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"=> cuhk loaded\")\r\n",
        "            self.print_dataset_statistics(train, query, gallery)\r\n",
        "\r\n",
        "        self.train = train\r\n",
        "        self.query = query\r\n",
        "        self.gallery = gallery\r\n",
        "\r\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "\r\n",
        "    def print_dataset_statistics(self, train, query, gallery):\r\n",
        "        num_train_pids, num_train_imgs, num_train_cams = self.get_imagedata_info(train)\r\n",
        "        num_query_pids, num_query_imgs, num_query_cams = self.get_imagedata_info(query)\r\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams = self.get_imagedata_info(gallery)\r\n",
        "\r\n",
        "        print(\"Dataset statistics:\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\r\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\r\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "    \r\n",
        "    def get_imagedata_info(self, data):\r\n",
        "        pids, cams = [], []\r\n",
        "        for _, pid, camid,_ in data:\r\n",
        "            pids += [pid]\r\n",
        "            cams += [camid]\r\n",
        "        pids = set(pids)\r\n",
        "        cams = set(cams)\r\n",
        "        num_pids = len(pids)\r\n",
        "        num_cams = len(cams)\r\n",
        "        num_imgs = len(data)\r\n",
        "        return num_pids, num_imgs, num_cams\r\n",
        "    \r\n",
        "\r\n",
        "  \r\n",
        "    \r\n",
        "    ##最终返回的是每张图片的路径，pid，camid\r\n",
        "    def _process_dir(self, dir_path, relabel=False):\r\n",
        "        image_names=get_image_files(dir_path)\r\n",
        "        \r\n",
        "        pid_container = set()\r\n",
        "        for name in image_names:\r\n",
        "            name=str(name)\r\n",
        "            path=name.split('/')[-1]\r\n",
        "            pid,date,_,_=path.split('_')\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            pid_container.add(pid)\r\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\r\n",
        "\r\n",
        "        dataset = []\r\n",
        "        for name in image_names:\r\n",
        "            name=str(name)\r\n",
        "            path=name.split('/')[-1]\r\n",
        "            pid,date,_,_=path.split('_')\r\n",
        "            if relabel: pid = pid2label[pid]\r\n",
        "            dataset.append((name, pid, 1,1))\r\n",
        "        return dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg404BowCtTV"
      },
      "source": [
        "class Real28():\r\n",
        "    def __init__(self, root='/content/datasets', verbose=True, **kwargs):\r\n",
        "        super(Real28, self).__init__()\r\n",
        "        self.dataset_dir = root\r\n",
        "        \r\n",
        "     \r\n",
        "\r\n",
        "        train = self._process_dir(self.dataset_dir, relabel=True)\r\n",
        "        query = self._process_dir(self.dataset_dir, relabel=False)\r\n",
        "        gallery = self._process_dir(self.dataset_dir, relabel=False)\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"=> real loaded\")\r\n",
        "            self.print_dataset_statistics(train, query, gallery)\r\n",
        "\r\n",
        "        self.train = train\r\n",
        "        self.query = query\r\n",
        "        self.gallery = gallery\r\n",
        "\r\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "\r\n",
        "    def print_dataset_statistics(self, train, query, gallery):\r\n",
        "        num_train_pids, num_train_imgs, num_train_cams = self.get_imagedata_info(train)\r\n",
        "        num_query_pids, num_query_imgs, num_query_cams = self.get_imagedata_info(query)\r\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams = self.get_imagedata_info(gallery)\r\n",
        "\r\n",
        "        print(\"Dataset statistics:\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\r\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\r\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "    \r\n",
        "    def get_imagedata_info(self, data):\r\n",
        "        pids, cams = [], []\r\n",
        "        for _, pid, camid,_ in data:\r\n",
        "            pids += [pid]\r\n",
        "            cams += [camid]\r\n",
        "        pids = set(pids)\r\n",
        "        cams = set(cams)\r\n",
        "        num_pids = len(pids)\r\n",
        "        num_cams = len(cams)\r\n",
        "        num_imgs = len(data)\r\n",
        "        return num_pids, num_imgs, num_cams\r\n",
        "    \r\n",
        "    \r\n",
        "    ##最终返回的是每张图片的路径，pid，camid\r\n",
        "    def _process_dir(self, dir_path, relabel=False):\r\n",
        "        image_names=get_image_files(dir_path)\r\n",
        "        pid_container = set()\r\n",
        "        for name in image_names:\r\n",
        "            name=str(name)\r\n",
        "            path=name.split('/')[-1]\r\n",
        "            pid,date,_,_=path.split('_')\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            pid_container.add(pid)\r\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\r\n",
        "        dataset = []\r\n",
        "        for name in image_names:\r\n",
        "            name=str(name)\r\n",
        "            path=name.split('/')[-1]\r\n",
        "            pid,date,_,_=path.split('_')\r\n",
        "            if relabel: pid = pid2label[pid]\r\n",
        "            dataset.append((name, pid, 1,1))\r\n",
        "        return dataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZZRpw5ICpJP"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "import torch.utils.data as Data\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "\r\n",
        "class ImageDataset(Data.Dataset):\r\n",
        "    def __init__(self, dataset, transform=None,ori_transform=None,pid_add=0):\r\n",
        "        self.dataset = dataset\r\n",
        "        self.transform = transform\r\n",
        "        self.ori_transform=ori_transform\r\n",
        "        self.pid_add=pid_add\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataset)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        img_path, pid, camid,date = self.dataset[index]\r\n",
        "        img = Image.open(img_path)\r\n",
        "        if self.transform is not None:\r\n",
        "            img = self.transform(img)\r\n",
        "        if self.ori_transform is not None:\r\n",
        "            ori_img=self.transform(img)\r\n",
        "        return img,ori_img ,pid+self.pid_add, camid, img_path,date\r\n",
        "def train_collate_fn1(batch):\r\n",
        "    imgs, ori_img,pids, _, _,_ = zip(*batch)\r\n",
        "    pids = torch.tensor(pids, dtype=torch.int64)\r\n",
        "    return torch.stack(imgs, dim=0),torch.stack(ori_img,dim=0), pids\r\n",
        "\r\n",
        "###对于验证集而言，为了提高验证的真实性，我们应该防止同一摄像头的图片进入验证（同一摄像头相当于数据泄露）\r\n",
        "def val_collate_fn1(batch):\r\n",
        "    imgs, ori_img,pids, camids, _,dates = zip(*batch)\r\n",
        "    return torch.stack(imgs, dim=0), torch.stack(ori_img,dim=0),pids, camids,dates\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def make_dataloader(datasets,cfg,pid_add=0):\r\n",
        "    train_transform=build_change_transforms(cfg,training=True)\r\n",
        "    test_transform=build_change_transforms(cfg,training=False)\r\n",
        "    ori_transform=build_origin_transform(cfg)\r\n",
        "   \r\n",
        "    data=datasets\r\n",
        "\r\n",
        "    num_classes=data.num_train_pids\r\n",
        "    train_set=ImageDataset(data.train,train_transform,ori_transform,pid_add=pid_add)\r\n",
        "\r\n",
        "    if cfg['SAMPLER']=='softmax':\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     shuffle=True,collate_fn=train_collate_fn1)\r\n",
        "    else:\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     sampler=DateRandomIdentitySampler(data.train,\r\n",
        "                                    cfg['train_bs'],cfg['train_K_instances']),\r\n",
        "                                    num_workers=cfg['num_workers'],\r\n",
        "                                    collate_fn=train_collate_fn1)\r\n",
        "    \r\n",
        "    val_set = ImageDataset(data.query + data.gallery, test_transform,ori_transform)\r\n",
        "    val_loader = Data.DataLoader(\r\n",
        "        val_set, batch_size=cfg['test_bs'], shuffle=False, num_workers=cfg['num_workers'],\r\n",
        "        collate_fn=val_collate_fn1\r\n",
        "    )\r\n",
        "    return train_loader, val_loader, len(data.query), num_classes\r\n",
        "\r\n",
        "def MAKE_DATALOADER(cfg):\r\n",
        "  train_names=cfg['train_data']\r\n",
        "  val_name=cfg['val_data']\r\n",
        "  val_loader_dict={}\r\n",
        "  train_loaders=[]\r\n",
        "  all_num_class=0\r\n",
        "  for name in train_names:\r\n",
        "    dataset=data_rott[name]\r\n",
        "    train_loader,val_loader,num_query,num_classes=make_dataloader(dataset,cfg,all_num_class)\r\n",
        "    train_loaders.append(train_loader)\r\n",
        "    val_loader_dict[name]=(val_loader,num_query)\r\n",
        "    all_num_class+=num_classes\r\n",
        "  val_loader,num_query=val_loader_dict[val_name]\r\n",
        "  return train_loaders,val_loader,num_query,all_num_class\r\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyGgwzNtFXik"
      },
      "source": [
        "#loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZdpxiDYFYzx"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "class CrossEntropyLabelSmoothLoss(nn.Module):\r\n",
        "    \"\"\"Cross entropy loss with label smoothing regularizer.\r\n",
        "    Reference:\r\n",
        "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\r\n",
        "    Equation: y = (1 - epsilon) * y + epsilon / K.\r\n",
        "    Args:\r\n",
        "        num_classes (int): number of classes.\r\n",
        "        epsilon (float): weight.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, num_classes, epsilon=0.1, use_gpu=True):\r\n",
        "        super(CrossEntropyLabelSmoothLoss, self).__init__()\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.epsilon = epsilon\r\n",
        "        self.use_gpu = use_gpu\r\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, inputs, targets):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\r\n",
        "            targets: ground truth labels with shape (num_classes)\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        log_probs = self.logsoftmax(inputs)\r\n",
        "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).cpu(), 1)\r\n",
        "        if self.use_gpu: targets = targets.cuda()\r\n",
        "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\r\n",
        "        loss = (- targets * log_probs).mean(0).sum()\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n",
        "from typing import Tuple\r\n",
        "from torch import Tensor\r\n",
        "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\r\n",
        "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\r\n",
        "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\r\n",
        "\r\n",
        "    positive_matrix = label_matrix.triu(diagonal=1)\r\n",
        "    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\r\n",
        "\r\n",
        "    similarity_matrix = similarity_matrix.view(-1)\r\n",
        "    positive_matrix = positive_matrix.view(-1)\r\n",
        "    negative_matrix = negative_matrix.view(-1)\r\n",
        "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\r\n",
        "\r\n",
        "\r\n",
        "def normalize(x, axis=-1):\r\n",
        "\r\n",
        "\tx = 1. * x / (torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12)\r\n",
        "\treturn x\r\n",
        "class CircleLoss(nn.Module):\r\n",
        "    def __init__(self, m: float, gamma: float) -> None:\r\n",
        "        super(CircleLoss, self).__init__()\r\n",
        "        self.m = m\r\n",
        "        self.gamma = gamma\r\n",
        "        self.soft_plus = nn.Softplus()\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self,feat,label) -> Tensor:\r\n",
        "\r\n",
        "        feat=normalize(feat,axis=-1)\r\n",
        "        sp, sn = convert_label_to_similarity(feat, label)\r\n",
        "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\r\n",
        "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\r\n",
        "\r\n",
        "        delta_p = 1 - self.m\r\n",
        "        delta_n = self.m\r\n",
        "\r\n",
        "        logit_p = - ap * (sp - delta_p) * self.gamma\r\n",
        "        logit_n = an * (sn - delta_n) * self.gamma\r\n",
        "\r\n",
        "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "\r\n",
        "def hard_sigmoid(x, c=2.5):\r\n",
        "    if x < -c:\r\n",
        "        return 0\r\n",
        "    if x > c:\r\n",
        "        return 1\r\n",
        "    return x / (2 * c) + 0.5\r\n",
        "\r\n",
        "class Adaptive_l2_loss(nn.Module):\r\n",
        "  def __init__(self,kernel_regularization_factor=0.005,bias_regularization_factor=0.005):\r\n",
        "    self.kernel_regularization_factor=kernel_regularization_factor\r\n",
        "    self.bias_regularization_factor=bias_regularization_factor\r\n",
        "    super(Adaptive_l2_loss,self).__init__()\r\n",
        "\r\n",
        "  def forward(self,out,pid):\r\n",
        "    model=out['model']\r\n",
        "    l2_reg = Variable(torch.tensor(0.0), requires_grad=True)\r\n",
        "    i = 0\r\n",
        "    for name, params in model.named_parameters():\r\n",
        "        if \"conv\" in name:\r\n",
        "            if \"weight\" in name:\r\n",
        "                l2_reg = l2_reg + hard_sigmoid(self.kernel_regularization_factor * model.adaptive_param[i] * params.norm(2))\r\n",
        "                i += 1\r\n",
        "            if \"bias\" in name:\r\n",
        "                l2_reg = l2_reg + hard_sigmoid(self.bias_regularization_factor * model.adaptive_param[i] * params.norm(2))\r\n",
        "                i += 1\r\n",
        "\r\n",
        "    return l2_reg"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGVr-TNWFdCi"
      },
      "source": [
        "#scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIFVn1ZlG9Ov"
      },
      "source": [
        "from bisect import bisect_right\r\n",
        "import torch\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        milestones,\r\n",
        "        gamma=0.1,\r\n",
        "        warmup_factor=1.0 / 3,\r\n",
        "        warmup_iters=500,\r\n",
        "        warmup_method=\"linear\",\r\n",
        "        last_epoch=-1,\r\n",
        "    ):\r\n",
        "        if not list(milestones) == sorted(milestones):\r\n",
        "            raise ValueError(\r\n",
        "                \"Milestones should be a list of\" \" increasing integers. Got {}\",\r\n",
        "                milestones,\r\n",
        "            )\r\n",
        "\r\n",
        "        if warmup_method not in (\"constant\", \"linear\"):\r\n",
        "            raise ValueError(\r\n",
        "                \"Only 'constant' or 'linear' warmup_method accepted\"\r\n",
        "                \"got {}\".format(warmup_method)\r\n",
        "            )\r\n",
        "        self.milestones = milestones\r\n",
        "        self.gamma = gamma\r\n",
        "        self.warmup_factor = warmup_factor\r\n",
        "        self.warmup_iters = warmup_iters\r\n",
        "        self.warmup_method = warmup_method\r\n",
        "        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\r\n",
        "\r\n",
        "    def get_lr(self):\r\n",
        "        warmup_factor = 1\r\n",
        "        if self.last_epoch < self.warmup_iters:\r\n",
        "            if self.warmup_method == \"constant\":\r\n",
        "                warmup_factor = self.warmup_factor\r\n",
        "            elif self.warmup_method == \"linear\":\r\n",
        "                alpha = self.last_epoch / self.warmup_iters\r\n",
        "                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\r\n",
        "        return [\r\n",
        "            base_lr\r\n",
        "            * warmup_factor\r\n",
        "            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\r\n",
        "            for base_lr in self.base_lrs\r\n",
        "        ]            \r\n",
        "class Flat(torch.optim.lr_scheduler._LRScheduler):\r\n",
        "  def __init__(self,optimizer,cfg,anneal_start=0.65,last_epoch=-1):\r\n",
        "    self.epochs=cfg['epochs']\r\n",
        "    self.start=anneal_start\r\n",
        "    super(Flat,self).__init__(optimizer,last_epoch)\r\n",
        "  \r\n",
        "  def get_lr(self):\r\n",
        "    if self.last_epoch<self.epochs*self.start:\r\n",
        "      return [base_lr for base_lr in self.base_lrs]\r\n",
        "    else:\r\n",
        "      return [\r\n",
        "        base_lr*(1+math.cos(math.pi*self.last_epoch/5))/2  \r\n",
        "        for base_lr in self.base_lrs    \r\n",
        "      ]\r\n",
        "     \r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O9pqBKEH9dV"
      },
      "source": [
        "#辅助函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPDgrILIAkg"
      },
      "source": [
        "class Checkpoint:\r\n",
        "  def __init__(self,ckpt):\r\n",
        "    self.ckpt=ckpt\r\n",
        "    self.init=-1000\r\n",
        "  \r\n",
        "  def __call__(self,metric,model,optimizer):\r\n",
        "    if metric>self.init:\r\n",
        "      self.init=metric\r\n",
        "      torch.save({'model':model.state_dict(),'optimizer':optimizer.state_dict()},self.ckpt)\r\n",
        "    \r\n",
        "from thop import profile\r\n",
        "from thop import clever_format\r\n",
        "\r\n",
        "def count_your_model(model, x, y):\r\n",
        "  outs=model(x)\r\n",
        "  \r\n",
        "def get_pf(model):\r\n",
        "\r\n",
        "\r\n",
        "  device=torch.device('cuda:0')\r\n",
        "  input = torch.randn(1, 3, 384, 128)\r\n",
        "  input=input.to(device)\r\n",
        "  flops, params = profile(model, inputs=(input, ))\r\n",
        "  flops, params = clever_format([flops, params], \"%.3f\")\r\n",
        "  return flops,params\r\n",
        "\r\n",
        "class EMA():\r\n",
        "    def __init__(self, model, decay):\r\n",
        "        self.model = model\r\n",
        "        self.decay = decay\r\n",
        "        self.shadow = {}\r\n",
        "        self.backup = {}\r\n",
        "\r\n",
        "    def register(self):\r\n",
        "        for name, param in self.model.named_parameters():\r\n",
        "            if param.requires_grad:\r\n",
        "                self.shadow[name] = param.data.clone()\r\n",
        "\r\n",
        "    def update(self):\r\n",
        "        for name, param in self.model.named_parameters():\r\n",
        "            if param.requires_grad:\r\n",
        "                assert name in self.shadow\r\n",
        "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\r\n",
        "                self.shadow[name] = new_average.clone()\r\n",
        "    \r\n",
        "    def apply_shadow(self):\r\n",
        "        for name, param in self.model.named_parameters():\r\n",
        "            if param.requires_grad:\r\n",
        "                assert name in self.shadow\r\n",
        "                self.backup[name] = param.data\r\n",
        "                param.data = self.shadow[name]\r\n",
        "    \r\n",
        "    def restore(self):\r\n",
        "        for name, param in self.model.named_parameters():\r\n",
        "            if param.requires_grad:\r\n",
        "                assert name in self.backup\r\n",
        "                param.data = self.backup[name]\r\n",
        "        self.backup = {}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq664kQWGfQM"
      },
      "source": [
        "#Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Gzfk2yGhkk"
      },
      "source": [
        "'''reid计算map函数'''\r\n",
        "def MARKET_EVAL_FUNC(dismat,q_pids,g_pids,q_camids,g_camids,max_rank=50):\r\n",
        "    '''\r\n",
        "    :param dismat: 每个query与其对应查询结果的距离\r\n",
        "    :param q_pids: 每个query行人的id\r\n",
        "    :param g_pids: 每个查询结果的行人的id\r\n",
        "    :param q_camids: 摄像头id，对于同一个query，如果查询结果与query都来自一个摄像头，则丢弃\r\n",
        "    :param g_camids: 同上\r\n",
        "    :param max_rank:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    num_q,num_g=dismat.shape\r\n",
        "    if num_g<max_rank:\r\n",
        "        max_rank=num_g\r\n",
        "\r\n",
        "    ##根据相似度进行排序\r\n",
        "    indices=np.argsort(dismat,axis=1)\r\n",
        "    \r\n",
        "    \r\n",
        "    ##在排序结果找到同id图片，用于计算cmc\r\n",
        "    matchs=(g_pids[indices]==q_pids[:,np.newaxis]).astype(np.int32)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    all_cmc=[]\r\n",
        "    all_AP=[]\r\n",
        "    num_valid_q=0\r\n",
        "\r\n",
        "    for q_idx in range(num_q):\r\n",
        "        q_pid=q_pids[q_idx]\r\n",
        "        q_camid=q_camids[q_idx]\r\n",
        "\r\n",
        "        '''当前query对应查询结果的排序'''\r\n",
        "        order=indices[q_idx]\r\n",
        "\r\n",
        "        '''若查询结果与query同摄像机且同id，删除'''\r\n",
        "        remove=(g_pids[order]==q_pid)&(g_camids[order]==q_camid)\r\n",
        "        keep=np.invert(remove)\r\n",
        "\r\n",
        "        '''计算CMC'''\r\n",
        "\r\n",
        "        '''首先先筛选'''\r\n",
        "        orig_cmc=matchs[q_idx][keep]\r\n",
        "\r\n",
        "        '''全是false'''\r\n",
        "        if not np.any(orig_cmc):\r\n",
        "            continue\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "        '''累加和，因为orig_cmc里面是true，false，true为1，得到cmc列表'''\r\n",
        "        cmc=orig_cmc.cumsum()\r\n",
        "        cmc[cmc>1]=1\r\n",
        "        all_cmc.append(cmc[:max_rank])\r\n",
        "        num_valid_q+=1\r\n",
        "\r\n",
        "        '''计算map'''\r\n",
        "\r\n",
        "        '''预测正确的数量'''\r\n",
        "        num_rel=orig_cmc.sum()\r\n",
        "\r\n",
        "        '''累加和，用于计算'''\r\n",
        "        tmp_cmc=orig_cmc.cumsum()\r\n",
        "\r\n",
        "        '''计算准确率，前k个查询结果的准确率'''\r\n",
        "        tmp_cmc=[x/(i+1) for i,x in enumerate(tmp_cmc)]\r\n",
        "\r\n",
        "        '''计算召回率，召回率每次变化都是当查询结果id等于query时，故乘orig_cmc'''\r\n",
        "        '''最终的结果类似 0,0,1,0,0,2,0,0,3这种，每个有值的都是查询正确的'''\r\n",
        "        '''然后后面除以查询结果中同id数量，也就是上面num_rel就是找召回率'''\r\n",
        "        '''但是这里其实已经提前乘上准确率了，最终结果类似于 0,0,1/3,0,0,2/6,0,0,3/9'''\r\n",
        "        tmp_cmc=np.asarray(tmp_cmc)*orig_cmc\r\n",
        "\r\n",
        "        '''以上为例，除以同id数量后，假设为五个'''\r\n",
        "        '''0，0，1/3 /5，0，0，2/6 /5，0，0，3/9 /5'''\r\n",
        "        '''等价于 0,0, 1/3(准确率)*1/5(召回率) ....'''\r\n",
        "        \r\n",
        "        if num_rel==0:\r\n",
        "          AP=0\r\n",
        "          print('AP=0')\r\n",
        "        else:  \r\n",
        "          AP = tmp_cmc.sum() / num_rel\r\n",
        "          \r\n",
        "        all_AP.append(AP)\r\n",
        "\r\n",
        "    all_cmc = np.asarray(all_cmc).astype(np.float32)\r\n",
        "    all_cmc = all_cmc.sum(0) / num_valid_q\r\n",
        "    if len(all_AP)==0:\r\n",
        "      print('all_ap=0')\r\n",
        "      return all_cmc,0\r\n",
        "    mAP = np.mean(all_AP)\r\n",
        "\r\n",
        "    return all_cmc, mAP\r\n",
        "\r\n",
        "\r\n",
        "class MARKET_MAP():\r\n",
        "    def __init__(self,num_query,max_rank=50,feat_norm=True,one_day=True,all=False,date_length=11):\r\n",
        "        '''\r\n",
        "\r\n",
        "        :param num_query: 查询数量\r\n",
        "        :param max_rank: 每个query限定的最大查询结果数量\r\n",
        "        :param feat_norm:是否对特征进行l2norm\r\n",
        "        '''\r\n",
        "        self.num_query=num_query\r\n",
        "        self.max_rank=max_rank\r\n",
        "        self.feat_norm=feat_norm\r\n",
        "        self.one_day=one_day\r\n",
        "        self.all=all\r\n",
        "        self.date_length=date_length\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        '''\r\n",
        "        feats:模型返回的特征\r\n",
        "        pids: 图片中行人的id\r\n",
        "        camids:图片摄像头的id\r\n",
        "        :return:\r\n",
        "        '''\r\n",
        "        self.feats=[]\r\n",
        "        self.pids=[]\r\n",
        "        self.camids=[]\r\n",
        "        self.dates=[]\r\n",
        "\r\n",
        "    def update(self,output):\r\n",
        "        feat,pid,camid,date=output\r\n",
        "    \r\n",
        "        self.feats.append(feat)\r\n",
        "        self.pids.append(pid)\r\n",
        "        self.camids.append(camid)\r\n",
        "        self.dates.append(date)\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        feats=torch.stack(self.feats,dim=0)\r\n",
        "          \r\n",
        "        if self.feat_norm:\r\n",
        "            feats=torch.nn.functional.normalize(feats,p=2)\r\n",
        "\r\n",
        "        \r\n",
        "        '''用于查询的图片的特征及其id和摄像头id'''\r\n",
        "        \r\n",
        "        qf=feats[:self.num_query]\r\n",
        "        q_pids=np.asarray(self.pids[:self.num_query])\r\n",
        "        q_camids = np.asarray(self.camids[:self.num_query])\r\n",
        "        q_dates=np.asarray(self.dates[:self.num_query])  \r\n",
        "\r\n",
        "        '''每个query查询结果的特征及其id和摄像头id'''\r\n",
        "        gf = feats[self.num_query:]\r\n",
        "        g_pids = np.asarray(self.pids[self.num_query:])\r\n",
        "        g_camids = np.asarray(self.camids[self.num_query:])\r\n",
        "        g_dates=np.asarray(self.dates[self.num_query:])\r\n",
        "        if self.all:\r\n",
        "          m,n=qf.shape[0],gf.shape[0]\r\n",
        "      \r\n",
        "          '''计算qf每个query和对应查询结果的距离'''\r\n",
        "          distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                    torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "        \r\n",
        "          distmat.addmm_(1, -2, qf, gf.t())\r\n",
        "          distmat = distmat.cpu().numpy()\r\n",
        "        \r\n",
        "          cmc,mAP=MARKET_EVAL_FUNC(distmat,q_pids,g_pids,q_camids,g_camids)\r\n",
        "          return mAP\r\n",
        "        else:\r\n",
        "          if self.one_day:\r\n",
        "            \r\n",
        "            mean_map=0\r\n",
        "            count=0\r\n",
        "            for date in range(self.date_length):\r\n",
        "              date=date+1\r\n",
        "              q_index=np.where(q_dates==date)\r\n",
        "              \r\n",
        "              date_qf=qf[q_index]\r\n",
        "              date_q_pids=q_pids[q_index]\r\n",
        "              date_q_camids=q_camids[q_index]\r\n",
        "\r\n",
        "              g_index=np.where(g_dates==date)\r\n",
        "              date_gf=gf[g_index]\r\n",
        "              date_g_pids=g_pids[g_index]\r\n",
        "              date_g_camids=g_camids[g_index]\r\n",
        "              m,n=date_qf.shape[0],date_gf.shape[0]\r\n",
        "\r\n",
        "              if m==0:\r\n",
        "                continue\r\n",
        "              distmat = torch.pow(date_qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                    torch.pow(date_gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "              \r\n",
        "              distmat.addmm_(1, -2, date_qf, date_gf.t())\r\n",
        "              distmat = distmat.cpu().numpy()\r\n",
        "              \r\n",
        "              cmc,mAP=MARKET_EVAL_FUNC(distmat,date_q_pids,date_g_pids,date_q_camids,date_g_camids)\r\n",
        "              mean_map+=mAP\r\n",
        "              count+=1\r\n",
        "            \r\n",
        "            if count==0:\r\n",
        "              return 0\r\n",
        "            return mean_map/count\r\n",
        "          else:\r\n",
        "           \r\n",
        "            mean_map=0\r\n",
        "            count=0\r\n",
        "            for date in range(self.date_length):\r\n",
        "              date=date+1\r\n",
        "              q_index=np.where(q_dates==date)\r\n",
        "              \r\n",
        "              date_qf=qf[q_index]\r\n",
        "              date_q_pids=q_pids[q_index]\r\n",
        "              date_q_camids=q_camids[q_index]\r\n",
        "\r\n",
        "              g_index=np.where(g_dates!=date)\r\n",
        "              date_gf=gf[g_index]\r\n",
        "              date_g_pids=g_pids[g_index]\r\n",
        "              date_g_camids=g_camids[g_index]\r\n",
        "              m,n=date_qf.shape[0],date_gf.shape[0]\r\n",
        "\r\n",
        "              if m==0 or n==0:\r\n",
        "                continue\r\n",
        "              distmat = torch.pow(date_qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                    torch.pow(date_gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "              \r\n",
        "              distmat.addmm_(1, -2, date_qf, date_gf.t())\r\n",
        "              distmat = distmat.cpu().numpy()\r\n",
        "              \r\n",
        "              cmc,mAP=MARKET_EVAL_FUNC(distmat,date_q_pids,date_g_pids,date_q_camids,date_g_camids)\r\n",
        "              mean_map+=mAP\r\n",
        "              count+=1\r\n",
        "            if count==0:\r\n",
        "              return 0\r\n",
        "            return mean_map/count"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGXW9IoODZX_"
      },
      "source": [
        "#model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcDxRf1UDb2N"
      },
      "source": [
        "##非主要函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYTUQz99DbO3"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "from lambda_networks import LambdaLayer\r\n",
        "def weights_init_kaiming(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find('Linear') != -1:\r\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\r\n",
        "        nn.init.constant_(m.bias, 0.0)\r\n",
        "    elif classname.find('Conv') != -1:\r\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\r\n",
        "        if m.bias is not None:\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "    elif classname.find('BatchNorm') != -1:\r\n",
        "        if m.affine:\r\n",
        "            nn.init.constant_(m.weight, 1.0)\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "\r\n",
        "def weights_init_classifier(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find('Linear') != -1:\r\n",
        "        nn.init.normal_(m.weight, std=0.001)\r\n",
        "        if m.bias:\r\n",
        "            nn.init.constant_(m.bias, 0.0)\r\n",
        "\r\n",
        "class ConvBlock(nn.Module):\r\n",
        "    \"\"\"Basic convolutional block.\r\n",
        "\r\n",
        "    convolution + batch normalization + relu.\r\n",
        "    Args:\r\n",
        "        in_c (int): number of input channels.\r\n",
        "        out_c (int): number of output channels.\r\n",
        "        k (int or tuple): kernel size.\r\n",
        "        s (int or tuple): stride.\r\n",
        "        p (int or tuple): padding.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_c, out_c, k, s=1, p=0):\r\n",
        "        super(ConvBlock, self).__init__()\r\n",
        "        self.conv = nn.Conv2d(in_c, out_c, k, stride=s, padding=p)\r\n",
        "        self.bn = nn.BatchNorm2d(out_c)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return F.relu(self.bn(self.conv(x)))\r\n",
        "\r\n",
        "\r\n",
        "class InceptionA(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, in_channels, out_channels):\r\n",
        "        super(InceptionA, self).__init__()\r\n",
        "        mid_channels = out_channels // 4\r\n",
        "\r\n",
        "        self.stream1 = nn.Sequential(\r\n",
        "            ConvBlock(in_channels, mid_channels, 1),\r\n",
        "            ConvBlock(mid_channels, mid_channels, 3, p=1),\r\n",
        "        )\r\n",
        "        self.stream2 = nn.Sequential(\r\n",
        "            ConvBlock(in_channels, mid_channels, 1),\r\n",
        "            ConvBlock(mid_channels, mid_channels, 3, p=1),\r\n",
        "        )\r\n",
        "        self.stream3 = nn.Sequential(\r\n",
        "            ConvBlock(in_channels, mid_channels, 1),\r\n",
        "            ConvBlock(mid_channels, mid_channels, 3, p=1),\r\n",
        "        )\r\n",
        "        self.stream4 = nn.Sequential(\r\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\r\n",
        "            ConvBlock(in_channels, mid_channels, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        s1 = self.stream1(x)\r\n",
        "        s2 = self.stream2(x)\r\n",
        "        s3 = self.stream3(x)\r\n",
        "        s4 = self.stream4(x)\r\n",
        "        y = torch.cat([s1, s2, s3, s4], dim=1)\r\n",
        "        return y\r\n",
        "\r\n",
        "\r\n",
        "class InceptionB(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, in_channels, out_channels):\r\n",
        "        super(InceptionB, self).__init__()\r\n",
        "        mid_channels = out_channels // 4\r\n",
        "\r\n",
        "        self.stream1 = nn.Sequential(\r\n",
        "            ConvBlock(in_channels, mid_channels, 1),\r\n",
        "            ConvBlock(mid_channels, mid_channels, 3, s=2, p=1),\r\n",
        "        )\r\n",
        "        self.stream2 = nn.Sequential(\r\n",
        "            ConvBlock(in_channels, mid_channels, 1),\r\n",
        "            ConvBlock(mid_channels, mid_channels, 3, p=1),\r\n",
        "            ConvBlock(mid_channels, mid_channels, 3, s=2, p=1),\r\n",
        "        )\r\n",
        "        self.stream3 = nn.Sequential(\r\n",
        "            nn.MaxPool2d(3, stride=2, padding=1),\r\n",
        "            ConvBlock(in_channels, mid_channels * 2, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        s1 = self.stream1(x)\r\n",
        "        s2 = self.stream2(x)\r\n",
        "        s3 = self.stream3(x)\r\n",
        "        y = torch.cat([s1, s2, s3], dim=1)\r\n",
        "        return y\r\n",
        "\r\n",
        "class Inception_block(nn.Module):\r\n",
        "    def __init__(self,channel1,channel2):\r\n",
        "        self.block1=InceptionA(channel1,channel2)\r\n",
        "        self.block2=InceptionB(channel2,channel2)\r\n",
        "        super(Inception_block, self).__init__()\r\n",
        "    def forward(self,x):\r\n",
        "        x=self.block1(x)\r\n",
        "        x=self.block2(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class DW_conv(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch,stride=1):\r\n",
        "        super(DW_conv, self).__init__()\r\n",
        "        self.depth_conv = nn.Conv2d(\r\n",
        "            in_channels=in_ch,\r\n",
        "            out_channels=in_ch,\r\n",
        "            kernel_size=3,\r\n",
        "            stride=1,\r\n",
        "            padding=1,\r\n",
        "            groups=in_ch\r\n",
        "        )\r\n",
        "        self.point_conv = nn.Conv2d(\r\n",
        "            in_channels=in_ch,\r\n",
        "            out_channels=out_ch,\r\n",
        "            kernel_size=1,\r\n",
        "            stride=stride,\r\n",
        "            padding=0,\r\n",
        "            groups=1\r\n",
        "        )\r\n",
        "        self.bn=nn.BatchNorm2d(out_ch)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        out = self.depth_conv(input)\r\n",
        "        out = self.point_conv(out)\r\n",
        "        out=self.bn(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "class ShuffleBlock(nn.Module):\r\n",
        "    def __init__(self, groups):\r\n",
        "        super(ShuffleBlock, self).__init__()\r\n",
        "        self.groups = groups\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\r\n",
        "        N,C,H,W = x.size()\r\n",
        "        g = self.groups\r\n",
        "        return x.view(N,g,C//g,H,W).permute(0,2,1,3,4).reshape(N,C,H,W)\r\n",
        "\r\n",
        "class Shuffle_A(nn.Module):\r\n",
        "    def __init__(self,inchannel):\r\n",
        "        split_channel=int(inchannel//2)\r\n",
        "        self.split_channel=split_channel\r\n",
        "        self.conv1=nn.Conv2d(split_channel,inchannel,kernel_size=1)\r\n",
        "        self.dw_conv=DW_conv(inchannel,inchannel)\r\n",
        "        self.conv2=nn.Conv2d(inchannel,split_channel,kernel_size=1)\r\n",
        "        self.bn_relu1=self.make_bn_relu(inchannel)\r\n",
        "        self.bn_relu2=self.make_bn_relu(inchannel)\r\n",
        "        self.relu=nn.ReLU()\r\n",
        "        self.channel_shuffle=ShuffleBlock(4)\r\n",
        "        super(Shuffle_A, self).__init__()\r\n",
        "\r\n",
        "    def make_bn_relu(self,in_channel):\r\n",
        "        return nn.Sequential(\r\n",
        "            nn.BatchNorm2d(in_channel),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "    def forward(self,x):\r\n",
        "        x_a=x[:,:self.split_channel,:,:]\r\n",
        "        x_b=x[:,self.split_channel:,:,:]\r\n",
        "\r\n",
        "        x_b=self.conv1(x_b)\r\n",
        "        x_b=self.bn_relu1(x_b)\r\n",
        "        x_b=self.dw_conv(x_b)\r\n",
        "        x_b=self.conv2(x_b)\r\n",
        "        x_b=self.bn_relu2(x_b)\r\n",
        "        \r\n",
        "        x=torch.cat([x_a,x_b],dim=1)\r\n",
        "        x=self.channel_shuffle(x)\r\n",
        "\r\n",
        "\r\n",
        "class Shuffle_B(nn.Module):\r\n",
        "    def __init__(self, inchannel,out_channel,stride=1):\r\n",
        "        split_channel = int(inchannel // 2)\r\n",
        "        self.split_channel = split_channel\r\n",
        "        self.conv1 = nn.Conv2d(split_channel, inchannel,kernel_size=1)\r\n",
        "        self.dw_conv = DW_conv(inchannel,inchannel,stride)\r\n",
        "        self.conv2 = nn.Conv2d(inchannel, out_channel,kernel_size=1)\r\n",
        "        self.bn_relu1 = self.make_bn_relu(inchannel)\r\n",
        "        self.bn_relu2 = self.make_bn_relu(out_channel)\r\n",
        "\r\n",
        "        self.dw_conv2=DW_conv(split_channel, split_channel,stride)\r\n",
        "        self.conv3=nn.Conv2d(split_channel,out_channel,kernel_size=1)\r\n",
        "        self.bn_relu3=self.make_bn_relu(out_channel)\r\n",
        "\r\n",
        "        self.channel_shuffle = ShuffleBlock(4)\r\n",
        "        super(Shuffle_B, self).__init__()\r\n",
        "    def make_bn_relu(self, in_channel):\r\n",
        "        return nn.Sequential(\r\n",
        "            nn.BatchNorm2d(in_channel),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x_a = x[:, :self.split_channel, :, :]\r\n",
        "        x_b = x[:, self.split_channel:, :, :]\r\n",
        "\r\n",
        "        x_b = self.conv1(x_b)\r\n",
        "        x_b = self.bn_relu1(x_b)\r\n",
        "        x_b = self.dw_conv(x_b)\r\n",
        "        x_b = self.conv2(x_b)\r\n",
        "        x_b = self.bn_relu2(x_b)\r\n",
        "\r\n",
        "        x_a=self.dw_conv2(x_a)\r\n",
        "        x_a=self.conv3(x_a)\r\n",
        "        x_a=self.bn_relu3(x_a)\r\n",
        "        x = torch.cat([x_a, x_b], dim=1)\r\n",
        "        x = self.channel_shuffle(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class SpatialAttn(nn.Module):\r\n",
        "    \"\"\"Spatial Attention (Sec. 3.1.I.1)\"\"\"\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(SpatialAttn, self).__init__()\r\n",
        "        self.conv1 = ConvBlock(1, 1, 3, s=2, p=1)\r\n",
        "        self.conv2 = ConvBlock(1, 1, 1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # global cross-channel averaging\r\n",
        "        x = x.mean(1, keepdim=True)\r\n",
        "        # 3-by-3 conv\r\n",
        "        x = self.conv1(x)\r\n",
        "        # bilinear resizing\r\n",
        "        x = F.upsample(\r\n",
        "            x, (x.size(2) * 2, x.size(3) * 2),\r\n",
        "            mode='bilinear',\r\n",
        "            align_corners=True\r\n",
        "        )\r\n",
        "        # scaling conv\r\n",
        "        x = self.conv2(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class ChannelAttn(nn.Module):\r\n",
        "    \"\"\"Channel Attention (Sec. 3.1.I.2)\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_channels, reduction_rate=16):\r\n",
        "        super(ChannelAttn, self).__init__()\r\n",
        "        assert in_channels % reduction_rate == 0\r\n",
        "        self.conv1 = ConvBlock(in_channels, in_channels // reduction_rate, 1)\r\n",
        "        self.conv2 = ConvBlock(in_channels // reduction_rate, in_channels, 1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # squeeze operation (global average pooling)\r\n",
        "        x = F.avg_pool2d(x, x.size()[2:])\r\n",
        "        # excitation operation (2 conv layers)\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.conv2(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class SoftAttn(nn.Module):\r\n",
        "    \"\"\"Soft Attention (Sec. 3.1.I)\r\n",
        "\r\n",
        "    Aim: Spatial Attention + Channel Attention\r\n",
        "\r\n",
        "    Output: attention maps with shape identical to input.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_channels):\r\n",
        "        super(SoftAttn, self).__init__()\r\n",
        "        self.spatial_attn = SpatialAttn()\r\n",
        "        self.channel_attn = ChannelAttn(in_channels)\r\n",
        "        self.conv = ConvBlock(in_channels, in_channels, 1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        y_spatial = self.spatial_attn(x)\r\n",
        "        y_channel = self.channel_attn(x)\r\n",
        "        y = y_spatial * y_channel\r\n",
        "        y = torch.sigmoid(self.conv(y))\r\n",
        "        return y\r\n",
        "\r\n",
        "\r\n",
        "class HardAttn(nn.Module):\r\n",
        "    \"\"\"Hard Attention (Sec. 3.1.II)\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_channels):\r\n",
        "        super(HardAttn, self).__init__()\r\n",
        "        self.fc = nn.Linear(in_channels, 4 * 2)\r\n",
        "        self.init_params()\r\n",
        "\r\n",
        "    def init_params(self):\r\n",
        "        self.fc.weight.data.zero_()\r\n",
        "        self.fc.bias.data.copy_(\r\n",
        "            torch.tensor(\r\n",
        "                [0, -0.75, 0, -0.25, 0, 0.25, 0, 0.75], dtype=torch.float\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # squeeze operation (global average pooling)\r\n",
        "        x = F.avg_pool2d(x, x.size()[2:]).view(x.size(0), x.size(1))\r\n",
        "        # predict transformation parameters\r\n",
        "        theta = torch.tanh(self.fc(x))\r\n",
        "        theta = theta.view(-1, 4, 2)\r\n",
        "        return theta\r\n",
        "\r\n",
        "\r\n",
        "class HarmAttn(nn.Module):\r\n",
        "    \"\"\"Harmonious Attention (Sec. 3.1)\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_channels):\r\n",
        "        super(HarmAttn, self).__init__()\r\n",
        "        self.soft_attn = SoftAttn(in_channels)\r\n",
        "        self.hard_attn = HardAttn(in_channels)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        y_soft_attn = self.soft_attn(x)\r\n",
        "        theta = self.hard_attn(x)\r\n",
        "        return y_soft_attn, theta\r\n",
        "\r\n",
        "\r\n",
        "def _make_divisible(v, divisor, min_value=None):\r\n",
        "    \"\"\"\r\n",
        "    This function is taken from the original tf repo.\r\n",
        "    It ensures that all layers have a channel number that is divisible by 8\r\n",
        "    It can be seen here:\r\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\r\n",
        "    \"\"\"\r\n",
        "    if min_value is None:\r\n",
        "        min_value = divisor\r\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\r\n",
        "    # Make sure that round down does not go down by more than 10%.\r\n",
        "    if new_v < 0.9 * v:\r\n",
        "        new_v += divisor\r\n",
        "    return new_v\r\n",
        "\r\n",
        "\r\n",
        "def hard_sigmoid(x, inplace: bool = False):\r\n",
        "    if inplace:\r\n",
        "        return x.add_(3.).clamp_(0., 6.).div_(6.)\r\n",
        "    else:\r\n",
        "        return F.relu6(x + 3.) / 6.\r\n",
        "\r\n",
        "import math\r\n",
        "class SqueezeExcite(nn.Module):\r\n",
        "    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\r\n",
        "                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\r\n",
        "        super(SqueezeExcite, self).__init__()\r\n",
        "        self.gate_fn = gate_fn\r\n",
        "        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\r\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\r\n",
        "        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\r\n",
        "        self.act1 = act_layer(inplace=True)\r\n",
        "        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x_se = self.avg_pool(x)\r\n",
        "        x_se = self.conv_reduce(x_se)\r\n",
        "        x_se = self.act1(x_se)\r\n",
        "        x_se = self.conv_expand(x_se)\r\n",
        "        x = x * self.gate_fn(x_se)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class ConvBnAct(nn.Module):\r\n",
        "    def __init__(self, in_chs, out_chs, kernel_size,\r\n",
        "                 stride=1, act_layer=nn.ReLU):\r\n",
        "        super(ConvBnAct, self).__init__()\r\n",
        "        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size // 2, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_chs)\r\n",
        "        self.act1 = act_layer(inplace=True)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.act1(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class GhostModule(nn.Module):\r\n",
        "    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\r\n",
        "        super(GhostModule, self).__init__()\r\n",
        "        self.oup = oup\r\n",
        "        init_channels = math.ceil(oup / ratio)\r\n",
        "        new_channels = init_channels * (ratio - 1)\r\n",
        "\r\n",
        "        self.primary_conv = nn.Sequential(\r\n",
        "            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size // 2, bias=False),\r\n",
        "            nn.BatchNorm2d(init_channels),\r\n",
        "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\r\n",
        "        )\r\n",
        "\r\n",
        "        self.cheap_operation = nn.Sequential(\r\n",
        "            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size // 2, groups=init_channels, bias=False),\r\n",
        "            nn.BatchNorm2d(new_channels),\r\n",
        "            nn.ReLU(inplace=True) if relu else nn.Sequential(),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x1 = self.primary_conv(x)\r\n",
        "        x2 = self.cheap_operation(x1)\r\n",
        "        out = torch.cat([x1, x2], dim=1)\r\n",
        "        return out[:, :self.oup, :, :]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class LambdaConv(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels, heads=4, k=16, u=1, m=23):\r\n",
        "        super(LambdaConv, self).__init__()\r\n",
        "        self.kk, self.uu, self.vv, self.mm, self.heads = k, u, out_channels // heads, m, heads\r\n",
        "        self.local_context = True if m > 0 else False\r\n",
        "        self.padding = (m - 1) // 2\r\n",
        "\r\n",
        "        self.queries = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels, k * heads, kernel_size=1, bias=False),\r\n",
        "            nn.BatchNorm2d(k * heads)\r\n",
        "        )\r\n",
        "        self.keys = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels, k * u, kernel_size=1, bias=False),\r\n",
        "        )\r\n",
        "        self.values = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels, self.vv * u, kernel_size=1, bias=False),\r\n",
        "            nn.BatchNorm2d(self.vv * u)\r\n",
        "        )\r\n",
        "\r\n",
        "        self.softmax = nn.Softmax(dim=-1)\r\n",
        "\r\n",
        "        if self.local_context:\r\n",
        "            self.embedding = nn.Parameter(torch.randn([self.kk, self.uu, 1, m, m]), requires_grad=True)\r\n",
        "        else:\r\n",
        "            self.embedding = nn.Parameter(torch.randn([self.kk, self.uu]), requires_grad=True)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        n_batch, C, w, h = x.size()\r\n",
        "\r\n",
        "        queries = self.queries(x).view(n_batch, self.heads, self.kk, w * h) # b, heads, k // heads, w * h\r\n",
        "        softmax = self.softmax(self.keys(x).view(n_batch, self.kk, self.uu, w * h)) # b, k, uu, w * h\r\n",
        "        values = self.values(x).view(n_batch, self.vv, self.uu, w * h) # b, v, uu, w * h\r\n",
        "\r\n",
        "        lambda_c = torch.einsum('bkum,bvum->bkv', softmax, values)\r\n",
        "        y_c = torch.einsum('bhkn,bkv->bhvn', queries, lambda_c)\r\n",
        "\r\n",
        "        if self.local_context:\r\n",
        "            values = values.view(n_batch, self.uu, -1, w, h)\r\n",
        "            lambda_p = F.conv3d(values, self.embedding, padding=(0, self.padding, self.padding))\r\n",
        "            lambda_p = lambda_p.view(n_batch, self.kk, self.vv, w * h)\r\n",
        "            y_p = torch.einsum('bhkn,bkvn->bhvn', queries, lambda_p)\r\n",
        "        else:\r\n",
        "            lambda_p = torch.einsum('ku,bvun->bkvn', self.embedding, values)\r\n",
        "            y_p = torch.einsum('bhkn,bkvn->bhvn', queries, lambda_p)\r\n",
        "\r\n",
        "        out = y_c + y_p\r\n",
        "        out = out.contiguous().view(n_batch, -1, w, h)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class LambdaBottleneck(nn.Module):\r\n",
        "    expansion = 4\r\n",
        "\r\n",
        "    def __init__(self, in_planes, planes, stride=1):\r\n",
        "        super(LambdaBottleneck, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\r\n",
        "\r\n",
        "        self.conv2 = nn.ModuleList([LambdaConv(planes, planes)])\r\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\r\n",
        "            self.conv2.append(nn.AvgPool2d(kernel_size=(3, 3), stride=stride, padding=(1, 1)))\r\n",
        "        self.conv2.append(nn.BatchNorm2d(planes))\r\n",
        "        self.conv2.append(nn.ReLU())\r\n",
        "        self.conv2 = nn.Sequential(*self.conv2)\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\r\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\r\n",
        "\r\n",
        "        self.shortcut = nn.Sequential()\r\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\r\n",
        "            self.shortcut = nn.Sequential(\r\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride),\r\n",
        "                nn.BatchNorm2d(self.expansion*planes)\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn3(self.conv3(out))\r\n",
        "        out += self.shortcut(x)\r\n",
        "        out = F.relu(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class ResNet(nn.Module):\r\n",
        "    def __init__(self, block, num_blocks, num_classes=1000):\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        self.in_planes = 64\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(64)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "\r\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0])\r\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\r\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\r\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=1)\r\n",
        "\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Dropout(0.3), # All architecture deeper than ResNet-200 dropout_rate: 0.2\r\n",
        "            nn.Linear(512 * block.expansion, num_classes)\r\n",
        "        )\r\n",
        "\r\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1):\r\n",
        "        strides = [stride] + [1]*(num_blocks-1)\r\n",
        "        layers = []\r\n",
        "        for idx, stride in enumerate(strides):\r\n",
        "            layers.append(block(self.in_planes, planes, stride))\r\n",
        "            self.in_planes = planes * block.expansion\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\r\n",
        "        out = self.maxpool(out)\r\n",
        "\r\n",
        "        out = self.layer1(out)\r\n",
        "        out = self.layer2(out)\r\n",
        "        out = self.layer3(out)\r\n",
        "        out = self.layer4(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "def LambdaResNet50():\r\n",
        "    return ResNet(LambdaBottleneck, [3, 4, 6, 3])\r\n",
        "\r\n",
        "\r\n",
        "class GeneralizedMeanPooling(nn.Module):\r\n",
        "    r\"\"\"Applies a 2D power-average adaptive pooling over an input signal composed of several input planes.\r\n",
        "    The function computed is: :math:`f(X) = pow(sum(pow(X, p)), 1/p)`\r\n",
        "        - At p = infinity, one gets Max Pooling\r\n",
        "        - At p = 1, one gets Average Pooling\r\n",
        "    The output is of size H x W, for any input size.\r\n",
        "    The number of output features is equal to the number of input planes.\r\n",
        "    Args:\r\n",
        "        output_size: the target output size of the image of the form H x W.\r\n",
        "                     Can be a tuple (H, W) or a single H for a square image H x H\r\n",
        "                     H and W can be either a ``int``, or ``None`` which means the size will\r\n",
        "                     be the same as that of the input.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, norm, output_size=1, eps=1e-6):\r\n",
        "        super(GeneralizedMeanPooling, self).__init__()\r\n",
        "        assert norm > 0\r\n",
        "        self.p = float(norm)\r\n",
        "        self.output_size = output_size\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x.clamp(min=self.eps).pow(self.p)\r\n",
        "        return torch.nn.functional.adaptive_avg_pool2d(x, self.output_size).pow(1. / self.p)\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__class__.__name__ + '(' \\\r\n",
        "               + str(self.p) + ', ' \\\r\n",
        "               + 'output_size=' + str(self.output_size) + ')'\r\n",
        "\r\n",
        "\r\n",
        "class GeneralizedMeanPoolingP(GeneralizedMeanPooling):\r\n",
        "    \"\"\" Same, but norm is trainable\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, norm=3, output_size=1, eps=1e-6):\r\n",
        "        super(GeneralizedMeanPoolingP, self).__init__(norm, output_size, eps)\r\n",
        "        self.p = nn.Parameter(torch.ones(1) * norm)\r\n",
        "\r\n",
        "class Flatten(nn.Module):\r\n",
        "    def forward(self, input):\r\n",
        "        return input.view(input.size(0), -1)\r\n",
        "def build_pool_conv(input_dim=2048,reduce_dim=512):\r\n",
        "    pool_reduce = nn.Sequential(\r\n",
        "        GeneralizedMeanPoolingP(),\r\n",
        "        nn.Conv2d(input_dim, reduce_dim, 1, bias=False),\r\n",
        "        nn.BatchNorm2d(reduce_dim),\r\n",
        "        nn.ReLU(True),\r\n",
        "        Flatten()\r\n",
        "    )\r\n",
        "    pool_reduce.apply(weights_init_kaiming)\r\n",
        "    return pool_reduce\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_reduction_layer(input_dim,feat_dim):\r\n",
        "    fc=nn.Linear(input_dim,feat_dim)\r\n",
        "    bn=nn.BatchNorm1d(feat_dim)\r\n",
        "    relu=nn.ReLU()\r\n",
        "    return nn.Sequential(\r\n",
        "        fc,\r\n",
        "        bn,\r\n",
        "        relu\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class ModelWrapper(nn.Module):\r\n",
        "    def __init__(self, model):\r\n",
        "        super().__init__()\r\n",
        "        self.model = model\r\n",
        "        self.adaptive_param = self._make_layer(self.model)\r\n",
        "\r\n",
        "    def _make_layer(self, model):\r\n",
        "        layers = []\r\n",
        "\r\n",
        "        for _name, _params in self.model.named_parameters():\r\n",
        "            if \"conv\" in _name:\r\n",
        "                if \"weight\" in _name:\r\n",
        "                    layers.append(nn.Parameter(data=torch.tensor(0.0)))\r\n",
        "                if \"bias\" in _name:\r\n",
        "                    layers.append(nn.Parameter(data=torch.tensor(0.0)))\r\n",
        "            if \"bn\" in _name:\r\n",
        "                if \"weight\" in _name:\r\n",
        "                    layers.append(nn.Parameter(data=torch.tensor(0.0)))\r\n",
        "                if \"bias\" in _name:\r\n",
        "                    layers.append(nn.Parameter(data=torch.tensor(0.0)))\r\n",
        "        return nn.ParameterList(layers)\r\n",
        "\r\n",
        "    def forward(self, images,heads,masks,labels):\r\n",
        "        x = self.model(images,heads,masks,labels)\r\n",
        "        return x"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJgcRaazDp_6"
      },
      "source": [
        "##主要函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_NJxhdYEs4x"
      },
      "source": [
        "###shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CifCQ9HGDriD"
      },
      "source": [
        "\r\n",
        "class Shuffle_block(nn.Module):\r\n",
        "    def __init__(self,inchannel,out_channel,repeat_count):\r\n",
        "        super(Shuffle_block, self).__init__()\r\n",
        "        layers=[]\r\n",
        "        layers.append(Shuffle_B(inchannel,inchannel))\r\n",
        "        for i in range(repeat_count):\r\n",
        "            layers.append(Shuffle_A)\r\n",
        "        layers.append(Shuffle_B(inchannel,out_channel))\r\n",
        "        self.model=nn.Sequential(*layers)\r\n",
        "    def forward(self,x):\r\n",
        "        return self.model(x)\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIpaBSREEvAr"
      },
      "source": [
        "###hacnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC5Tai3EExhm"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class HACNN(nn.Module):\r\n",
        "    \"\"\"Harmonious Attention Convolutional Neural Network.\r\n",
        "    Reference:\r\n",
        "        Li et al. Harmonious Attention Network for Person Re-identification. CVPR 2018.\r\n",
        "    Public keys:\r\n",
        "        - ``hacnn``: HACNN.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Args:\r\n",
        "    #    num_classes (int): number of classes to predict\r\n",
        "    #    nchannels (list): number of channels AFTER concatenation\r\n",
        "    #    feat_dim (int): feature dimension for a single stream\r\n",
        "    #    learn_region (bool): whether to learn region features (i.e. local branch)\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            loss='softmax',\r\n",
        "            nchannels=[128, 256, 384],\r\n",
        "            feat_dim=2048,\r\n",
        "            use_gpu=True,\r\n",
        "            **kwargs\r\n",
        "    ):\r\n",
        "        super(HACNN, self).__init__()\r\n",
        "        self.loss = loss\r\n",
        "\r\n",
        "        self.use_gpu = use_gpu\r\n",
        "\r\n",
        "        self.conv = ConvBlock(3, 32, 3, s=2, p=1)\r\n",
        "\r\n",
        "        # Construct Inception + HarmAttn blocks\r\n",
        "        # ============== Block 1 ==============\r\n",
        "        self.inception1 = nn.Sequential(\r\n",
        "            InceptionA(32, nchannels[0]),\r\n",
        "            InceptionB(nchannels[0], nchannels[0]),\r\n",
        "        )\r\n",
        "        self.ha1 = HarmAttn(nchannels[0])\r\n",
        "\r\n",
        "        # ============== Block 2 ==============\r\n",
        "        self.inception2 = nn.Sequential(\r\n",
        "            InceptionA(nchannels[0], nchannels[1]),\r\n",
        "            InceptionB(nchannels[1], nchannels[1]),\r\n",
        "        )\r\n",
        "        self.ha2 = HarmAttn(nchannels[1])\r\n",
        "\r\n",
        "        # ============== Block 3 ==============\r\n",
        "        self.inception3 = nn.Sequential(\r\n",
        "            InceptionA(nchannels[1], nchannels[2]),\r\n",
        "            InceptionB(nchannels[2], nchannels[2]),\r\n",
        "        )\r\n",
        "        self.ha3 = HarmAttn(nchannels[2])\r\n",
        "\r\n",
        "        self.conv_block=nn.Sequential(\r\n",
        "            nn.Conv2d(nchannels[2],2048,kernel_size=1),\r\n",
        "            nn.BatchNorm2d(2048),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "        self.feat_dim = feat_dim\r\n",
        "\r\n",
        "    def init_scale_factors(self):\r\n",
        "        # initialize scale factors (s_w, s_h) for four regions\r\n",
        "        self.scale_factors = []\r\n",
        "        self.scale_factors.append(\r\n",
        "            torch.tensor([[1, 0], [0, 0.25]], dtype=torch.float)\r\n",
        "        )\r\n",
        "        self.scale_factors.append(\r\n",
        "            torch.tensor([[1, 0], [0, 0.25]], dtype=torch.float)\r\n",
        "        )\r\n",
        "        self.scale_factors.append(\r\n",
        "            torch.tensor([[1, 0], [0, 0.25]], dtype=torch.float)\r\n",
        "        )\r\n",
        "        self.scale_factors.append(\r\n",
        "            torch.tensor([[1, 0], [0, 0.25]], dtype=torch.float)\r\n",
        "        )\r\n",
        "\r\n",
        "    def stn(self, x, theta):\r\n",
        "        \"\"\"Performs spatial transform\r\n",
        "\r\n",
        "        x: (batch, channel, height, width)\r\n",
        "        theta: (batch, 2, 3)\r\n",
        "        \"\"\"\r\n",
        "        grid = F.affine_grid(theta, x.size())\r\n",
        "        x = F.grid_sample(x, grid)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def transform_theta(self, theta_i, region_idx):\r\n",
        "        \"\"\"Transforms theta to include (s_w, s_h), resulting in (batch, 2, 3)\"\"\"\r\n",
        "        scale_factors = self.scale_factors[region_idx]\r\n",
        "        theta = torch.zeros(theta_i.size(0), 2, 3)\r\n",
        "        theta[:, :, :2] = scale_factors\r\n",
        "        theta[:, :, -1] = theta_i\r\n",
        "        if self.use_gpu:\r\n",
        "            theta = theta.cuda()\r\n",
        "        return theta\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        assert x.size(2) == 160 and x.size(3) == 64, \\\r\n",
        "            'Input size does not match, expected (160, 64) but got ({}, {})'.format(x.size(2), x.size(3))\r\n",
        "        x = self.conv(x)\r\n",
        "        x1 = self.inception1(x)\r\n",
        "        x1_attn, x1_theta = self.ha1(x1)\r\n",
        "        x1_out = x1 * x1_attn\r\n",
        "        x2 = self.inception2(x1_out)\r\n",
        "        x2_attn, x2_theta = self.ha2(x2)\r\n",
        "        x2_out = x2 * x2_attn\r\n",
        "        x3 = self.inception3(x2_out)\r\n",
        "        x3_attn, x3_theta = self.ha3(x3)\r\n",
        "        x3_out = x3 * x3_attn\r\n",
        "        feat=self.conv_block(x3_out)\r\n",
        "        return feat\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wDLbJj5EzVN"
      },
      "source": [
        "###ghost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KY5OHajE0Cd"
      },
      "source": [
        "\r\n",
        "class GhostBottleneck(nn.Module):\r\n",
        "    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\r\n",
        "                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\r\n",
        "        super(GhostBottleneck, self).__init__()\r\n",
        "        has_se = se_ratio is not None and se_ratio > 0.\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "        # Point-wise expansion\r\n",
        "        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\r\n",
        "\r\n",
        "        # Depth-wise convolution\r\n",
        "        if self.stride > 1:\r\n",
        "            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\r\n",
        "                                     padding=(dw_kernel_size - 1) // 2,\r\n",
        "                                     groups=mid_chs, bias=False)\r\n",
        "            self.bn_dw = nn.BatchNorm2d(mid_chs)\r\n",
        "\r\n",
        "        # Squeeze-and-excitation\r\n",
        "        if has_se:\r\n",
        "            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\r\n",
        "        else:\r\n",
        "            self.se = None\r\n",
        "\r\n",
        "        # Point-wise linear projection\r\n",
        "        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\r\n",
        "\r\n",
        "        # shortcut\r\n",
        "        if (in_chs == out_chs and self.stride == 1):\r\n",
        "            self.shortcut = nn.Sequential()\r\n",
        "        else:\r\n",
        "            self.shortcut = nn.Sequential(\r\n",
        "                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\r\n",
        "                          padding=(dw_kernel_size - 1) // 2, groups=in_chs, bias=False),\r\n",
        "                nn.BatchNorm2d(in_chs),\r\n",
        "                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\r\n",
        "                nn.BatchNorm2d(out_chs),\r\n",
        "            )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "\r\n",
        "        # 1st ghost bottleneck\r\n",
        "        x = self.ghost1(x)\r\n",
        "\r\n",
        "        # Depth-wise convolution\r\n",
        "        if self.stride > 1:\r\n",
        "            x = self.conv_dw(x)\r\n",
        "            x = self.bn_dw(x)\r\n",
        "\r\n",
        "        # Squeeze-and-excitation\r\n",
        "        if self.se is not None:\r\n",
        "            x = self.se(x)\r\n",
        "\r\n",
        "        # 2nd ghost bottleneck\r\n",
        "        x = self.ghost2(x)\r\n",
        "\r\n",
        "        x += self.shortcut(residual)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class GhostNet(nn.Module):\r\n",
        "    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\r\n",
        "        super(GhostNet, self).__init__()\r\n",
        "        # setting of inverted residual blocks\r\n",
        "        self.cfgs = cfgs\r\n",
        "        self.dropout = dropout\r\n",
        "\r\n",
        "        # building first layer\r\n",
        "        output_channel = _make_divisible(16 * width, 4)\r\n",
        "        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(output_channel)\r\n",
        "        self.act1 = nn.ReLU(inplace=True)\r\n",
        "        input_channel = output_channel\r\n",
        "\r\n",
        "        # building inverted residual blocks\r\n",
        "        stages = []\r\n",
        "        block = GhostBottleneck\r\n",
        "        for cfg in self.cfgs:\r\n",
        "            layers = []\r\n",
        "            for k, exp_size, c, se_ratio, s in cfg:\r\n",
        "                output_channel = _make_divisible(c * width, 4)\r\n",
        "                hidden_channel = _make_divisible(exp_size * width, 4)\r\n",
        "                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\r\n",
        "                                    se_ratio=se_ratio))\r\n",
        "                input_channel = output_channel\r\n",
        "            stages.append(nn.Sequential(*layers))\r\n",
        "\r\n",
        "        output_channel = _make_divisible(exp_size * width, 4)\r\n",
        "        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\r\n",
        "        input_channel = output_channel\r\n",
        "\r\n",
        "        self.blocks = nn.Sequential(*stages)\r\n",
        "        self.conv_final=nn.Sequential(\r\n",
        "            nn.Conv2d(960,2048,kernel_size=1),\r\n",
        "            nn.BatchNorm2d(2048),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "        # building last several layers\r\n",
        "        output_channel = 1280\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv_stem(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.act1(x)\r\n",
        "        x = self.blocks(x)\r\n",
        "        x=self.conv_final(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "def ghostnet(**kwargs):\r\n",
        "    \"\"\"\r\n",
        "    Constructs a GhostNet model\r\n",
        "    \"\"\"\r\n",
        "    cfgs = [\r\n",
        "        # k, t, c, SE, s\r\n",
        "        # stage1\r\n",
        "        [[3,  16,  16, 0, 1]],\r\n",
        "        # stage2\r\n",
        "        [[3,  48,  24, 0, 2]],\r\n",
        "        [[3,  72,  24, 0, 1]],\r\n",
        "        # stage3\r\n",
        "        [[5,  72,  40, 0.25, 2]],\r\n",
        "        [[5, 120,  40, 0.25, 1]],\r\n",
        "        # stage4\r\n",
        "        [[3, 240,  80, 0, 2]],\r\n",
        "        [[3, 200,  80, 0, 1],\r\n",
        "         [3, 184,  80, 0, 1],\r\n",
        "         [3, 184,  80, 0, 1],\r\n",
        "         [3, 480, 112, 0.25, 1],\r\n",
        "         [3, 672, 112, 0.25, 1]\r\n",
        "        ],\r\n",
        "        # stage5\r\n",
        "        [[5, 672, 160, 0.25, 2]],\r\n",
        "        [[5, 960, 160, 0, 1],\r\n",
        "         [5, 960, 160, 0.25, 1],\r\n",
        "         [5, 960, 160, 0, 1],\r\n",
        "         [5, 960, 160, 0.25, 1]\r\n",
        "        ]\r\n",
        "    ]\r\n",
        "    return GhostNet(cfgs, **kwargs)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_OzHZJFMmI"
      },
      "source": [
        "###backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG-kqbFiFN4i"
      },
      "source": [
        "\r\n",
        "\r\n",
        "from torchvision.models import resnet50\r\n",
        "from geffnet import efficientnet_b0\r\n",
        "def build_backbone(model_name):\r\n",
        "    if model_name=='resnet50':\r\n",
        "        model = resnet50(True)\r\n",
        "        stride = 1\r\n",
        "        model.layer4[0].downsample[0].stride = stride\r\n",
        "        model.layer4[0].conv2.stride = stride\r\n",
        "        base = nn.Sequential(\r\n",
        "            model.conv1,\r\n",
        "            model.bn1,\r\n",
        "            model.maxpool,\r\n",
        "            model.layer1,\r\n",
        "            model.layer2,\r\n",
        "            model.layer3,\r\n",
        "            model.layer4\r\n",
        "        )\r\n",
        "        feat=2048\r\n",
        "    elif model_name=='ghost':\r\n",
        "        base=ghostnet()\r\n",
        "        feat=2048\r\n",
        "    elif model_name=='Lambda':\r\n",
        "        base=LambdaResNet50()\r\n",
        "        feat=2048\r\n",
        "    elif model_name=='hacnn':\r\n",
        "        base=HACNN()\r\n",
        "        feat=2048\r\n",
        "\r\n",
        "\r\n",
        "    return base,feat\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejaGcD5fFO07"
      },
      "source": [
        "#head branch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRk5mSJDFPLy"
      },
      "source": [
        "\r\n",
        "def conv_block(inchannel,outchannel):\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Conv2d(inchannel,outchannel,kernel_size=1),\r\n",
        "        nn.BatchNorm2d(outchannel),\r\n",
        "        nn.ReLU()\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Head_Branch(nn.Module):\r\n",
        "    def __init__(self,cfg,num_class):\r\n",
        "        super(Head_Branch,self).__init__()\r\n",
        "        self.backbone,feat_dim=build_backbone(cfg['head_backbone'])\r\n",
        "        self.cfg=cfg\r\n",
        "        self.cls_criterion=CrossEntropyLabelSmoothLoss(num_class)\r\n",
        "        self.circle_criterion=CircleLoss(m=0.25, gamma=80)\r\n",
        "        self.flat=Flatten()\r\n",
        "        if cfg['head_attention']:\r\n",
        "            if cfg['HAN']:\r\n",
        "                self.has_1_pool=build_pool_conv(feat_dim,512)\r\n",
        "                self.has_2_pool=build_pool_conv(feat_dim,512)\r\n",
        "                self.has_3_pool=build_pool_conv(feat_dim,512)\r\n",
        "\r\n",
        "                self.has_red=build_reduction_layer(512*3,1024)\r\n",
        "                self.head_bn = nn.BatchNorm1d(1024)\r\n",
        "                self.head_bn.bias.requires_grad_(False)\r\n",
        "                self.head_classfier = nn.Linear(1024, num_class, bias=False)\r\n",
        "                self.head_bn.apply(weights_init_kaiming)\r\n",
        "                self.head_classfier.apply(weights_init_classifier)\r\n",
        "\r\n",
        "                self.han_pool_1 = GeneralizedMeanPoolingP()\r\n",
        "                self.han_c_att_1 = nn.Sequential(nn.Linear(2048, 1024, bias=False), nn.ReLU(),\r\n",
        "                                                 nn.Linear(1024, 2048, bias=False))\r\n",
        "                \r\n",
        "                self.han_c_att_1[0].apply(weights_init_classifier)\r\n",
        "                self.han_c_att_1[1].apply(weights_init_classifier)\r\n",
        "                self.han_pool_2 = GeneralizedMeanPoolingP()\r\n",
        "                self.han_c_att_2 = nn.Sequential(nn.Linear(2048, 1024, bias=False), nn.ReLU(),\r\n",
        "                                                 nn.Linear(1024, 2048, bias=False))\r\n",
        "                self.han_c_att_2[0].apply(weights_init_classifier)\r\n",
        "                self.han_c_att_2[1].apply(weights_init_classifier)\r\n",
        "                self.han_pool_3 = GeneralizedMeanPoolingP()\r\n",
        "                self.han_c_att_3 = nn.Sequential(nn.Linear(2048, 1024, bias=False), nn.ReLU(),\r\n",
        "                                                 nn.Linear(1024, 2048, bias=False))\r\n",
        "                self.han_c_att_3[0].apply(weights_init_classifier)\r\n",
        "                self.han_c_att_3[1].apply(weights_init_classifier)\r\n",
        "\r\n",
        "            elif cfg['BCOS']:\r\n",
        "                self.branch1_conv=conv_block(feat_dim,512)\r\n",
        "                self.branch1_pool=GeneralizedMeanPoolingP()\r\n",
        "                self.branch1_bn=nn.BatchNorm1d(512)\r\n",
        "                self.branch1_bn.bias.requires_grad_(False)\r\n",
        "                self.branch1_fc=nn.Linear(512,num_class,bias=False)\r\n",
        "                self.branch1_bn.apply(weights_init_kaiming)\r\n",
        "                self.branch1_fc.apply(weights_init_classifier)\r\n",
        "\r\n",
        "\r\n",
        "                self.branch4_conv=conv_block(2048,1024)\r\n",
        "                self.branch4_pool=nn.ModuleList()\r\n",
        "                self.branch4_pool_conv=nn.ModuleList()\r\n",
        "                for i in range(3):\r\n",
        "                    self.branch4_pool.append(GeneralizedMeanPoolingP())\r\n",
        "                    self.branch4_pool_conv.append(conv_block(1024,512))\r\n",
        "\r\n",
        "\r\n",
        "                self.branch4_bn = nn.BatchNorm1d(512*3)\r\n",
        "                self.branch4_bn.bias.requires_grad_(False)\r\n",
        "                self.branch4_fc = nn.Linear(512*3, num_class, bias=False)\r\n",
        "                self.branch4_bn.apply(weights_init_kaiming)\r\n",
        "                self.branch4_fc.apply(weights_init_classifier)\r\n",
        "\r\n",
        "\r\n",
        "                self.branch2_conv=conv_block(2048,2048)\r\n",
        "                self.local_3_conv_list = nn.ModuleList()\r\n",
        "                self.rest_3_conv_list = nn.ModuleList()\r\n",
        "                self.relation_3_conv_list = nn.ModuleList()\r\n",
        "                for i in range(3):\r\n",
        "                    self.local_3_conv_list.append(nn.Sequential(\r\n",
        "                        nn.Conv2d(2048, 256, 1),\r\n",
        "                        nn.BatchNorm2d(256),\r\n",
        "                        nn.ReLU(inplace=True)))\r\n",
        "                for i in range(3):\r\n",
        "                    self.rest_3_conv_list.append(nn.Sequential(\r\n",
        "                        nn.Conv2d(2048, 256, 1),\r\n",
        "                        nn.BatchNorm2d(256),\r\n",
        "                        nn.ReLU(inplace=True)))\r\n",
        "                for i in range(3):\r\n",
        "                    self.relation_3_conv_list.append(nn.Sequential(\r\n",
        "                        nn.Conv2d(256 * 2, 256, 1),\r\n",
        "                        nn.BatchNorm2d(256),\r\n",
        "                        nn.ReLU(inplace=True)))\r\n",
        "                self.fc_local_rest_3_list = nn.ModuleList()\r\n",
        "                for _ in range(3):\r\n",
        "                    fc = nn.Linear(256, num_class,bias=False)\r\n",
        "                    fc.apply(weights_init_classifier)\r\n",
        "                    self.fc_local_rest_3_list.append(fc)\r\n",
        "\r\n",
        "                self.branch3_conv=conv_block(2048,2048)\r\n",
        "                self.global_3_max_conv_list=nn.ModuleList()\r\n",
        "                self.global_3_rest_conv_list=nn.ModuleList()\r\n",
        "                self.global_3_pooling_conv_list=nn.ModuleList()\r\n",
        "                self.global_3_max_conv_list.append(nn.Sequential(\r\n",
        "                    nn.Conv2d(2048, 256, 1),\r\n",
        "                    nn.BatchNorm2d(256),\r\n",
        "                    nn.ReLU(inplace=True)))\r\n",
        "                self.global_3_rest_conv_list.append(nn.Sequential(\r\n",
        "                    nn.Conv2d(2048, 256, 1),\r\n",
        "                    nn.BatchNorm2d(256),\r\n",
        "                    nn.ReLU(inplace=True)))\r\n",
        "                self.global_3_pooling_conv_list.append(nn.Sequential(\r\n",
        "                    nn.Conv2d(512, 256, 1),\r\n",
        "                    nn.BatchNorm2d(256),\r\n",
        "                    nn.ReLU(inplace=True)))\r\n",
        "                self.fc_global_rest_3_list = nn.ModuleList()\r\n",
        "                fc = nn.Linear(256, num_class,bias=False)\r\n",
        "                fc.apply(weights_init_classifier)\r\n",
        "                self.fc_global_rest_3_list.append(fc)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        else:\r\n",
        "            self.pool=GeneralizedMeanPoolingP()\r\n",
        "            self.bn=nn.BatchNorm1d(feat_dim)\r\n",
        "            self.bn.bias.requires_grad_(False)\r\n",
        "            self.fc=nn.Linear(feat_dim,num_class,bias=False)\r\n",
        "            self.bn.apply(weights_init_kaiming)\r\n",
        "            self.fc.apply(weights_init_classifier)\r\n",
        "\r\n",
        "\r\n",
        "    def HAN_1(self, x):\r\n",
        "        c_att = self.han_c_att_1(self.han_pool_1(x).view(x.shape[0], -1))\r\n",
        "        c_att = F.sigmoid(c_att).view(x.shape[0], -1, 1, 1)\r\n",
        "        feat = x + torch.mul(x, c_att)\r\n",
        "        s_att = F.sigmoid(torch.sum(feat, dim=1)).unsqueeze(1)\r\n",
        "        han_feat = torch.mul(feat, s_att)\r\n",
        "        return han_feat\r\n",
        "\r\n",
        "    def HAN_2(self, x):\r\n",
        "        c_att = self.han_c_att_2(self.han_pool_2(x).view(x.shape[0], -1))\r\n",
        "        c_att = F.sigmoid(c_att).view(x.shape[0], -1, 1, 1)\r\n",
        "        feat = x + torch.mul(x, c_att)\r\n",
        "        s_att = F.sigmoid(torch.sum(feat, dim=1)).unsqueeze(1)\r\n",
        "        han_feat = torch.mul(feat, s_att)\r\n",
        "        return han_feat\r\n",
        "\r\n",
        "    def HAN_3(self, x):\r\n",
        "        c_att = self.han_c_att_3(self.han_pool_3(x).view(x.shape[0], -1))\r\n",
        "        c_att = F.sigmoid(c_att).view(x.shape[0], -1, 1, 1)\r\n",
        "        feat = x + torch.mul(x, c_att)\r\n",
        "        s_att = F.sigmoid(torch.sum(feat, dim=1)).unsqueeze(1)\r\n",
        "        han_feat = torch.mul(feat, s_att)\r\n",
        "        return han_feat\r\n",
        "    def forward(self,x,label):\r\n",
        "        x=self.backbone(x)\r\n",
        "        loss=0\r\n",
        "        if self.cfg['head_attention']:\r\n",
        "            if self.cfg['HAN']:\r\n",
        "                height=int(x.shape[2]//3)\r\n",
        "                hf_1=self.HAN_1(x[:,:,0*height:1*height,:])\r\n",
        "                hf_1=self.has_1_pool(hf_1)\r\n",
        "                \r\n",
        "\r\n",
        "                hf_2=self.HAN_2(x[:,:,1*height:2*height,:])\r\n",
        "                hf_2=self.has_2_pool(hf_2)\r\n",
        "                \r\n",
        "                hf_3=self.HAN_3(x[:,:,2*height:3*height,:])\r\n",
        "                hf_3=self.has_3_pool(hf_3)\r\n",
        "                \r\n",
        "                hf=torch.cat([hf_1,hf_2,hf_3],dim=1)\r\n",
        "                global_feat=self.has_red(hf)\r\n",
        "                feat=self.head_bn(global_feat)\r\n",
        "                logit=self.head_classfier(feat)\r\n",
        "\r\n",
        "                if self.training:\r\n",
        "                  loss=loss+self.circle_criterion(hf_3,label)\r\n",
        "                  loss=loss+self.circle_criterion(hf_2,label)\r\n",
        "                  loss=loss+self.circle_criterion(hf_1,label)\r\n",
        "                  loss=loss+self.circle_criterion(global_feat,label)\r\n",
        "                  loss=loss+self.cls_criterion(logit,label)\r\n",
        "            elif self.cfg['BCOS']:\r\n",
        "                BCOS_feat=[]\r\n",
        "                branch1_feat=self.branch1_conv(x)\r\n",
        "                branch1_feat=self.branch1_pool(branch1_feat)\r\n",
        "                branch1_feat=self.flat(branch1_feat)\r\n",
        "                feat1=self.branch1_bn(branch1_feat)\r\n",
        "                logit1=self.branch1_fc(feat1)\r\n",
        "                BCOS_feat.append(feat1)\r\n",
        "\r\n",
        "                branch2_feat=self.branch2_conv(x)\r\n",
        "                stripe_h_3 = int(branch2_feat.size(2) / 3)\r\n",
        "                local_feat_list=[]\r\n",
        "                final_feat_list=[]\r\n",
        "                rest_feat_list=[]\r\n",
        "\r\n",
        "                local_rest_logit=[]\r\n",
        "                for i in range(3):\r\n",
        "                    local_3_feat = F.max_pool2d(\r\n",
        "                        branch2_feat[:, :, i * stripe_h_3: (i + 1) * stripe_h_3, :],\r\n",
        "                        (stripe_h_3, branch2_feat.size(-1)))\r\n",
        "                    local_feat_list.append(local_3_feat)\r\n",
        "                for i in range(3):\r\n",
        "                    rest_feat_list.append((local_feat_list[(i + 1) % 3]\r\n",
        "                                             + local_feat_list[(i + 2) % 3]\r\n",
        "                                           )/2)\r\n",
        "\r\n",
        "\r\n",
        "                for i in range(3):\r\n",
        "                    local_6_feat = self.local_3_conv_list[i](local_feat_list[i]).squeeze(3).squeeze(2)\r\n",
        "                    input_rest_6_feat = self.rest_3_conv_list[i](rest_feat_list[i]).squeeze(3).squeeze(2)\r\n",
        "\r\n",
        "                    input_local_rest_6_feat = torch.cat((local_6_feat, input_rest_6_feat), 1).unsqueeze(2).unsqueeze(3)\r\n",
        "\r\n",
        "                    local_rest_6_feat = self.relation_3_conv_list[i](input_local_rest_6_feat)\r\n",
        "\r\n",
        "                    local_rest_6_feat = (local_rest_6_feat\r\n",
        "                                         + local_6_feat.unsqueeze(2).unsqueeze(3)).squeeze(3).squeeze(2)\r\n",
        "\r\n",
        "                    final_feat_list.append(local_rest_6_feat)\r\n",
        "\r\n",
        "                    local_rest_logit.append(self.fc_local_rest_3_list[i](local_rest_6_feat))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "                global_feat_list=[]\r\n",
        "                global_logit_list=[]\r\n",
        "\r\n",
        "                branch3_feat=self.branch3_conv(x)\r\n",
        "                for i in range(3):\r\n",
        "                    local_3_feat = F.max_pool2d(\r\n",
        "                        branch2_feat[:, :, i * stripe_h_3: (i + 1) * stripe_h_3, :],\r\n",
        "                        (stripe_h_3, branch2_feat.size(-1)))\r\n",
        "                    global_feat_list.append(local_3_feat)\r\n",
        "                global_2_max_feat = F.max_pool2d(branch3_feat, (branch3_feat.size(2), branch3_feat.size(3)))\r\n",
        "                global_2_rest_feat = (global_feat_list[0] + global_feat_list[1] - global_2_max_feat)\r\n",
        "                global_2_max_feat = self.global_3_max_conv_list[0](global_2_max_feat)\r\n",
        "                global_2_rest_feat = self.global_3_rest_conv_list[0](global_2_rest_feat)\r\n",
        "                global_2_max_rest_feat = self.global_3_pooling_conv_list[0](\r\n",
        "                    torch.cat((global_2_max_feat, global_2_rest_feat), 1))\r\n",
        "                global_2_feat = (global_2_max_feat + global_2_max_rest_feat).squeeze(3).squeeze(2)\r\n",
        "                final_feat_list.append(global_2_feat)\r\n",
        "                global_logit_list.append(self.fc_global_rest_3_list[0](global_2_feat))\r\n",
        "\r\n",
        "                branch4_feat=self.branch4_conv(x)\r\n",
        "                stride=int(branch4_feat.shape[2]//3)\r\n",
        "                branch4_feat_list=[]\r\n",
        "                for i in range(3):\r\n",
        "                    stride_feat=branch4_feat[:,:,i*stride:(i+1)*stride,:]\r\n",
        "                    branch4_local_feat=self.branch4_pool[i](stride_feat)\r\n",
        "                    branch4_local_feat=self.branch4_pool_conv[i](branch4_local_feat)\r\n",
        "                    branch4_local_feat=self.flat(branch4_local_feat)\r\n",
        "                    branch4_feat_list.append(branch4_local_feat)\r\n",
        "                branch4_feat=torch.cat(branch4_feat_list,dim=1)\r\n",
        "                feat4=self.branch4_bn(branch4_feat)\r\n",
        "                BCOS_feat.append(feat4)\r\n",
        "                logit4=self.branch4_fc(feat4)\r\n",
        "\r\n",
        "                \r\n",
        "\r\n",
        "                if self.training:\r\n",
        "                  for feat in final_feat_list:\r\n",
        "                      loss=loss+self.circle_criterion(feat,label)\r\n",
        "\r\n",
        "                  for logit in global_logit_list:\r\n",
        "                      loss=loss+self.cls_criterion(logit,label)\r\n",
        "\r\n",
        "                  for logit in local_rest_logit:\r\n",
        "                      loss=loss+self.cls_criterion(logit,label)\r\n",
        "\r\n",
        "\r\n",
        "                  loss=loss+self.cls_criterion(logit1,label)\r\n",
        "                  loss=loss+self.circle_criterion(feat1,label)\r\n",
        "\r\n",
        "                  loss=loss+self.cls_criterion(logit4,label)\r\n",
        "                  loss=loss+self.circle_criterion(feat4,label)\r\n",
        "                BCOS_feat.extend(final_feat_list)\r\n",
        "                feat=torch.cat(BCOS_feat,dim=1)\r\n",
        "                \r\n",
        "\r\n",
        "\r\n",
        "        else:\r\n",
        "            global_feat=self.pool(x)\r\n",
        "            global_feat=self.flat(global_feat)\r\n",
        "            feat=self.bn(global_feat)\r\n",
        "            logit=self.fc(feat)\r\n",
        "            if self.training:\r\n",
        "              loss=loss+self.circle_criterion(global_feat,label)\r\n",
        "              loss=loss+self.cls_criterion(logit,label)\r\n",
        "        return feat,loss\r\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mw59Kz1IcCV"
      },
      "source": [
        "# global_branch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qag9PFQHI9U_"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class Global_Branch(nn.Module):\r\n",
        "    def __init__(self,cfg,num_class):\r\n",
        "        self.backbone,feat_dim=build_backbone(cfg['global_backbone'])\r\n",
        "        self.pool=GeneralizedMeanPoolingP()\r\n",
        "        self.bn=nn.BatchNorm1d(self.in_planes)\r\n",
        "        self.bn.bias.requires_grad_(False)\r\n",
        "        self.bn.apply(weights_init_kaiming)\r\n",
        "        self.fc=nn.Linear(feat_dim,num_class)\r\n",
        "        self.fc.apply(weights_init_classifier)\r\n",
        "        self.cls_criterion=CrossEntropyLabelSmoothLoss(num_class)\r\n",
        "        self.circle_criterion=CircleLoss(m=0.25,gamma=80)\r\n",
        "\r\n",
        "    def forward(self,x,label):\r\n",
        "        global_feat = self.pool(self.backbone(x))\r\n",
        "        global_feat = global_feat.view(global_feat.shape[0], -1)\r\n",
        "        feat=self.bn(global_feat)\r\n",
        "        logit=self.fc(feat)\r\n",
        "        loss=0\r\n",
        "        if self.training:\r\n",
        "          loss=loss+self.cls_criterion(logit,label)\r\n",
        "          loss=loss+self.circle_criterion(global_feat,label)\r\n",
        "        return feat,loss\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjBCvGpvJrp3"
      },
      "source": [
        "# mask_branch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d87k1ptI86-"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import math\r\n",
        "class Mask_Branch(nn.Module):\r\n",
        "    def __init__(self,cfg,num_class):\r\n",
        "        super(Mask_Branch, self).__init__()\r\n",
        "        self.cls_criterion=CrossEntropyLabelSmoothLoss(num_class)\r\n",
        "        self.circle_criterion=CircleLoss(m=0.25,gamma=80)\r\n",
        "        self.N=cfg['N']\r\n",
        "        self.M=cfg['M']\r\n",
        "        self.lambdak=nn.Parameter(torch.from_numpy(np.array([0.05]*self.M)))\r\n",
        "        self.a0=-1*math.pi\r\n",
        "        self.b0=1*math.pi\r\n",
        "        self.a1=-1*math.pi/4\r\n",
        "        self.b1=-3*math.pi/4\r\n",
        "        self.a2=3*math.pi/4\r\n",
        "        self.b2=math.pi/4\r\n",
        "        self.R_max=2\r\n",
        "        self.backbone_list=nn.ModuleList()\r\n",
        "        for i in range(3):\r\n",
        "            backbone,feat_dim=build_backbone(cfg['mask_backbone'])\r\n",
        "            self.backbone_list.append(backbone)\r\n",
        "\r\n",
        "        self.pool_list=nn.ModuleList()\r\n",
        "        for i in range(3):\r\n",
        "            for j in range(4):\r\n",
        "                pool=GeneralizedMeanPoolingP()\r\n",
        "                self.pool_list.append(pool)\r\n",
        "\r\n",
        "        self.ASE_FC_list=nn.ModuleList()\r\n",
        "        self.ASE_CONV_list=nn.ModuleList()\r\n",
        "        for i in range(3):\r\n",
        "            for j in range(4):\r\n",
        "                fc_block=nn.Sequential(\r\n",
        "                    nn.Linear(2048,512),\r\n",
        "                    nn.ReLU(),\r\n",
        "                    nn.Linear(512,2048),\r\n",
        "                )\r\n",
        "                conv_b=nn.Sequential(\r\n",
        "                    nn.Conv2d(2048,256,kernel_size=1),\r\n",
        "                    nn.BatchNorm2d(256),\r\n",
        "                    nn.ReLU()\r\n",
        "                )\r\n",
        "                self.ASE_FC_list.append(fc_block)\r\n",
        "                self.ASE_CONV_list.append(conv_b)\r\n",
        "\r\n",
        "        self.fc_list=nn.ModuleList()\r\n",
        "        for i in range(3):\r\n",
        "            for j in range(4):\r\n",
        "                fc=nn.Linear(256,num_class)\r\n",
        "                self.fc_list.append(fc)\r\n",
        "\r\n",
        "    def ASE(self,x,index):\r\n",
        "        feat=self.ASE_FC_list[index](x.view(x.shape[0],-1))\r\n",
        "        x=x+torch.mul(x,F.sigmoid(feat).view(x.shape[0],-1,1,1))\r\n",
        "        x=self.ASE_CONV_list[index](x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self,x,label):\r\n",
        "        theta_list0 = []\r\n",
        "        theta_list1 = []\r\n",
        "        theta_list2 = []\r\n",
        "        for i in range(self.M):\r\n",
        "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\r\n",
        "            fi = (self.b0 - self.a0) * zi + self.a0\r\n",
        "            theta_list0.append(fi)\r\n",
        "        for i in range(self.M):\r\n",
        "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\r\n",
        "            fi = (self.b1 - self.a1) * zi + self.a1\r\n",
        "            theta_list1.append(fi)\r\n",
        "        for i in range(self.M):\r\n",
        "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\r\n",
        "            fi = (self.b2 - self.a2) * zi + self.a2\r\n",
        "            theta_list2.append(fi)\r\n",
        "        theta_lists=np.zeros((3,self.M))\r\n",
        "        theta_lists[0]=theta_list0\r\n",
        "        theta_lists[1]=theta_list1\r\n",
        "        theta_lists[2]=theta_list2\r\n",
        "        theta_lists=torch.from_numpy(theta_lists)\r\n",
        "        self.grid = torch.zeros((3, 1, self.N, self.M, 2))\r\n",
        "        for k in range(3):\r\n",
        "            for i in range(self.N):\r\n",
        "                for j in range(self.M):\r\n",
        "                    self.grid[k,0,i,j,0]=self.R_max*j*torch.cos(theta_lists[k,i])/self.M\r\n",
        "                    self.grid[k,0,i,j,1]=self.R_max*j*torch.sin(theta_lists[k,i])/self.M\r\n",
        "        self.grid = self.grid.cuda()\r\n",
        "\r\n",
        "        grid_features=[]\r\n",
        "        for i in range(3):\r\n",
        "            grid=self.grid[i]\r\n",
        "            grid=grid.repeat(x.shape[0],1,1,1)\r\n",
        "            grid_feature=F.grid_sample(x,grid)\r\n",
        "            grid_features.append(grid_feature)\r\n",
        "        loss=0\r\n",
        "        logit_list=[]\r\n",
        "        final_feature_list=[]\r\n",
        "        for i in range(3):\r\n",
        "            features=self.backbone_list[i](grid_features[i])\r\n",
        "            stride=int(features.shape[2]/4)\r\n",
        "            for j in range(4):\r\n",
        "                feature=features[:,:,j*stride:(j+1)*stride,:]\r\n",
        "                feature=self.pool_list[i*4+j](feature)\r\n",
        "                feature=self.ASE(feature,i*4+j)\r\n",
        "                feature=feature.view(feature.shape[0],-1)\r\n",
        "                logit=self.fc_list[i*4+j](feature)\r\n",
        "                logit_list.append(logit)\r\n",
        "                final_feature_list.append(feature)\r\n",
        "        feat=torch.cat(final_feature_list,dim=1)\r\n",
        "        if self.training:        \r\n",
        "          for logit in logit_list:\r\n",
        "              loss=loss+self.cls_criterion(logit,label)\r\n",
        "\r\n",
        "          \r\n",
        "          loss=loss+self.circle_criterion(feat,label)\r\n",
        "        else:\r\n",
        "          loss=0\r\n",
        "        return feat,loss\r\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HqlmvxJyIT"
      },
      "source": [
        "#final_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d_5gRQ4J0KQ"
      },
      "source": [
        "\r\n",
        "class myModel(nn.Module):\r\n",
        "    def __init__(self,cfg,num_class):\r\n",
        "        super(myModel,self).__init__()\r\n",
        "        self.global_branch=Global_Branch(cfg,num_class)\r\n",
        "        global_feat=2048\r\n",
        "        self.head_branch=Head_Branch(cfg,num_class)\r\n",
        "        if cfg['head_attention']:\r\n",
        "          if cfg['HAN']:\r\n",
        "            head_feat=1024\r\n",
        "          elif cfg['BCOS']:\r\n",
        "            head_feat=3072\r\n",
        "        else:\r\n",
        "          head_feat=2048\r\n",
        "\r\n",
        "        self.mask_branch=Mask_Branch(cfg,num_class)\r\n",
        "        mask_feat=3072\r\n",
        "        self.cls_criterion = CrossEntropyLabelSmoothLoss()\r\n",
        "        self.circle_criterion = CircleLoss(m=0.25, gamma=80)\r\n",
        "\r\n",
        "        all_feat=global_feat+head_feat+mask_feat\r\n",
        "        self.reduce_linear=nn.Linear(4096,2048)\r\n",
        "        self.reduction_block=nn.Sequential(\r\n",
        "            nn.Conv2d(all_feat,4096,kernel_size=1),\r\n",
        "            nn.BatchNorm2d(4096),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Dropout()\r\n",
        "        )\r\n",
        "        self.reduction_bn=nn.BatchNorm1d(2048)\r\n",
        "        self.reduction_bn.bias.requires_grad_(False)\r\n",
        "        self.final_attention=nn.Sequential(\r\n",
        "            nn.Linear(2048,2048//16),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(2048//16,2048),\r\n",
        "            nn.Sigmoid()\r\n",
        "        )\r\n",
        "        self.final_cls=nn.Linear(2048,num_class,bias=False)\r\n",
        "        super(myModel, self).__init__()\r\n",
        "\r\n",
        "    def forward(self,images,heads,masks,label):\r\n",
        "        out={}\r\n",
        "        global_feature,global_loss=self.global_branch(images)\r\n",
        "        head_feature,head_loss=self.head_branch(heads)\r\n",
        "        mask_feature,mask_loss=self.mask_branch(masks)\r\n",
        "        all_feature=torch.cat([global_feature,head_feature,mask_feature],dim=1)\r\n",
        "        batch=all_feature.shape[0]\r\n",
        "        all_feature=all_feature.reshape(batch,-1,1,1)\r\n",
        "        all_feature=self.reduction_block(all_feature)\r\n",
        "        all_feature=all_feature.squeeze()\r\n",
        "        all_feature=self.reduce_linear(all_feature)\r\n",
        "        all_feature=self.reduce_bn(all_feature)\r\n",
        "        attention_mask=self.final_attention(all_feature)\r\n",
        "        all_feature=all_feature*attention_mask\r\n",
        "        all_logit=self.final_cls(all_feature)\r\n",
        "        if self.training:\r\n",
        "          loss=0\r\n",
        "          loss=loss+self.cls_criterion(all_logit,label)\r\n",
        "          loss=loss+self.circle_criterion(all_feature,label)\r\n",
        "        else:\r\n",
        "          loss=0\r\n",
        "        out['global_loss']=global_loss\r\n",
        "        out['head_loss']=head_loss\r\n",
        "        out['final_loss']=loss\r\n",
        "        out['mask_loss']=mask_loss\r\n",
        "        \r\n",
        "        out['global_feature']=global_feature\r\n",
        "        out['head_feature']=head_feature\r\n",
        "        out['mask_feature']=mask_feature\r\n",
        "        out['final_feature']=all_feature\r\n",
        "        \r\n",
        "        return out\r\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "yCW-YPhfu4Su",
        "outputId": "e83f192a-eb89-4db7-d92c-f2a261c6907b"
      },
      "source": [
        "device=torch.device('cuda:0')\r\n",
        "model=myModel(cfg,100)\r\n",
        "images=torch.rand(64,3,384,128)\r\n",
        "masks=torch.rand(64,3,384,128)\r\n",
        "faces=torch.rand(64,3,224,224)\r\n",
        "\r\n",
        "labels=torch.ones(64).long()\r\n",
        "\r\n",
        "model=model.to(device)\r\n",
        "images=images.to(device)\r\n",
        "masks=masks.to(device)\r\n",
        "faces=faces.to(device)\r\n",
        "labels=labels.to(device)\r\n",
        "scaler=GradScaler()\r\n",
        "with autocast():\r\n",
        "  out=model(images,masks,faces,labels)\r\n",
        "  \r\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-0b04d0a20df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b6b92eac2ab3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, num_class)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_branch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGlobal_Branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_branch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHead_Branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_branch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMask_Branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-f950e6ce6449>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, num_class)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGlobal_Branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'global_backbone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGeneralizedMeanPoolingP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_planes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    975\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                     raise AttributeError(\n\u001b[0;32m--> 977\u001b[0;31m                         \"cannot assign module before Module.__init__() call\")\n\u001b[0m\u001b[1;32m    978\u001b[0m                 \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DmkJsDJ-MR"
      },
      "source": [
        "#训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHxPB6WZJ_ip"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import logging\r\n",
        "import albumentations as albu\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from iglovikov_helper_functions.utils.image_utils import load_rgb, pad, unpad\r\n",
        "from iglovikov_helper_functions.dl.pytorch.utils import tensor_from_rgb_image\r\n",
        "scaler=GradScaler()\r\n",
        "class Trainer:\r\n",
        "  def __init__(self,cfg,train_component):\r\n",
        "    self.train_loaders=train_component['train_loaders']\r\n",
        "    self.val_loader=train_component['val_loader']\r\n",
        "    self.reid_model=train_component['model']\r\n",
        "    self.optimizer=train_component['optimizer']\r\n",
        "    self.scheduler=train_component['scheduler']\r\n",
        "    self.logger=train_component['logger']\r\n",
        "    self.num_class=train_component['num_class']\r\n",
        "    self.num_query=train_component['num_query']\r\n",
        "    self.checkpoint=train_component['checkpoint']\r\n",
        "    self.face_detect_model=train_component['face_model']\r\n",
        "    self.mask_detect_model=train_component['mask_model']\r\n",
        "    self.face_t=train_component['face_transform']\r\n",
        "    if cfg['ema']:\r\n",
        "      self.ema=train_component['ema']\r\n",
        "    if cfg['l2']:\r\n",
        "      self.l2_criterion=train_component['l2_criterion']\r\n",
        "    self.device=torch.device('cuda:0')\r\n",
        "    self.cfg=cfg\r\n",
        "\r\n",
        "\r\n",
        "  def train_one_epoch(self,train_loader):\r\n",
        "    face_id=0\r\n",
        "    for images,origin_images,pids in train_loader:\r\n",
        "      loss=0\r\n",
        "      self.optimizer.zero_grad()\r\n",
        "      heads=[]\r\n",
        "      for image in origin_images:\r\n",
        "        head=self.face_detect(image)\r\n",
        "        heads.append(head)\r\n",
        "      heads=torch.cat(heads,dim=0)\r\n",
        "      heads=heads.to(device)\r\n",
        "      torch.cuda.empty_cache()\r\n",
        "      masks=[]\r\n",
        "      for image in origin_images:\r\n",
        "        mask=self.mask_detect(image)\r\n",
        "        masks.append(mask)\r\n",
        "      masks=torch.cat(masks,dim=0)\r\n",
        "      masks=masks.to(device)\r\n",
        "      torch.cuda.empty_cache()\r\n",
        "      images=images.to(device)\r\n",
        "      pids=pids.to(devide)\r\n",
        "      with autocast():\r\n",
        "        outs=model(images,heads,masks,pids)\r\n",
        "        loss=loss+out['global_loss']\r\n",
        "        loss=loss+out['head_loss']\r\n",
        "        loss=loss+out['mask_loss']\r\n",
        "        loss=loss+out['final_loss']\r\n",
        "        if cfg['l2']:\r\n",
        "          loss=loss+self.l2_criterion(model,pids)\r\n",
        "      scaler.scale(loss).backward()\r\n",
        "      scaler.step(optimizer)\r\n",
        "      scaler.update() \r\n",
        "      if self.cfg['ema']:\r\n",
        "        self.ema.update()\r\n",
        "      del images\r\n",
        "      del heads\r\n",
        "      del masks\r\n",
        "    self.scheduler.step()\r\n",
        "  \r\n",
        "  def train(self):\r\n",
        "    epochs=self.cfg['epochs']\r\n",
        "\r\n",
        "    with tqdm(total=epochs) as pbar:\r\n",
        "      for epoch in range(epochs):\r\n",
        "        for loader in self.train_loaders:\r\n",
        "          model.train()\r\n",
        "          self.train_one_epoch(loader)\r\n",
        "          \r\n",
        "          if epoch%5==0:\r\n",
        "            if cfg['ema']:\r\n",
        "              ema.apply_shadow()\r\n",
        "            map0,map1=do_eval(cfg,model,val_loader,num_query)\r\n",
        "            checkpoint((map0+map1)/2,model,optimizer)\r\n",
        "            if cfg['ema']:\r\n",
        "              ema.restore()\r\n",
        "        pbar.update(1)\r\n",
        "  \r\n",
        "  \r\n",
        "  def mask_detect(self,image):\r\n",
        "    transform=albu.Compose([albu.Normalize(p=1)],p=1)\r\n",
        "    padded_image, pads = pad(image, factor=32, border=cv2.BORDER_CONSTANT)\r\n",
        "    x = transform(image=padded_image)[\"image\"]\r\n",
        "    x = torch.unsqueeze(tensor_from_rgb_image(x), 0)\r\n",
        "    x=x.to(self.device)\r\n",
        "    x=x.half()\r\n",
        "    with torch.no_grad():\r\n",
        "      prediction=self.mask_detect_model(x)[0][0]\r\n",
        "    mask = (prediction > 0).detach().cpu().numpy().astype(np.uint8)\r\n",
        "    contours , hierarchy = cv2.findContours ( mask , cv2.RETR_LIST , cv2.CHAIN_APPROX_SIMPLE )\r\n",
        "    new_image=np.zeros((mask.shape[0],mask.shape[1]))\r\n",
        "    cv2.drawContours(new_image, contours,-1,(255,0,255),3)\r\n",
        "    new_image=torch.from_numpy(new_image)\r\n",
        "    new_image=new_image.unsqueeze(0)\r\n",
        "    new_image=new_image.repeat(3,1,1)\r\n",
        "    new_image=new_image.unsqueeze(0)\r\n",
        "    return new_image\r\n",
        "\r\n",
        "  \r\n",
        "  def face_detect(self,image):\r\n",
        "    image=image.numpy().transpose(1,2,0)\r\n",
        "    image=image*255\r\n",
        "    boxes,labels,probs=self.face_detect_model.predict(image,1500/2,0.6)\r\n",
        "    if boxes.shape[0]==0:\r\n",
        "          head=image[:75,120:200]\r\n",
        "    else:\r\n",
        "          max_index=int(probs.argmax())\r\n",
        "          x,y,h,w=boxes[max_index]\r\n",
        "          x=int(x)\r\n",
        "          y=int(y)\r\n",
        "          h=int(h)\r\n",
        "          w=int(w)\r\n",
        "          miny=max(0,y-20)\r\n",
        "          maxy=min(y+w+20,320)\r\n",
        "          minx=max(0,x-40)\r\n",
        "          maxx=min(x+h+5,320)\r\n",
        "          head=image[miny:maxy,minx:maxx]\r\n",
        "    if os.path.exit(os.path.join('face','{}.jpg'.format(face_id))):\r\n",
        "          pass\r\n",
        "    else:\r\n",
        "          cv2.imwrite(os.path.join('face','{}.jpg'.format(face_id)),head)\r\n",
        "    head=self.face_t(head)\r\n",
        "    head=head.unsqueeze(0)\r\n",
        "    return head\r\n",
        "  \r\n",
        "  def eval(self):\r\n",
        "    self.reid_model.eval()\r\n",
        "    if cfg['eval_all']:\r\n",
        "      metric0=MARKET_MAP(num_query,all=True,one_day=True,date_length=date_length)\r\n",
        "    else:\r\n",
        "      metric0=MARKET_MAP(num_query,all=False,one_day=True,date_length=date_length)\r\n",
        "      metric1=MARKET_MAP(num_query,all=False,one_day=False,date_length=date_length)\r\n",
        "    with torch.no_grad():\r\n",
        "        for imgs,origin_images,pids,cams,dates in self.val_loader:\r\n",
        "          heads=[]\r\n",
        "          for image in origin_images:\r\n",
        "            head=self.face_detect(image)\r\n",
        "            heads.append(head)\r\n",
        "          heads=torch.cat(heads,dim=0)\r\n",
        "          heads=heads.to(device)\r\n",
        "          torch.cuda.empty_cache()\r\n",
        "          masks=[]\r\n",
        "          for image in origin_images:\r\n",
        "            mask=self.mask_detect(image)\r\n",
        "            masks.append(mask)\r\n",
        "          masks=torch.cat(masks,dim=0)\r\n",
        "          masks=masks.to(device)\r\n",
        "\r\n",
        "          images=imgs.to(device)    \r\n",
        "          out=model(images,heads,masks,pids)\r\n",
        "          for out in zip(out['final_feat'],pids,cams,dates):\r\n",
        "            metric0.update(out)\r\n",
        "            if cfg['eval_all']:\r\n",
        "              pass\r\n",
        "            else:\r\n",
        "              metric1.update(out)\r\n",
        "\r\n",
        "    map0=metric0.compute()\r\n",
        "    if cfg['eval_all']:\r\n",
        "      return map0,map0\r\n",
        "    else:\r\n",
        "      map1=metric1.compute()\r\n",
        "      return map0,map1       \r\n",
        "    \r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      \r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8B1uPILOELb"
      },
      "source": [
        "#主要函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aEPqMheOFw0",
        "outputId": "bce82c6f-39f1-4ae1-f16b-baa7fb14e0c2"
      },
      "source": [
        "import sys\r\n",
        "import cv2\r\n",
        "sys.path.append('/content/Ultra-Light-Fast-Generic-Face-Detector-1MB/vision')\r\n",
        "from vision.ssd.config.fd_config import define_img_size\r\n",
        "define_img_size(320)\r\n",
        "from vision.ssd.mb_tiny_fd import create_mb_tiny_fd, create_mb_tiny_fd_predictor\r\n",
        "from vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor\r\n",
        "from people_segmentation.pre_trained_models import create_model\r\n",
        "from people_segmentation.pre_trained_models import create_model\r\n",
        "def main(cfg):\r\n",
        "  train_component={}\r\n",
        "  train_loaders,val_loader,num_query,num_classes=MAKE_DATALOADER(cfg)\r\n",
        "  if cfg['l2']:\r\n",
        "    model=ModelWrapper(model)\r\n",
        "  model=myModel(cfg,num_classes)\r\n",
        "  if cfg['optimizer']=='adam':\r\n",
        "    optimizer=torch.optim.Adam(model.parameters(),lr=cfg['lr'],weight_decay=cfg['weight_decay'])\r\n",
        "  elif cfg['optimizer']=='sgd':\r\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=cfg['lr'],momentum=cfg['momentum'],weight_decay=cfg['weight_decay'])\r\n",
        "  elif cfg['optimizer']=='rangerlars':\r\n",
        "    optimizer=RangerLars(model.parameters(),lr=cfg['lr'])\r\n",
        "  \r\n",
        "  if cfg['scheduler']=='warmup':\r\n",
        "\r\n",
        "    scheduler = WarmupMultiStepLR(optimizer,cfg['steps'], cfg['gamma'],\r\n",
        "                  cfg['warmup_factors'],cfg['warmup_iters'], \r\n",
        "                  'linear')\r\n",
        "  elif cfg['scheduler']=='flat':\r\n",
        "    scheduler=Flat(optimizer,cfg)\r\n",
        "  \r\n",
        "  ckpt=cfg['ckpt']\r\n",
        "  logpt=cfg['logpt']\r\n",
        "  logger=logging.getLogger()\r\n",
        "  fh = logging.FileHandler(\"spam.log\")\r\n",
        "  fh.setLevel(logging.DEBUG)\r\n",
        "  logger.addHandler(fh)\r\n",
        "  checkpoint=Checkpoint(ckpt)\r\n",
        "  face_model_path=ckpt['face_ckpt']\r\n",
        "  class_names = ['face','background']\r\n",
        "  face_net = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True, device='cuda:0')\r\n",
        "\r\n",
        "  face_net.load(face_model_path)\r\n",
        "  face_predictor = create_Mb_Tiny_RFB_fd_predictor(face_net, candidate_size=1500, device='cuda:0')\r\n",
        "  mask_model = create_model(\"Unet_2020-07-20\")\r\n",
        "  mask_model.eval()\r\n",
        "  face_transform=build_face_transform(cfg)\r\n",
        "  train_component['train_loaders']=train_loaders\r\n",
        "  train_component['val_loader']=val_loader\r\n",
        "  train_component['model']=model\r\n",
        "  train_component['optimizer']=optimizer\r\n",
        "  train_component['scheduler']=scheduler\r\n",
        "  train_component['logger']=logger\r\n",
        "  train_component['num_class']=num_classes\r\n",
        "  train_component['num_query']=num_query\r\n",
        "  train_component['checkpoint']=checkpoint\r\n",
        "  train_component['face_model']=face_predictor\r\n",
        "  train_component['mask_model']=mask_model\r\n",
        "  train_component['face_transform']=face_transform\r\n",
        "  if cfg['ema']:\r\n",
        "    ema=EMA(model,0.999)\r\n",
        "    ema.register()\r\n",
        "    train_component['ema']=ema\r\n",
        "  if cfg['l2']:\r\n",
        "    l2_criterion=Adaptive_l2_loss()\r\n",
        "    train_component['l2_criterion']\r\n",
        "  trainer=Trainer(cfg,train_component)\r\n",
        "\r\n",
        "\r\n",
        "  \r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "priors nums:4420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BI8npZeV8ap"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "np.random.seed(42)\r\n",
        "random.seed(42)\r\n",
        "torch.backends.cudnn.benchmark=True\r\n",
        "torch.manual_seed(42)\r\n",
        "torch.cuda.manual_seed(42)       \r\n",
        "cfg={\r\n",
        "    'train_size':[384,128],\r\n",
        "    'test_size':[384,128],\r\n",
        "    'train_face_size':[224,224],\r\n",
        "    'test_face_size':[224,224],\r\n",
        "    'padding':10,\r\n",
        "    'mean':[0.485, 0.456, 0.406],\r\n",
        "    'std':[0.229, 0.224, 0.225],\r\n",
        "\r\n",
        "    \r\n",
        "    'num_workers':8,\r\n",
        "    'SAMPLER':'',\r\n",
        "\r\n",
        "    'train_bs':64,\r\n",
        "    'test_bs':64,\r\n",
        "\r\n",
        "    'train_K_instances':16,\r\n",
        "     \r\n",
        "    'epochs':50,\r\n",
        "    'ckpt':'reid_test1.pt',\r\n",
        "    'logpt':'test1.log',\r\n",
        "\r\n",
        "    \r\n",
        "   \r\n",
        "    'lr':3e-4,\r\n",
        "    'momentum':0.9,\r\n",
        "    'weight_decay':5e-4,\r\n",
        "     \r\n",
        "    'margin':0.3,\r\n",
        "    'steps':[35,55],\r\n",
        "    'warmup_iters':10,\r\n",
        "    'warmup_factors':1/3,\r\n",
        "    'gamma':0.1,\r\n",
        "    \r\n",
        "    \r\n",
        "     \r\n",
        "    'optimizer':'rangerlars',\r\n",
        "    'scheduler':'flat',\r\n",
        "    'l2':True,\r\n",
        "    'ema':False,\r\n",
        "    'mixstyle':True,\r\n",
        "     \r\n",
        "    'face_ckpt':'/content/drive/MyDrive/dataset/REID/version-RFB-320.pth',\r\n",
        "    'N':128,\r\n",
        "    'M':128,\r\n",
        "    'eval_all':True,\r\n",
        "    'global_backbone':'ghost',\r\n",
        "    'head_backbone':'ghost',\r\n",
        "    'mask_backbone':'ghost',\r\n",
        "    'head_attention':True,\r\n",
        "    'HAN':False,\r\n",
        "    'BCOS':True,\r\n",
        "    \r\n",
        "     \r\n",
        "    'train_data':['market'],\r\n",
        "    'val_data':['market'],\r\n",
        "\r\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdwtCVhDaAtv"
      },
      "source": [
        "data_roots={\r\n",
        "    'pdataset':PDataset('/content/datasets/reid1/reid1'),\r\n",
        "    'vc':VC_Clothes('/content/datasets/train'),\r\n",
        "    'market':Market('/content/datasets')\r\n",
        "}\r\n",
        "\r\n",
        "# main(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}